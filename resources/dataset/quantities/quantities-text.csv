id,filename,pid,text
0,inserm-00851058.training.tei.xml,0,"The influences of sex and age upon endurance performance have previously been documented for both running and swimming.Age and sex influences upon performance have also been investigated for ultra-triathlons with distances corresponding to several Ironman distances and lasting several days, and for off-road triathlons combining swimming, mountain biking and trail running. Triathlon represents an intriguing alternative model for analyzing the effects of age and sex upon endurance and ultra-endurance (> 6 hours) performance because sex differences and age-related declines in performance can be analyzed in the same persons across the three separate disciplines. The relative participation of both females and masters athletes (age > 40 years old) in triathlon has increased consistently over the past 25 years. Sex differences in triathlon performance are also known to differ between the modes of locomotion adopted (swimming, cycling or running) for both elite and non-elite triathletes. Generally, time differences between sexes in swimming have been shown to be smaller on average than during cycling and running. Both physiological and morphological factors contribute to explaining these findings. Performance density (i.e. the time difference between the winner and 10 th placed competitor) has progressively improved (time differences have decreased) for international races over the past two decades for both males and females, with performance density now very similar for both sexes. For age-group triathletes, sex differences in total triathlon performance time increases with age. The possible difference in age-related changes in the physiological determinants of endurance and ultra-endurance performances between males and females needs further investigation however. Non"
1,inserm-00851058.training.tei.xml,1,"Triathlon is a unique endurance sport that combinesthree disciplines (swimming, cycling and running) over a variety of distances.  Triathlon first appeared at the end of 1970""s and in only one-quarter of a century has developed into a well-organized sport with global participation."
2,inserm-00851058.training.tei.xml,2,"The first Hawaii Ironman triathlon consisting of a3.8km swim, 180km cycle and 42km run had 12 male participants in 1978. Currently, more than 1700 triathletes , most of whom have to qualify at one of 25 Ironman triathlons worldwide, participate in this event which has become the Ironman World Championship.  Short or Olympic distance triathlons (1.5km swim, 40km cycle and 10km run) have also rapidly grown in popularity during approximately the same period with the first World Championship being held in Avignon (France) in 1989. Triathlon was officially accepted into the Olympic games as a full sport in Sydney in 2000.  Ultra-triathlons, consisting of distances greater than Ironman distance include double Ironman triathlons (7.6km swim, 360km cycle and 84 km run) and decaIronman triathlon (38km swim, 1800km cycle and 420 km run) also appeared during the 1980""s with the first double Ironman triathlon held in 1985 in Huntsville (USA).  More recently, in addition to conventional road based triathlon, off-road triathlon combining swimming, mountain biking and trail running has also established itself in the field of endurance sports, and has grown very rapidly in popularity.  Triathlon performance has been studied from numerous perspectives by a number of research groups over approximately the same period that the sport has grown. Research has investigated physiological  , biomechanical  , training  , nutritional  or medical  aspects of triathlon performance. Although Ironman triathlon is a relatively new ultra-endurance event, recently elite Ironman triathletes appear to have reached their performance limits  as has been observed in more traditional sports such as marathon running where performance times have plateaued.  With the increase in popularity of endurance sports worldwide, endurance sports have attract a greater participation of female and masters athletes (age >40 years) during recent times.  This review addresses the specific aspects of: i) sex differences and ii) age-related declines in triathlon performance. Sex differences in performance will be presented and discussed for each of the three disciplines and total event times for different distances and for both elite and age group triathletes. Age-related declines will be outlined by focusing on the performances of the best master triathletes because these athletes represent a unique model for studying the effects of high levels of physical training in persons of older age."
3,inserm-00851058.training.tei.xml,3,"Independent of age, the number of females competing in triathlon has increased progressively since the1980""s. For example, between 2000 and 2011, female USA Triathlon membership has grown from 27 % of the total number of annual members to more than 38 %.  Factors leading to this growth include society""s increasing acceptance of ""active"" females; females feeling more comfortable living an active lifestyle , and the growth of females-only events, good examples of which are the ""Danskin"" and ""Trek Triathlon"" Series in USA."
4,inserm-00851058.training.tei.xml,4,"The rates of female participation appear to decrease with an increase in distance of triathlon races. For example, in2010 females accounted for 26%, 19% and 13% of athletes competing at short, half-Ironman and full-Ironman distance triathlons held in the region of Zurich (Switzerland)  . However, for ultra-triathlons, female participation remains relatively low, representing generally less than 10% of overall participant numbers.  The number of females finishing the Hawaii Ironman World Championship increased from 20 in 1981 (6% of participants) to more than 470 in 2010 (27 % of participants).  In Europe, there was also a progressive rise in the number of female Ironman finishers but overall rates were lower. In 2011, females accounted for 13% of the field at Ironman Switzerland.  Lower rates of female participation in Europe may because European Ironman triathlon is relatively younger compared to the Hawaii Ironman triathlon. Indeed, the first Hawaii Ironman was held in 1978, while one the first European Ironman, Ironman Switzerland, was held in 1995. Female participation in Ironman triathlon remains slightly lower compared to traditional endurance events such as marathon running, but greater than ultra-endurance events such as a 161-km ultramarathon. For example, Lepers & Cattagni  showed that the relative participation of females at the New-York City Marathon increased over the last 30 years from 17% to 33% of the total field. In contrast, females accounted for 20 % of the finishers in the same time period at 161-km ultramarathons in North America. "
5,inserm-00851058.training.tei.xml,5,"Physiological and morphological characteristics may account for the sex differences seen in triathlon performance.Even though there is a lesser amount of data for females compared to males, studies have shown that maximal oxygen uptake ( V  O 2max ), lactate threshold and running economy interact similarly in females as determinants of endurance performance as they do in males.  Current explanations for sex differences in V  O 2max among elite athletes, when expressed relative to body mass, provide two major findings.  First, elite females have more  body fat than males.  Indeed, much of the difference in V  O 2max between males and females disappears when it is expressed relative to lean body mass.  Second, the hemoglobin concentration of elite athletes is 5-10% lower in females than in males.  Concerning lactate threshold, there is no reason to believe that values should be lower in females than in males because mitochondrial adaptations in the skeletal muscles of highly-trained male and female athletes appear to be similar.  Finally, the average oxygen cost to run a given speed, (i.e. running economy) by groups of elite male and female athletes is similar and appears to play the same role in determining success in endurance performance. Therefore, the major physiological reason that explains the slower record performances by females compared to males is probably the lower V  O 2max values observed in females."
6,inserm-00851058.training.tei.xml,6,"Physiological differences between male and female triathletes have been infrequently investigated. The differences in the physiological responses in cycling and the energy cost of running after cycling have been compared in males and females for both elite junior and senior triathletes. These authors showed that senior females had a significantly higher cycling peak power output than their junior counterparts. Additionally, senior males had a higher ventilatory threshold than junior males whereas the ventilatory threshold was similar in junior and senior females."
7,inserm-00851058.training.tei.xml,7,"A recent study conducted on junior triathletes has suggested that morphological characteristics of triathletes have evolved since late1990""s.  In 2011, both male and female junior elite triathletes appeared more ectomorphic than their 1997 counterparts."
8,inserm-00851058.training.tei.xml,8,"Nowadays, junior triathletes are proportionally lighter, with significantly smaller flexed arm and thigh girths, and femur breadths. The junior males in2011 also had significantly longer segmental lengths and lower endomorphic values than their 1997 counterparts."
9,inserm-00851058.training.tei.xml,9,"More research is required however, to validate these findings for senior triathletes."
10,inserm-00851058.training.tei.xml,10,"Male triathletes possess a larger muscle mass, greater muscular strength and lower relative body fat compared to female triathletes. Low body fat is an important predictor variable for total time performance in triathlon. For example, Knechtle et al.  showed that low body fat was associated with faster race times in male Ironman triathletes but not in females. Males possess on average 7-9% less percent body fat than females, which is likely an advantage for males. Therefore, it appears that sex differences in percentage body fat, oxygen carrying capacity and muscle mass may be major factors for sex differences in overall triathlon performance. Menstrual cycle, and possibly pregnancy, may also impact training and racing in female athletes, factors that do not affect males. "
11,inserm-00851058.training.tei.xml,11,"Sex differences in endurance and ultra-endurance (>6h)  performance have received considerable attention over the past few decades, but the majority of studies have focused on running performances.  Interestingly, Speechly et al.  reported that females, who were matched with males for a 42-km run, were faster than males in a 90-km race. Similarly, it has been shown that females and males who were matched for 50-km trail running performance also performed similarly in trail runs of 80-and 161-km distances.  Despite the suggestion in 1992 that females may one day outrun males in competitive ultra-endurance events  , elite males appear to run approximately 10-12% faster than elite females across all endurance running race distances up to marathon, with the sex difference narrowing as the race distance increases.  However, at distances greater than 100km, such as the 161-km ultramarathon, the difference seems even larger, with females 20-30% slower than males. Insert TABLE 1"
12,inserm-00851058.training.tei.xml,12,"Sex differences in triathlon performance have previously been described for elite triathletes and for non-elite triathletes of different age groups  for long distances (distances greater or equal to half-Ironman)  and for off-road triathlon  . Surprisingly, very little data is available for short distance triathlons especially for high-level international races. One reason may be that the top international short distance triathlons, (i.e. World Championship Series events or Olympics) have all been draft legal for several years and therefore it is difficult to find a reference of high level Olympic distance triathlon without drafting for comparison. The increase in nondrafting international distance triathlons such the Hy-Vee triathlon that offers significant prize money for both male and female winners and represents a highly competitive platform for professional triathletes should help to better determine sex differences in short distance triathlon performance when athletes compete on a level (non-drafting) playing field in the future. Table 2 presents the most recent data for sex differences in triathlon performance for international elite triathletes participating in the most competitive races at each triathlon distance. By 2012, values narrowed so much that, for the top 10 triathletes overall in three top level international road-based triathlon races, the difference between males and females was 11.3% for the Hawaii Ironman triathlon, 14.1% for the Olympics and 9.3% for the Hy-Vee short distance triathlon (see Table 2). Non road-based triathlon produced a greater sex difference (discussed in more detail below). Interestingly, for ultra-triathlons, it has been shown that with increasing length of the event, the best females became relatively slower compared to the best males.  Indeed, if the world""s best performances are considered, males were 19% faster than the females in both Double and"
13,inserm-00851058.training.tei.xml,13,"Triple Ironman distance, and30% faster in the Deca-Ironman distance.  However, nonphysiological factors may have contributed to these observations, in particular a fewer overall number of female finishers in ultra-triathlons, compared to formats of more standard length. 2.4 Sex differences in triathlon swimming performance"
14,inserm-00851058.training.tei.xml,14,"The average sex difference in swimming performance during triathlon for race distances between1.5 and 3.8 km ranged between approximately 10% and 15% for elite triathletes (see Table 1). The sex difference in triathlon swimming performance is consistent with values found for different pool swimming events. It has been shown that sex differences in pool swimming performances become progressively smaller with increasing distance from.  This seems also to be the case for triathlon as Table 1 shows that, the longer the distance of the swim component of a triathlon race, the smaller the sex difference, particularly when the top 10 age group athlete performances are considered. In ultra-endurance outdoor swimming events, female swimmers have tended to reduce the gap with their male counterparts. For example, time differences between male and female swimming records are 6.7% for the 32-km ""English Channel Swim"" and 2.3% for the 26-km ""Marathon Swim in Lake of"
15,inserm-00851058.training.tei.xml,15,"Zurich"" in Switzerland, respectively. However, the sex difference in performance between the best male and female ultra-swimmers is more generally close to 11-12%, which corresponds to values observed for swimming in triathlon.  It has been shown that the elite male and female triathletes completed the 3.8-km swim stage of the Hawaii Ironman triathlon ~ 10% slower than the elite swimmer specialists for the same distance at the Waikiki Roughwater Swim race.  This difference could be explained by better propelling efficiency in elite swimmers compared with elite triathletes.  Analysis from 2005 showed that the sex difference in triathlon swimming appears lower than in cycling and running at the Hawaii Ironman triathlon and at the off-road triathlon Xterra World Championship.  The difference between swimming and the two other disciplines could be explained in part by the biological difference in relative body fat (7-9 % higher in females).  Performance in activities that involve supporting ones weight, for example running, may be more susceptible to a greater body fat percentage compared to water-based events such as swimming, during which body fat can increase buoyancy and therefore improve performance.  Buoyancy is also improved in females through a lower ""underwater torque"", which can be defined loosely as the tendency for the feet to sink.  In addition, in contrast to running where the energy cost appears to be similar between females and males, the energy cost of freestyle swimming has been shown to be significantly higher (i.e. lower economy) in males compared to females.  At a velocity of 1 m/s, there are differences in drag force and coefficient of drag between males and females.  The energy cost of swimming depends essentially on the propelling efficiency of the arm stroke and hydrodynamic resistance, but it has been suggested that differences in energy costs of swimming between sexes are mainly to be attributed to differences in hydrodynamic resistance.  In contrast, differences in energy cost of swimming across ages may be attributed also to changes in the propelling efficiency of the arm stroke.  Females have smaller body size (resulting in smaller body drag), smaller body density (greater fat percent) and shorter lower limbs, resulting in a more horizontal and streamlined position and therefore a smaller underwater torque. "
16,inserm-00851058.training.tei.xml,16,"Sex differences in triathlon cycling vary from12 to 16% according to the level of expertise of participating triathletes for road-based triathlons (Table 1). Data gained from analysing male and female participants"" performances in the 180-km cycling leg of an"
17,inserm-00851058.training.tei.xml,17,"Ironman triathlon are unique datasets as official time-trial road-cycling championships generally take place on distances much shorter than180-km. Moreover, there is a paucity of data concerning sex differences in road or track cycling performance.  In track cycling, where females are generally weaker than males in terms of power/weight ratios, the performance gap between males and females appears to be constant and independent of the race distance from 200 to 1000 m.  In ultra-cycling events, such as the ""Race Across America"", sex difference in performance was around 15% among top competitors.  Greater muscle mass and aerobic capacity in males, even expressed relative to the lean body mass  , may represent an advantage during long distance cycling, especially on a relative flat course such as Ironman cycling where cycling approximates to a non-weight-bearing sport. Indeed, it has been shown that absolute power output (which is greater for males than for females) is associated with successful cycling endurance performance because the primary force inhibiting forward motion on a flat course is air resistance.  Interestingly, for elite triathletes the sex difference in mountain bike cycling during off-road triathlon  is greater compared to cycling sex differences in conventional roadbased events.  Mountain biking differs in many ways from road cycling. Factors other than aerobic power and capacity such as off-road cycling economy, anaerobic power and capacity, and technical ability might influence off-road cycling performance.  Bouts of high intensity exercise frequently encountered during the mountain biking leg of off-road triathlon  can result from: 1) having to overcome the constraints of gravity associated with steep climbs, 2) variable terrain necessitating wider tires and thus greater rolling resistance, and 3) isometric muscle contractions associated with the needs of more skilled bike handling skills, not so often encountered in road cycling. In particular however, lower power-to-weight ratios for females compared to male triathletes inevitably leave them at a disadvantage during steep climbs.  Moreover, the increased bike handling skills, required especially during downhill mountain biking may be impaired by relatively lower female arm or leg isometric muscle strength, meaning that greater sex differences in off-road triathlon could be attributed to decreased ability to control the bicycle in females. However, this assumption needs to be confirmed with specific investigations of the effect of sex on technical ability in mountain biking.  As percentage differences in time do not equate to percent differences in power output, due to non-linear relationships between speed and power output from air or water resistance  , the magnitude of the sex difference has also been examined by calculating the percentage difference between males and females in estimated power outputs for each discipline.  In this case, differences in estimated power output between the sexes are greater for triathlon cycling than for the swimming and running disciplines. For example, sex differences in power output in swimming, cycling and running were estimated to be 28%, 39% and 33%, respectively for Ironman triathlon, and 30%, 45% and 33% for off-road triathlon.  Because power output is proportional to oxygen uptake, the magnitude of sex differences in power output provides a more realistic representation of underlying sex differences in physiological capacity. 2.6 Sex differences in triathlon running performance"
18,inserm-00851058.training.tei.xml,18,"During the1988-2007 period, the top 10 elite males have run the Hawaii Ironman marathon on average 13.3% faster than the top 10 females.  In contrast, during the same period, elite female triathletes have improved their marathon running times by 0.8 min/yr while times have remained stable for the males. While these improvements in female performances compared to males are impressive, they remain as yet unexplained, which is strange when it is considered that females have benefited from similar training and nutritional advances. If females continue to improve their running performance at Ironman, they could reduce the sex difference in the marathon leg and therefore their overall performance times."
19,inserm-00851058.training.tei.xml,19,"For example, at the2012 Hawaii Ironman, the difference in marathon times between the top 10 elite males and the top 10 elite females reduced to 7.3 % . Another notable example is that the female winner of the famous Roth"
20,inserm-00851058.training.tei.xml,20,"Ironman (Germany) in2011 ran only 2.6% slower than the winning male . At the same race, both female and male winners bettered the Ironman distance performance world record .  Thus, it appears that the marathon running leg of Ironman triathlon has become the discipline in which female elite triathletes have most reduced the gap with their male counterparts. Interestingly, Lepers  showed that for the top 10 elite finishers the sex difference in running at the Hawaii Ironman marathon was similar to that recorded during the New-York marathon, suggesting above all, that the swim and cycle legs of the triathlon do not exacerbate the sex difference in running a marathon. Thus, the physiological differences between males and females in running performance that have been identified to occur in a regular marathon still persist in the marathon of an Ironman."
21,inserm-00851058.training.tei.xml,21,"Contrary to the knowledge of running performance differences between sexes for the marathon distance, few analyses have been conducted between males and females who have run10-km during an Olympic distance triathlon. A mean sex difference of 17% was found for the Zurich Switzerland Olympic distance triathlon during the 2000-2010 period.  However, as Table 2 shows, when data from 2012 only is considered, there is a considerable difference even within elite level Olympic distance run performances, between males and females. A difference of 12.3% existed during the draft-legal Olympic short distance race in London, whereas the difference was lower (8.6%) at the Hy-Vee international triathlon in Des Moines (draft-free). It would appear that the format of the triathlon (drafting or not) affects males differently to females, with the possibility that males benefit in terms of their subsequent running performance off the bike, to a greater extent when riding in a peloton. However, running performances in males and females need to be more closely analyzed in relation to the effort produced during the cycle leg of an Olympic distance triathlon."
22,inserm-00851058.training.tei.xml,22,"Performance density (i.e. the time difference between the winner and 10 th placed competitor) has been quantified by considering overall performance times in triathlon. For example, at the Hawaii Ironman triathlon between 1981 and 2008, the average time difference between 1st and 10 th place was smaller for male (5.8%) than for female athletes (7.5%).  During the past five years, the performance density has decreased , a trend which would suggest that in the future, high performing female athletes may be as performance dense as the males. In fact, Table 2 indicates that in 2012 performance density was similar across the two sexes for top international road-based triathlon events, regardless of format (Hawaii, Olympics and Hy-Vee). A notable exception to this was the Xterra World Championships (off-road triathlon) in which female performance density (12.2%) was considerably lower than the male performance density (4.6%). As explained earlier (section 2.5), this can likely be explained by the specific physiological and technical abilities needed for the mountain bike leg of Xterra, in particular explosive strength which favours males over females.  The small performance density observed for the London Olympics compared to other events, for both males and females, may be explained by the drafting component of the cycling leg that led to a grouped start of a number of athletes (approximately half of the field) at the beginning of the run leg. It must also be considered that a number of nations fielded >1 athlete, and identified a designated leader who was ""protected"" during the swim/cycle legs. This would also have led to a higher overall athlete density in terms of overall performance times in both males and females."
23,inserm-00851058.training.tei.xml,23,"Physiological, (e.g. lower muscle strength and oxygen carrying capacity), morphological, (e.g. greater percentage of body fat, lower muscle mass), and functional capacities are well known to change with advancing age in both males and females. In addition, is has been shown that, after 55 years of age, the decline in endurance performance is more pronounced in females than in males. [ years.  The occurrence of this large sex difference was somewhat earlier with respect to athletes"" age than recorded during the Hawaii Ironman triathlon. Possible explanations for this include the greater competitive level of older participants at the Hawaii Ironman triathlon because athletes are required to qualify as it is a World Championship event."
24,inserm-00851058.training.tei.xml,24,"There is a need for further research to understand if exercise duration exerts an influence upon sex differences across older age groups by analysing performances of athletes in a wider range of triathlon events, from Sprint distance, through Olympic distance and Halfironman, to Ironman distances. The exact reasons for these sex-related differences with advancing age are currently not clear but may result from physiological, sociological and psychological changes.  For example, a greater decline of one or more physiological determinants of endurance performance for females compared to males, (e.g. V  O 2max , lactate threshold, or exercise economy), or a difference in age-related changes in body composition (increase in percentage body fat and loss of muscular mass), hormonal changes, and fluid balance changes (e.g. decline in the thirst mechanism), could affect triathlon performance.  In addition, differences in terms of years of training, training volume and intensity between elderly male and female triathletes performing Ironman triathlon may exist, but further research is needed to clarify this. However  This participation difference may diminish over the next couple of decades as has been observed in marathon running , such that because the welltrained females move up to the older age groups, the improvement of the oldest females may actually surpass the oldest males."
25,inserm-00851058.training.tei.xml,25,"Age-related declines in endurance and ultra-endurance performance have been well described in the literature for running , cycling  , swimming  and more recently, for triathlon.    The greatest declines in endurance and triathlon performance occur after the age of 70 years (Table 3)."
26,inserm-00851058.training.tei.xml,26,"Physiological factors contribute to age-related declines in endurance performance in older athletes and there is obviously an interaction between training behaviour and performance in older athletes. It has been found that the age of peak performance in Ironman triathlon is around 33-overall performance of marathoners.  However, this finding observed for the Ironman Switzerland triathlon needs be confirmed for other Ironman events. For non-elite triathletes, the fastest race times are usually achieved between 25 and 44 years for both Ironman and Ultra-triathlons. "
27,inserm-00851058.training.tei.xml,27,"Total triathlon performance decreases progressively in a curvilinear manner with advancing age. However, there is a smaller age-related decline in cycling performance compared to running and swimming performances for both short and long distance triathlons. These findings suggest that age-related declines in endurance performance are specific to the mode of locomotion, although the cause for such modespecificity is not clear. Where does the focus of this locomotor mode-specificity with age lie? Our group has previously proposed that mechanical power could explain these agerelated differences in cycling and running. According to the formula P = k.V (k: constant), mechanical power output (P) is dependent upon velocity (V) during running, whereas it is dependent upon the third power of velocity during cycling (P = k.V 3 )."
28,inserm-00851058.training.tei.xml,28,"Following this, we previously suggested that as changes in aerobic capacity with age are tied to reductions in P, those reductions during running and cycling with age would give rise to lower cycling velocities than running velocities. A number of explanations can be given to explain the smaller decline in cycling performance during triathlon compared to the declines in the other two disciplines. These include a lesser reduction in lactate threshold or economy during cycling, or a greater muscular fatigue during running with age, although these propositions remain to be validated. Some authors have though, attempted to explain this age-related phenomenon in cycling by proposing that the ""training stimulus"", the ability or will to train, is reduced in running compared to cycling.  In particular, this may be because running is associated with a greater amount of orthopedic injuries, which limits the ability to train in running. The training stimulus would therefore be maintained in cycling due to the less traumatic nature of the endurance activity and therefore with age, triathletes would tend to cycle more than run."
29,inserm-00851058.training.tei.xml,29,"However, this proposition remains speculative and requires further investigation."
30,inserm-00851058.training.tei.xml,30,"The duration of a triathlon race exerts an important influence on the age-related changes in triathlon performance.Age-related declines in swimming performance are not influenced by triathlon duration with the magnitudes of decreases in swimming performance similar for Olympic distance versus Ironman triathlon. However, in contrast to swimming the magnitude of the declines in cycling and running performances with advancing age during"
31,inserm-00851058.training.tei.xml,31,"Olympic distance triathlon were less pronounced than during Ironman. For age group competitors of 70-74 yrs, total finishing time is approximately three hours for the Olympic distance triathlon whereas it is around 15 hours for an Ironman (Table 3). Certainly the Ironman triathlon induces greater neuromuscular fatigue in cycling and running compared to the Olympic distance event.  Furthermore, muscle damage during a 10-k run of the Olympic distance triathlon is limited compared to that which occurs during an Ironman marathon. Greater muscle fatigability and greater sensibility to muscle damage of older triathletes needs further investigation to determine if these factors underlie the greater declines in cycling and running performance of older triathletes."
32,inserm-00851058.training.tei.xml,32,"It has recently been shown that the rate of the decline in performance for off-road triathlon is greater than for road-based triathlon. This suggests that the type of discipline (road versus mountain bike cycling and road versus trail running) exerts an important influence on the magnitude of the age-associated changes in triathlon performance.  The specific aspects of mountain biking and trail running may explain why age-related declines in off-road triathlon are more pronounced for off-road compared to conventional on road triathlon. In particular, a decrease in power-to-weight ratio that has been shown to occur with age  leaves older athletes with a distinct advantage during steep climbs often encountered in off-road triathlons. Moreover, despite a preservation of muscle strength in comparison to sedentary persons  , older athletes may be disadvantaged in their technical bike handling skills due to lower arm/leg muscle strength. Poorer running performances in older athletes, especially when variations in intensity are required, may be explained be a reduced ability to modify the biomechanical components of running (e.g., support phases, stride frequency, changes in stride length, etc.) Insert TABLE 3"
33,inserm-00851058.training.tei.xml,33,"An increase in participation of both male and female athletes older than40 yrs over the past few decades has been reported for marathon and ultra-marathon running, such as 161-km ultra-marathons and for 100-km running in Switzerland.  A relative increase in participation of masters triathletes has also been observed during the past decade for short and long distance triathlon, while the participation of triathletes younger than 40 yrs of age has decreased.  The relative increase in participation of master triathletes at the Ironman distance triathlons has been accompanied by an improvement in their performance.  We have previously presented data for the 1986-2010 period showing that swimming, cycling, running and total time performances at the Hawaii Ironman Triathlon improved for male triathletes older than 44 years age and female triathletes older than 40 years age.  "
34,inserm-00851058.training.tei.xml,34, Similar findings have been observed for a Hawaii Ironman qualifying race such as the Switzerland
35,inserm-00851058.training.tei.xml,35,"Ironman. In Table 3, we show that, when the most recent data (Ironman Hawaii, 2011 and) is added, improvements of older age groups, especially for the males has continued, and for some age groups, is quite startling. For example, the best male finisher in the 70-74 age group has bettered the best time set in 2010  by almost one hour ."
36,inserm-00851058.training.tei.xml,36,"  What is clear however, is that, if the most recent (2012) data is compared to that of 1986, age groups show improvements of between 114% and 194 % for the males over 45 years, and 106% and 179%"
37,inserm-00851058.training.tei.xml,37,"for females over45, and that these improvements seem to be continuing, most significantly for the oldest athletes."
38,inserm-00851058.training.tei.xml,38,"Several reasons may explain the improved performance of master triathletes. These include an improvement of training facilities, coaching, training techniques, nutritional strategies and equipment, as well as the possibility that they have had better and prolonged access to the facilities required to train effectively. It is also obvious that the higher participation rates of master athletes increases the possibility of them obtaining better results due to the competitive nature of the sport. Additionally, athletes that have attained a number of good results and positive outcomes, have increased levels of motivation to train and compete in endurance events.  Accordingly, the better physical condition of older athletes is likely to increase competitive spirit, participation and performance.  The advancement of training quality is also a possible reason for these improved performance trends. Earlier studies suggested the performance decline with advancing age was due to decreased training volumes and intensity.  Other, more sociological, factors may also play a role in reducing the ability and motivation to train. These include greater work demands, demands of the family, a diminished inherent drive to push oneself or even train.  An important factor is also the greater amount of time needed by the body to recover from hard, physical effort resulting from training as humans age. Indeed, training for an Ironman triathlon is very demanding and in order to place in the top ten of an age group category, extremely high training volumes and intensities are required.  However, it has been suggested that masters athletes could optimize their quality of training, so they could reduce their training volume to save time for adequate recovery and remain injury free.  These studies suggest that masters triathletes have probably not reached their limits in ultra-endurance performance. Unfortunately, no information is available concerning the sporting background of these successful masters triathletes. Nevertheless, even if recreational masters athletes had relatively short training histories and little experience in triathlon, these successful athletes probably have lifelong histories of physical activity. "
39,inserm-00851058.training.tei.xml,39,"Sex differences in triathlon performance differ generally between the locomotion modes with lower differences seen in swimming compared to cycling and running for both elite and non-elite triathletes. In elite triathletes, sex differences in performance (~10-14%) are in agreement with values generally observed in endurance sports. At present, the performance density in highly competitive international triathlon races appears to have become similar between elite male and female triathletes. Marathon running during an"
40,inserm-00851058.training.tei.xml,40,"Ironman triathlon appears to be the discipline where female elite triathletes have reduced the gap with the males to the greatest extent over the recent years. Sex-related differences increase with advancing age most likely due to physiological, sociological and psychological changes. However, these differences should decrease in the future with the increase in participation of elderly female triathletes."
41,inserm-00851058.training.tei.xml,41,"Age-related declines in triathlon performance depend on the locomotion mode, the exercise duration (short versus long distance triathlon) and the triathlon format (off-road versus road based triathlon)."
42,inserm-00851058.training.tei.xml,42,"Sex difference in performance in top10 athletes in 2012 (mean ± SD) Difference between 10th and 1st (%) Table 2 Mean percentage differences in times for swimming, cycling, running and total event between the top 10 females and males and time difference between the winner and 10 th placed athlete in 2012 at four international triathlons: Hawaii Ironman triathlon World championship (Kona, Hawaii, USA Table 3 Total time records (actualized with 2012 data) and corresponding split times for male and female age-groups at the Hawaii Ironman Triathlon between 1986 and 2012. The change in percentage compared to the fastest total time (age group 18-39 years) is given for each different age group."
43,inserm-00851058.training.tei.xml,43,This research project was not funded. All authors declare that there were no conflicts of interests.
44,hal-00643787.training.tei.xml,0,"An allergy is an abnormal reaction of the immune system towards foreign substances (allergens) that are normally harmless. Peanut allergies in particular affect more than0.5% of the entire French population, and its increasing prevalence and potentially severe clinical reactions make it a public health problem. It is also the most lethal food allergy [4]. Following a strict avoidance diet is currently the only effective treatment that minimises potentially lethal accidents."
45,hal-00643787.training.tei.xml,1,"Diagnosing and scoring peanut allergies is currently performed with a double blind placebo controlled food challenge (DBPCFC). Patients are given increasing peanut doses until the first clinical reaction appears. Those showing specific allergy symptoms are declared allergic, and a particular avoidance treatment is then initiated. DBPCFC is also used to judge the severity of an established peanut allergy by determining the cumulative dose that triggers the first reaction, known as the eliciting dose [in milligrams (mg)]. However, these tests require patient hospitalisation in specialised centers and can potentially result in life-threatening reactions from patients with severe allergies. The DBPCFC is also a costly and time consuming test to conduct. The severity of peanut allergies is usually scored using the following scale :"
46,hal-00643787.training.tei.xml,2,• Score 1: Mild symptoms among : abdominal pains that spontaneously resolve under30 minutes and/or rhinocunjunctivitis and/or urticaria < 10 papulas and/or a rash (eczema onset);
47,hal-00643787.training.tei.xml,3,• Score 2:One moderate symptom among : abdominal pain requiring treatment or generalized urticaria or non-laryngeal angioedema or cough or fall of Peak Expiratory Flow between 15 and 20%;
48,hal-00643787.training.tei.xml,4,• Score 3:Two moderate symptoms in the preceding list;
49,hal-00643787.training.tei.xml,5,• Score 4:Three moderate symptoms in the preceding list or laryngeal oedema or hypotension or asthma requiring treatment;
50,hal-00643787.training.tei.xml,6,• Score 5: Any symptom requiring hospitalisation in intensive care.
51,hal-00643787.training.tei.xml,7,"For an already diagnosed allergy, it would be much more advantageous to predict the severity of the reaction from accidental exposure by using a blood sample or cutaneous test. This would replace the DBPCFC test with a simple statistical tool that can still evaluate potential risk without exposing the patient to a life-threatening allergic situation. Such a diagnostic method would be a major advance in food allergies and be beneficial to both patients and clinicians. The first objective of this paper was to select a set of discriminant variables which can offer useful information about the severity of peanut allergy. These variables could provide biologists and allergists a better understanding of the mechanisms inducing allergic reactions. Moreover this will allow to avoid measuring useless variables in further studies."
52,hal-00643787.training.tei.xml,8,"The second goal of this study was to predict the DBPCFC score, the eliciting dose and the first accidental exposure score, evaluated a posteriori with the patient's medical record according to the same scale as the DBPCFC score. The first accidental exposure score would then reveal the ""real"" severity of the allergy. Compare this to using the DBPCFC, which only offers a minimal view of the severity since the procedure is terminated once the first symptoms appear."
53,hal-00643787.training.tei.xml,9,"A clinical study was performed using76 allergic patients with ages from 3 to 18 years. Tables 1 and 2 describe the frequencies observed for DBPCFC score, the first accidental exposure score and the eliciting dose. Note that only 47 out of the 76 patients experienced a first accident. The remaining 29 patients were diagnosed during an allergy check-up and subsequently confirmed by DBPCFC, thus avoiding further accidents. Patients were homogeneously distributed in age and sex across severity scores. Thirty-four variables were measured to reveal the presence of Immunoglobulins of type E (IgE) antibodies. These are proteins produced by the immune system that can elicit allergic reactions . Each antibody is specific to an allergen, i.e., it is coded to identify a particular protein for elimination. We measured the levels of IgE for the proteins of interest with the goal of building a predictive model of allergy severity. The variables used to test for IgE were measured either by immunoassays or by Skin Prick Tests (SPTs)."
54,hal-00643787.training.tei.xml,10,"Immunoassays are biochemical tests that quantify the level of antibodies in a blood sample (in kilo-units per liter ). We performedsix immunoassays aimed at measuring the following: the total IgE, the specific IgE to peanut (f13), and the specific IgE to recombinant (r)Ara-h1, rAra-h2, rAra-h3, rAra-h8, which are IgE especially directed against peanut recombinant major allergens ."
55,hal-00643787.training.tei.xml,11,"SPTs are used to detect an immunological sensitivity to a particular substance. They show the functional aspect of cellular IgE, which are linked to mast cells releasing chemical mediators that elicit symptoms. A small dose of allergen is applied under the skin by pricking with a needle, and the diameter of the resulting wheal is measured in millimeters. We also measured the diameter of prick-tests to codeine as a positive control showing the basal reactivity of the skin. The ratio of the two diameters is used to measure the allergen reaction."
56,hal-00643787.training.tei.xml,12,We performed prick-tests for28 allergens divided into three families:
57,hal-00643787.training.tei.xml,13,"1.11 nuts: almond, Brazil nut, cashew nut, chestnut, hazelnut, peanut, pecan nut, pine nut, pistachio, Queensland nut, walnut, which are often related to peanut allergies by cross-reactivity;"
58,hal-00643787.training.tei.xml,14,"3.10 aeroallergens: 12 grass pollens, Alternaria, ash, birch, cat epithelia, dog epithelia, Dpte (Dermatophago¨ıdesDermatophago¨ıdes pteronyssinus), mugwort, ribwort, rape seed, which are the common clinical allergens."
59,hal-00643787.training.tei.xml,15,Immunoassays and SPTs were measured immediately before DBPCFC.
60,hal-00643787.training.tei.xml,16,All computations were performed using the SAS Enterprise Guide 4.1.0.471 R or R 2.7.0.
61,hal-00643787.training.tei.xml,17,We first performed a Principal Component Analysis (PCA) to gain an overview of the data.
62,hal-00643787.training.tei.xml,18,"To solve our problems, discriminant analyses of DBPCFC score, first accident score and eliciting dose were performed by using several classifiers.Two studies were performed for each measure of severity by treating it as a four-class variable, and then as a two-class variable. For DBPCFC and first accidental exposure, 4 classes were built by considering the score groups {1}, {2}, {3} and {4, 5} because of the low frequency of score 5. For the two-class discrimination, groups were formed by scores of either {1, 2, 3} or {4, 5}, as recommended by clinicians. Since eliciting dose values are fixed by levels by the clinician , its measure is not a continuous variable and cannot be predicted by a regression analysis. Moreover although eliciting dose could be considered as a class variable, a discriminant analysis cannot be directly performed because of numerous categories with low frequencie (Table 2). A first solution consisted in converting eliciting dose into a four or two-class variable by searching the best discriminated classification computed with all available variables. A second solution will be proposed in Section 3.3. Careful variable selection appeared especially as a major point of the analysis. Therefore three different statistical approaches were proposed for each measure of severity :"
63,hal-00643787.training.tei.xml,19,"The performances of several classification rules were first compared without preselecting variables. Linear Discriminant Analysis (LDA), k-Nearest Neighbors (k−NN), Classification And Regression Trees (CART) and AdaBoost with CART  were performed using all the 34 available variables as predictors. k−NN were performed for k ∈ {1,. .. , 5} and the number of nearest neighbors giving the best results was kept."
64,hal-00643787.training.tei.xml,20,"Since in supervised learning keeping noisy predictors can increase the misclassification error,two methods that simulteanously perform variable selection and classification were also used : stepwise logistic regression  and penalized SVM ."
65,hal-00643787.training.tei.xml,21,"As explained earlier, the determination of a set of predictors to keep is one of the main points of the study. Thus a variable selection scheme independent from classification was also developed. Variables were retained in the model if either the corresponding p-value of the Kruskal-Wallis test was smaller than 0.10, or if the variable was selected by the stepwise Wilks' lambda (Λ) criterion . The Λ statistic was computed at each step with all variables already present in the model, whereas the F − to − enter statistic and corresponding p-value measure the discriminant power of a variable added to the preceding ones. For the latter, the maximal F − to − enter p-values used as entry and removal criteria were set by default to 0.15, as recommended by [7]. The nonparametric Kruskal-Wallis test was preferred to ANOVA, because variables were not always normally distributed in the classes induced by the scores. Note that the Wilks' lambda selection is based on the hypotheses of multi-normality of the variables vector distribution and equality of the within-class covariance matrices. In 1975, Lachenbruch  asserted that the F −test is robust to small deviations of these hypotheses. We therefore decided to use the Wilks' lambda selection even though variables were not always normally distributed in the classes induced by the measures of severity. This variable selection scheme is a compromise between the assessment of variable marginal importance and the detection of a discriminant subset of predictors. This will allow first to provide biologists and allergists a list of informative variables in regards to the mechanisms involved in allergic reactions, and second to avoid keeping noisy variables that could degrade the performance of the learning algorithms. Biologists and allergists are particularly interested in using immunoassays as markers of the severity of peanut allergy because Specific IgE to rAra-h1, rAra-h2, rAra-h3 are yet known to be very useful in practice to detect peanut allergic patients . Moreover immunoassays are precise and reliable measures contrary to SPTs. But the number of discriminant SPTs can far exceed the number of selected immunoassays, which could possibly smear out the signal brought by immunoassays. As we wanted nevertheless to keep the information provided by SPTs, a Multiple Factorial Analysis (MFA)  was performed to equalize the influence of both groups of selected variables, which enabled the use of factors as new predictors of severity. Thus we performed two discriminant analyses :"
66,hal-00643787.training.tei.xml,22,"1. by using directly the discriminant variables as predictors for classification with the methods introduced in Section 3.1.1, 2. by computing MFA factors of the discriminant variables and selecting a limited number of discriminant factors by the same selection process as the one used with raw variables, before performing the classification rules."
67,hal-00643787.training.tei.xml,23,"The overall specific statistical approach is summarized in Figure 1. Figure 1: Specific statistical analysis for DBPCFC and first accident scores. The left path of the analysis gives a set of discriminant predictors without allowing for the weights of both groups of variables, whereas the right one uses equally-weighted groups of predictors."
68,hal-00643787.training.tei.xml,24,"To discriminate this variable, we also devised an algorithm that simultaneously clusters the eliciting dose values and selects predictors by minimizing the Wilks' lambda (Section 3.3). This algorithm was applied by using raw variables or factors computed by MFA with the34 available variables as predictors. Once the predictors were chosen and the clusters built, the same statistical approaches as for DBPCFC and first accident scores were used. The corresponding statistical analysis is summarized in Figure 2."
69,hal-00643787.training.tei.xml,25,"MFA was introduced by B.Escofier and J.Pagèsfor sensory analysis, and is a PCA with a particular choice of metric. The aim of this method is to give a similar part to several groups of variables when determining factors, i.e., uncorrelated linear combinations of the initial variables. This procedure is useful for avoiding models that are fully influenced by a single group of numerous variables which could partially cancel the effect of the other groups. Briefly, an MFA is performed as follows : Suppose p variables are measured on n subjects and divided in q groups :"
70,hal-00643787.training.tei.xml,26,"with q k=1 m k =p, where m k is the number of variables in group k. Denote X k the matrix of data of size n × p corresponding to the k th group of variables, namely :"
71,hal-00643787.training.tei.xml,27,"n where the generic element x k,j i denotes the measure of the variable x k,j for the sample point i. Let also X = (X 1 |. .. |X q ) be the matrix corresponding to the whole set of variables. For the k th group of variables, let M k be a metric matrix in R m k , k=1,. .. , q. Let D be the diagonal matrix of the weights assigned to the sample points. The MFA algorithm is then as follows :"
72,hal-00643787.training.tei.xml,28,"• Step 1 : For any 1 ≤ k ≤ q, perform PCA(X k ,M k ,D), and denote λ k 1 the greatest eigenvalue corresponding to the first factor ;"
73,hal-00643787.training.tei.xml,29,• Step 2 : consider the metric matrix in R p :
74,hal-00643787.training.tei.xml,30,"Note that in our case, q =2 with the immunoassays as the first group of variables and the SPTs as the second. Variables were first centered and scaled to unity, and the metric matrix M k was then set to the identity matrix I k in R m k. For the DBPCFC and first accident scores, we thought it made more sense to compute the factors using only the discriminant variables rather than using all available variables. Indeed, the predictive model needed to be built with a reasonable number of characters. Even if a limited number of factors were chosen afterwards, all variables would still have to be measured to compute the factors. Moreover, a non-discriminant variable could have a large coefficient for some retained factors even though it would not improve the overall discriminative power of the model. Nonetheless, we did compute factors using all variables as well, but the results did not improve the discrimination of the first accident score. For the eliciting dose, factors were computed using all 34 available variables, not only the discriminant ones. As described in Section 3.3, the set of discriminant variables depends on the choice of the clustering of eliciting dose values and vice versa. Thus it did not seem appropriate to replace the optimal set of variables by factors."
75,hal-00643787.training.tei.xml,31,variable and selecting discriminant variables
76,hal-00643787.training.tei.xml,32,"Eliciting dose is a variable whose values are taken according to an increasing scale of fixed doses of peanut. For a given patient, only an interval including the eliciting dose is actually known. We wished first to group eliciting dose values in a limited number of intervals, and second to select the most discriminant variables for these categories."
77,hal-00643787.training.tei.xml,33,Here we propose an algorithm to perform thesetwo steps simultaneously using alternate optimization.
78,hal-00643787.training.tei.xml,34,"For a given partition in intervals, a set of discriminant variables of fixed cardinal is selected using a certain optimality criterion. A new partition in intervals is then determined to optimize this criterion with the chosen variables, and so on. In order to not repeat this algorithm for different values of the cardinal, the number of variables to include could be increased one-by-one at each step of the procedure. The optimal set of variables could then be searched into all the possible subsets of variables of fixed cardinal corresponding to this step. To avoid heavy computations, variables were included forward in the model. At each step, a new variable was chosen according to the optimality criterion and added to the preceding variables."
79,hal-00643787.training.tei.xml,35,"Since Wilks' lambda provides a non-empirical stopping rule by testing its significance, this approach was preferred over using within-class inertia computation as the optimality criterion. The p-value of F − to − enter was set to0.15."
80,hal-00643787.training.tei.xml,36,"Let y be an ordinal categorial variable of levels {m 1 ,. .. , m l }, m 1 < m 2 <. .. < m l. Suppose that we want to cluster the levels of y into a limited number r of intervals ]m i , m j ]. Only consecutive levels can be gathered. We build consecutive left-opened and right-closed intervals by selecting the upper bounds. Thus the number of possible clusterings is C r−1 l−1 ."
81,hal-00643787.training.tei.xml,37,The algorithm for computing intervals and selecting discriminant variables is as follows:
82,hal-00643787.training.tei.xml,38,• Step 1 :
83,hal-00643787.training.tei.xml,39,"1. choose the clustering C 1 of eliciting dose values that minimises Λ, computed using all34 available predictors ;"
84,hal-00643787.training.tei.xml,40,2. select the predictor v 1 that minimises Λ with the obtained clustering C 1 ;
85,hal-00643787.training.tei.xml,41,• Step 2 :
86,hal-00643787.training.tei.xml,42,"1. choose the clustering C 2 of eliciting dose values that minimises Λ, computed using the previously selected predictor v 1 ;"
87,hal-00643787.training.tei.xml,43,"2. select the predictor v 2 such that the paired predictors (v 1 , v 2 ) minimise Λ with the new clustering C 2 ;"
88,hal-00643787.training.tei.xml,44,• and so on. . .
89,hal-00643787.training.tei.xml,45,"• procedure stops if either no left predictor can improve the discriminant power of the model, i.e., if F − to − enter p−value is greater than0.15 , or if every predictor is already entered."
90,hal-00643787.training.tei.xml,46,"Note that the F −to−enter value is only computed when a new variable is entered into the model. This algorithm was used with both the variables and the MFA factors computed using all34 available variables. Since new discriminant variables could have been chosen at each step of the algorithm, there was no default starting set of discriminant variables. Moreover, it did not seem appropriate to perform the MFA at the end of the algorithm, since the selected variables were specifically chosen to discriminate the found clusters."
91,hal-00643787.training.tei.xml,47,"Linear discriminant analysis, k-NN, CART and stepwise logistic regression  are classic methods. For two-class discrimination we also used the AdaBoost algorithm with CART and penalized SVM. Since these are still recently developed algorithms, we briefly summarize their concepts below."
92,hal-00643787.training.tei.xml,48,"Let {(x i , y i ) 1≤i≤n } a dataset, where x i ∈ R p is the vector of predictors, and y i ∈ {0, 1} is a binary response variable to discriminate. The principle of the AdaBoost algorithm is to re-weight observations that were misclassified by a base classifier (CART in our case). At each step of the procedure, a new classification tree is randomly built, inducing new misclassified sample points whose weights are updated before the following step starts. The method proceeds according to the following algorithm:"
93,hal-00643787.training.tei.xml,49,"• Step 1 : assign equal weigths to all sample points w [0] i =1/n, ∀i = 1,. .. , n ; • Step 2 : for m=1,. .. , M do :"
94,hal-00643787.training.tei.xml,50,"2. classify the data by resubstitution : determinê g [m] (x i ), i = 1,. .. , n ;"
95,hal-00643787.training.tei.xml,51,3. compute the misclassification rate :
96,hal-00643787.training.tei.xml,52,where
97,hal-00643787.training.tei.xml,53,4. update the weights
98,hal-00643787.training.tei.xml,54,• Step 3 : build the aggregated classifier
99,hal-00643787.training.tei.xml,55,"A novel observation x is classified by the majority votê f AdaBoost (x), where vote m is weighted by α [m] . In this study AdaBoost was performed for M =50, 100 and 200 on the training set and the parameter value giving the best result was kept."
100,hal-00643787.training.tei.xml,56,"Let {(x i , y i ) 1≤i≤n } a training set, where x i ∈ R p is the vector of predictors, and y i ∈ {−1, 1} is the class label. The Support Vector Machine (SVM) algorithm gives the hyperplane H that best splits both groups and that is defined by the equation"
101,hal-00643787.training.tei.xml,57,"where < .,. > is the usual dot product in R p , w =(w 1 ,.. .,w p ) are the coefficients of the hyperplane and b is the intercept. The coefficients are obtained by solving the convex optimization problem :"
102,hal-00643787.training.tei.xml,58,"where λ is a positive tuning parameter, [.] + = max(., 0) is the positive part and pen λ (w ) = λw 2 2 = λ p j=1 (w j ) 2. The class label of a novel observation x is then given by sign(f (x))."
103,hal-00643787.training.tei.xml,59,"To select a limited number of variables, the term pen λ in (9) can be replaced by a penalization function being singular at the origin and having a continuous first-order derivative. Two functions were used in this study :"
104,hal-00643787.training.tei.xml,60,• Smoothly Clipped Absolute Deviation (SCAD) : pen λ (w ) = p j=1 p λ (w j ) where
105,hal-00643787.training.tei.xml,61,"where a >2 is a tuning parameter. As suggested in [2], a is set by default to 3.7."
106,hal-00643787.training.tei.xml,62,"The optimal λ was chosen by the algorithm in the set {0.05, , ,. .. , 0.100.150.95} for L 1 penalization and in {0.10, 0.20,. .. , 1} for SCAD penalization. For both penalizations, variables with coefficient |w j | lower than a given were considered useless and removed from the model. In the penalized SVM R package, is set to 0.001 ."
107,hal-00643787.training.tei.xml,63,"The PCA representation of the variables was relevant. As seen on the correlation circle of Figure 3, intra-family correlations between variables were rather high but inter-family correlations were quite low (with the notable exception of nuts and aeroallergens). Moreover, the total IgE and specific IgE to rAra-h8 did not seem closely related to the other immunoassays, an observation fully validated by clinicians. Indeed, as explained earlier, the level of total IgE is the global measure of this antibody subclass, whereas the specific IgE to rAra-h1, rAra-h2 and rAra-h3 are directed against peanut allergens alone. Also, rAra-h8 is a homologous protein to the birch pollen allergen Bet-v1, sharing about66% of their amino acid sequences. Thus patients sensitive to both peanut and birch pollen could present high values of specific IgE to rAra-h8 without being allergic to peanuts. These results confirmed clinical observations. Also, the individual representation did not provide supplementary information (data not shown), and no particular interpretation was evident for the PCA axes."
108,hal-00643787.training.tei.xml,64,"Here we show the results for the prediction of the first accidental exposure score, the DBPCFC score and the eliciting dose. Tables 3, 4 and 5 give the well-classification rates obtained by combining the classifiers with the3 different statistical approaches :"
109,hal-00643787.training.tei.xml,65,"1. direct application of the classifiers, The percentage of detected patients with high severity was also computed. According to clinicians, failing to detect patients with severe allergies could indeed lead to inappropriate food intake by the patient. Results are given including the variable selection scheme and/or MFA in the crossvalidation. For the first and the third variable selection schemes, only the classifier giving the best performances among LDA, k−NN, CART and AdaBoost is displayed. For penalized SVM, the best result between L 1 ans SCAD penalizations was kept."
110,hal-00643787.training.tei.xml,66,The variables obtained with the selection process offering the bests results are also given and summarized in Figure 4 for each measure of severity .
111,hal-00643787.training.tei.xml,67,"Four-class study Recall that in what follows, we combined scores 4 and 5 because of the low frequency of score 5. Thus the four classes considered here are for scores of {1}, {2}, {3}, and {4, 5}. Well-classification rates obtained by fourth-fold cross-validation with each statistical approach are shown in Table 3. The percentage of patients of score 4 who were correctly classified in class 4 is also given. Note that direct application of the classifiers could not have been performed since the number of variables was greater than the size of the learning sets. 5−NN combined with our specific variable selection scheme was the most performant classifier with 41% of well-classified patients, since stepwise logistic regression and penalized SVM did not give better results. Nevertheless all these results remained poor. Replacing variables by factors did not give better results than direct use of the variables. On the whole dataset 7 variables had a Kruskal-Wallis p-value lower than 0.10 (peanut, walnut, chick pea, pecan nut, broad bean, green pea, cashew nut, ordered by increasing p-value, Table 6) and 8 were retained in the Wilks' lambda selection (chick pea, specific IgE to rAra-h8, green pea, rape seed, peanut, ribwort, total IgE, specific IgE to rAra-h1, Table 7). Thus 12 different variables were selected in total (3 immunoassays and 9 SPTs). Two-class study The methodology used was the same as for the four-class study. The method which gave the best results for two-class discrimination was 1−NN with factors computed from discriminant variables as predictors (81% of well-classified sample points) (Table 3). Of the score 4 patients, 74% were correctly classified. Contrary to the four-class study, the use of factors improved the classification rates compared to the direct use of variables since using selected variables with 1−NN gave 79% of well-classification rate. The other models yielded poor results. Processing the variable selection on the whole dataset gave interesting results. Eleven different variables were selected to build discriminant factors: peanut, walnut, specific IgE to rAra-h1, specific IgE to rAra-h3, pecan nut for Kruskal-Wallis and peanut, specific IgE to rAra-h1, lupine flour, specific IgE to rAra-h2, ribwort, Dpte, birch, dog epithelia for Wilks' lambda. These immunoassays, as well as SPTs to nuts and legumes, are variables expected by clinicians. This indicates that our selection process seems to detect ""useful"" variables. More surprisingly, a few SPTs to aeroallergens were also selected. Indeed, these variables are not known to cross-react with peanuts. Note that the discriminant factors will have to be computed these variables to use the best model for further classification."
112,hal-00643787.training.tei.xml,68,"The statistical approach used to predict the DBPCFC score was the same as for the first accident score.Four-class study Scores of 4 and 5 were again grouped together due to the low frequency of score 5. Although applying CART without preselecting variables gave the best results with a 38% successful classification rate, the overall misclassification error remained high ( Table 4). Note that with the specific variable selection approach, MFA was not performed since only SPTs were entered in the model during cross-validation. Interestingly 7 variables were entered in the classification tree performed on the whole dataset : lupine flour, specific IgE to f13, lentil, total IgE, specific IgE to rAra-h3, 12 grass pollens, specific IgE to rAra-h8, in order of selection. This means that although all variables were available for building a model only a few were considered as useful by the CART algorithm. Two-class study DBPCFC scores can be gathered into two classes in the same manner as for the first accidental exposure score. Overall, using 3-NN classification with our variable selection scheme offered the best results, with 66% of successfully classified sample points and 33% of successfully classified severe patients (Table 4). Total IgE was the only immunoassay retained in the model during cross-validation. Thus, no factor was computed. Indeed, assigning the same weight to this single variable as to the other variables did not seem appropriate, because total IgE are not antibodies specifically involved in peanut allergies, but in all allergic reactions. On the whole dataset this resulted in selecting variables lupine flour for Kruskal-Wallis and almond, total IgE, lupine, broad bean, pine nut for Wilks' lambda."
113,hal-00643787.training.tei.xml,69,"Before applying directly all the classifiers without selecting variables and performing stepwise logistic regression and penalized SVM, eliciting dose was first converted into a class variable. During cross-validation values were gathered into the partition of minimal Wilks' lambda computed with all available variables. These results were compared to those obtained with our algorithm that simultaneously selects discriminant variables and groups the eliciting dose values in an optimal partition (Section 3.3). This was performed for bothfour-class (r = 4) and two-class (r = 2) studies. The minimal number of sample points in each class was set to 10, in order to have class frequencies large enough to perform cross-validation. Note that this process was included in cross-validation. Four-class study"
114,hal-00643787.training.tei.xml,70,"The eliciting dose was correctly predicted for39% of the patients using our algorithm and CART with variables. Moreover, only 52% of the patients from the lowest eliciting doses group were correctly classified. Other approaches did not give better results (Table 5) . Table 8 The algorithm stopped at step 7 because no additional improvement resulted afterwards. The selected variables were hazelnut, birch, pistachio, cashew nut, green pea, total IgE, pine nut and the bounds were 95, 215, and 500 mg. The same study was performed with factors as predictors but it did not enhance the model. Two-class study Selecting factors of all available variables with our algorithm was the most discriminant model to discriminate the eliciting dose in the two-class study (77% were successfully classified with 5−NN). It also successfully classified 72% of the highly reactive patients. The threshold of eliciting dose was 300 mg with 7 factors selected (Λ = 0.62). Nevertheless, this model cannot easily be used in practice. Indeed, as mentioned earlier, MFA was performed on all 34 variables, not just the discriminant variables. This means that to correctly predict the eliciting dose, all variables would have to be measured to compute the factors; this does not seem feasible."
115,hal-00643787.training.tei.xml,71,"To the best of our knowledge, this paper presents the first discriminant analysis of DBPCFC score, first accident score, and eliciting dose measurement of peanut allergy severity. Previous studies were aimed at finding links between immunoassays or SPTs and allergy severity, but their statistical analyses were limited to either comparing distributions between groups of patients (using, for instance, the Mann-Whitney test) or to computing linear correlation coefficients. In our approach, we used several classification rules to aid in comparing and choosing the optimal and most efficient method. It appeared that in general selecting discriminant variables by a process independent from classification gave better results. In addition, we found that using MFA to compute new predictors was an attractive solution when equalizing the weights of group variables, and we proposed a novel algorithm for simultaneously clustering the levels of ordinal qualitative variables and for selecting discriminant variables. Our work differs from earlier studies in several respects. Previous studies were performed on small sample sizes of 30 to 40 patients  using a small number of measured variables, which only permitted a limited choice of discriminating predictors. Also, specific IgE to Ara-h1,2,3 were measured by SPTs instead of immunoassays  and although a positive response to an SPT does indeed indicate allergen sensitivity, it is still less accurate than immunoassays. There are several scoring methods in the literature to evaluate peanut allergy severity. For example, Hourihane et al. devised a complex 25-class scoring system combining observed reactions and eliciting doses . A graduation of symptoms was also proposed by Müller , but this score is based on allergic reactions in response to bee or wasp venom (and not peanut allergens). Thus, developing a standardized scoring method is still necessary and would facilitate comparison studies from different centers. One possible solution would be comparing the results of Hourihane et al.'s score with the one used in this study using the same cohort of patients. Moreover, the large number of scoring levels in Hourihane et al.'s approach could be reduced by our algorithm. Nevertheless, several potential biases in our study must be noted. First, SPT diameters are relatively imprecise. There is no standard method of measuring SPT reaction since both wheal diameters and areas are popular metrics . Additionally, the score of the first accident can be inaccurate or imprecise since it depends on the medical history of the patient, and hence subject to inaccuracies in the patient's memory which may underestimate symptom severity. Other factors such as medication can affect the first reaction symptoms as well  yielding a severity score higher than it should be. Finally, the first exposure score is a past event predicted using variables measured during a subsequent DBPCFC. As mentioned in Section 4.2.2, Hourihane et al. also used such a reverse prediction with a community score that was evaluated a posteriori using the record file of the patient . The predictive models used in our study yielded correct classifications for the first accidental exposure and DBPCFC of up to 81% and 66% for the two-class study. Our algorithm also allowed us to group eliciting dose values and to select discriminant predictors, leading to an 77% classification rate for the two-class study. This indicates that it is indeed possible to correctly predict peanut allergy severity by measuring well-chosen variables. Considering that all immunoassays of specific IgE were selected once, we also hypothesize that measuring new antibodies to peanut allergens, such as those directed against rArah-6, rArah-7 and rArah-9, will further improve the discriminative power of our models. This also argues for these antibodies playing key roles in the diagnosis of peanut allergy severity. Our variable selection process also offers a new perspective on conducting allergy checkups. Indeed, some unexpected variables appeared several times in our models, such as SPTs to dog epithelia as shown in Figure 4. If future experiments could distinguish the medical relevance of this observation from cross-reactivity, then the importance of these SPTs in discriminating severity would be confirmed. Besides, some SPTs never appeared in our models, such as SPTs to Alternaria or Brazil nut, and thus should no longer be performed in practice when diagnosing peanut allergy severity. Furthermore, some SPTs with proven cross-reactivity to peanuts were retained in our models, such as lupine flour , indicating that our results were in line with other medical discoveries. The discriminating models described in this paper are a first step towards a simple, safe and efficient diagnosis of peanut allergy severity by quantifying antibodies. Before being applied in clinical practices, they must first be validated on an independent set of patients. New variables must also be added as additional predictors toward improving successful classification rates. These models could then become practical tools for clinicians. When scoring severity, the clinical test results could be reported online at the allergy vigilance network (Réseau d'allergovigilance, http://www.cicbaa.com/ ), or a simple statistical software could be programmed. Table 3: Results of the discriminant analysis of first accidental exposure score for the three kinds of variable selection schemes. The best classification method is given for the four-class and two-class studies. Results are expressed as successful classification rates and as severe patient detection rates. Table 4: Results of the discriminant analysis of DBPCFC score for the three kinds of variable selection schemes. The best classification method is given for the four-class and two-class studies. Results are expressed as successful classification rates and as severe patient detection rates. Table 5: Results of the discriminant analysis of eliciting dose for the three kinds of variable selection schemes. The best classification method is given for the four-class and two-class studies. Results are expressed as successful classification rates and as severe patient detection rates. "
116,hal-00643787.training.tei.xml,72,The authors would like to thank Frances T. Yen for her critical review of the manuscript.
117,hal-00082280.training.tei.xml,0,"In this letter we present the discovery of a very light planetary companion to the star µ Ara (HD 160691). The planet orbits its host once every9.5 days, and induces a sinusoidal radial velocity signal with a semi-amplitude of 4.1 m s −1 , the smallest Doppler amplitude detected so far. These values imply a mass of m2 sin i=14 M⊕ (earth-masses). This detection represents the discovery of a planet with a mass slightly smaller than that of Uranus, the smallest ""ice giant"" in our Solar System. Whether this planet can be considered an ice giant or a super-earth planet is discussed in the context of the core-accretion and migration models."
118,hal-00082280.training.tei.xml,1,"The discovery of giant planets around other solar-type stars has opened the way to a new era of planetary research. The new worlds present a wide variety of orbital characteristics and minimum masses, and9 years after the first announcement (Mayor & Queloz 1995), some of their properties are still defying the theories of planetary formation. The increasing number of known systems is, however, giving the possibility to explore their properties from a statistical point of view (e.g. Santos et al. 2001;Zucker & Mazeh 2002;Udry et al. 2003;Eggenberger et al. 2004), and the observational and theoretical approaches are now starting to converge (e.g. Trilling et al. 2002;Alibert et al. 2004;Ida & Lin 2004a)."
119,hal-00082280.training.tei.xml,2,"Recently, with the installation of the new HARPS spectrograph ( Pepe et al. 2002) at the3.6-m ESO telescope (La Silla, Chile) a significant quantitative advance has been possible. This state of the art instrument is capable of attaining a"
120,hal-00082280.training.tei.xml,3,"Send offprint requests to: Nuno C. Santos, e-mail: Nuno.Santos@oal.ul.pt ⋆ Based on observations collected at La Silla Observatory, ESO, Chile, with the HARPS spectrograph, at the3.6-m ESO telescope (programs 073.D-0578 and 072.C-0488)."
121,hal-00082280.training.tei.xml,4,"precision better than1 m s −1. After only a few weeks of operation, it has discovered a first ""hot-jupiter"" ( Pepe et al. 2004) orbiting the K dwarf HD 330075. The level of precision in radialvelocity measurements achieved with HARPS gives now, for the first time, the possibility of lowering significantly the detection limit to the ""few-earth-mass"" regime, provided that the signal induced by stellar oscillations can be reduced with the use of an appropriate observing strategy ( Bouchy et al., in prep.)."
122,hal-00082280.training.tei.xml,5,"In this letter we present the discovery of a ∼14-M ⊕ short period (P∼9.5 days) extra-solar planet orbiting the star µ Ara, a star that was already known to be orbited by a longer period giant planet (Butler et al. 2001). Together with the very low mass companion to 55 Cnc ( McArthur et al. 2004), these are the only two sub-neptunian planets discovered to date. They are suspected to be earth-like rocky planets, orbiting solar-type stars."
123,hal-00082280.training.tei.xml,6,"µ Ara (HD 160691, HR 6585, GJ 691) is a nearby V=5.12 magnitude southern G5V star in the constellation Ara, the Altar, and according to the Hipparcos catalog (ESA 1997), it has a parallax of 65.5±0.8 mas, which implies a distance from the Sun of From a HARPS spectrum with a S/N ratio of the order of ∼1000 (average of 275 individual spectra), we have derived the stellar parameters for µ Ara using a fully spectroscopic analysis ( Santos et al. 2004). The resulting parameters (T eff , log g, V t , [Fe/H])=(5813±40 K, 4.25±0.07 dex, 1.30±0.05 km s −1 , +0.32±0.05 dex), are in almost perfect agreement with the values published in Santos et al. (2004), Bensby et al. (2004), and Laws et al. (2003). The surface gravity derived using the Hipparcos parallax and an effective temperature of 5800 K is 4.25 dex (see e.g. Santos et al. 2004)."
124,hal-00082280.training.tei.xml,7,"Using the temperature, [Fe/H], absolute magnitude and bolometric correction (Flower 1996), we derived a stellar mass of1.10±0.05 M ⊙ for µ Ara, from an interpolation of the theoretical isochrones of Schaerer et al. (1993). This is in excellent agreement with the 1.08 and 1.14 M ⊙ derived by Butler et al. (2001) and Laws et al. (2003), respectively. Preliminary results from the asteroseismology analysis are also in excellent agreement with these values ( Bazot et al., in prep.)."
125,hal-00082280.training.tei.xml,8,"From the width of the CORALIE Cross-Correlation Function (CCF) we have computed a projected rotational velocity of2.4 km s −1 for µ Ara ( Santos et al. 2002). This value is in agreement with the low chromospheric activity level of the star, log R ′ HK =−5.034±0.006, obtained from the HARPS spectra. Similar values of −5.02 were obtained both from the CORALIE data (Santos et al. 2000) and by Henry et al. (1996) at different epochs. The inactivity of this star is further supported by its low (and non-variable) X-ray luminosity (Marino 2002), as well as by the lack of significant photometric variation in the Hipparcos data (ESA 1997)."
126,hal-00082280.training.tei.xml,9,"From the observed value of log R ′ HK we can infer an age above ∼2 Gyr (Pace & Pasquini 2004) and a rotational period of ∼31 days ( Noyes et al. 1984). This age is compatible with the 4.5 Gyr obtained from an interpolation of theoretical isochrones (e.g. Laws et al. 2003), and with the upper value for the lithium abundance log ǫ(Li)<0.86 dex derived by Israelian et al. (2004) for this dwarf."
127,hal-00082280.training.tei.xml,10,"In June 2004, µ Ara was intensively measured over8 consecutive nights with the HARPS spectrograph as part of an asteroseismology program ( Bouchy et al., in prep). During each night, we obtained more than 250 spectra of this star, from which we derived accurate radial velocities. The average radial velocity for each night was then computed from a weighted average of each individual value, its precision being limited by the uncertainty in the wavelength calibration 1 ."
128,hal-00082280.training.tei.xml,11,"The main motivation of this program was to study the possibility that the high metal content of the planet-host stars (e.g. Gonzalez 1998; Santos et al. 2001Santos et al. , 2004, and references therein) is due to the engulfment of metal rich planetary material into their convective envelopes. Although current studies seem to favor that the observed ""excess"" metallicity reflects a higher metal content of the cloud of gas and dust that gave origin to the star and planetary system, recent results have suggested that this matter may still be unsettled (e.g. Vauclair 2004). The asteroseismology technique provides us with a good tool to possibly solve this problem. As shown by Bazot & Vauclair (2004), precise stellar oscillation measurements may be able to determine if there is some metallicity gradient in the stellar interior, that could be a hint of strong stellar ""pollution"" Table 1. Orbital elements of the fitted9.5-days period orbit and main planetary properties."
129,hal-00082280.training.tei.xml,12,⋆ Equilibrium temperature computed with an albedo of0.35.
130,hal-00082280.training.tei.xml,13,"events. The results of the asteroseismology campaign will be presented in Bouchy et al. (in prep.) and Bazot et al. (in prep). A first analysis of the data revealed what could be a periodic variation with an amplitude of about4 m s −1 (see Figs. 1 and 2). As part of the HARPS GTO program, this star was then closely followed from July 14th to August 19th 2004 (16 radial-velocity measurements were obtained). Each night the radial velocity was measured from the average of about 15 consecutive independent radial velocity estimates (computed from different spectra) taken during a period of ∼20 minutes. This methodology makes it possible to average the radial-velocity variations due to stellar oscillations ( Mayor et al. 2003)-see also Bouchy et al. (in prep.). As seen in Fig. 1, the measurements done during the first 8 nights (when the star was followed during the whole night) have a considerable lower rms around the best keplerian fit than the following measurements. This scatter results from the photon noise error (∼20 cm s −1 ), the calibration uncertainty (∼40 cm s −1 ), and from the stellar noise (∼80 cm s −1 ) that is not completely averaged on the nights with only 15 radial velocity measurements (Bouchy et al., in prep.)."
131,hal-00082280.training.tei.xml,14,"µ Ara was previously announced to harbor a giant planet in a long period (∼740 days) orbit (Butler et al. 2001). This orbital solution has since been updated by Jones et al. (2002), who found that the residuals of the radial-velocity planetary fit followed a long term trend, due to the presence of a second body in the system."
132,hal-00082280.training.tei.xml,15,"In Fig. 3 we plot the radial-velocity measurements of µ Ara obtained during the last6 years using three different instruments (see figure caption), as well as the best 2-keplerian fit. The orbit of the ∼740-day period planet (actually with a period of ∼660 days) is confirmed. However, the orbital parameters of the second (longer period) companion are not well constrained; we find a strong degeneracy between the derived orbital period and the value of the orbital excentricity, making it possible to fit the data with the former parameter varying between ∼3000 and 10000 days. Although not precisely determined, the mass of this companion remains probably in the planet regime. Despite of the still unconstrained long period of this outer companion, some stability studies of the system has been discussed (e.g. Gozdziewski et al. 2003). (2002) (open circles). The curve represents the best 2-body keplerian fit to the data. In the lower panel we present the rms around the fit. For the longer period keplerian fit, the eccentricity was fixed to a value of 0.2."
133,hal-00082280.training.tei.xml,16,"In Figs. 1 and 2 we present the HARPS radial-velocity measurements of µ Ara as a function of time. In this figure, the curve represents the best fit to the data, obtained with the sum of a keplerian function and a linear trend. The derived slope of this trend is in agreement with the expected effect due to the longer period companions (see Fig. 3). The analysis of the radial velocity measurements reveals a variation with a period of9.5 days, and a semi-amplitude of about 4 m s −1. These values can be explained by the presence of a m 2 sin i = 14 M ⊕ planet orbiting µ Ara in a circular orbit."
134,hal-00082280.training.tei.xml,17,"The residuals around the best fit to the HARPS data are flat, with a rms of only of0.9 m s −1. This rms decreases to the calibration level (0.43 m s −1 ) for the first 8 nights, attesting the incredible precision of this instrument. Despite the low amplitude of the radial velocity signal, the false alarm probability that it is due to random noise is lower than 1%, as derived through a Monte-Carlo simulation."
135,hal-00082280.training.tei.xml,18,"From the stellar luminosity and effective temperature we can derive a radius of ∼1.32 solar radii for µ Ara. Combined with the rotational period of 31 days (see Sect. 2), this implies a rotational velocity of the order of 2.2 km s −1 for µ Ara, close to the measured value v sin i=2.4 km s −1. Supposing that the orbital plane is perpendicular to the stellar rotation axis, this means that the orbital inclination sin i is close to unity, and that the observed minimum mass for the planet is not very different from its real mass."
136,hal-00082280.training.tei.xml,19,"Using the HARPS spectra we have derived both an activity index, based on Ca II H and K lines, and the bisector of the cross-correlation function from the individual spectra. No correlation is found between these quantities and the radial velocities within the measurement precision. Given the very low activity level of µ Ara and the inferred rotational period of ∼30 days, it is very unlikely that rotational modulation is capable of producing the observed stable periodic radial-velocity variation. Furthermore, to have a rotational period of 9.5 days, this star would have to rotate at about 7 km s −1. Such a rotational velocity would imply a much younger age for µ Ara, not compatible with its low level of activity."
137,hal-00082280.training.tei.xml,20,The presence of a14 M ⊕ planet around µ Ara thus remains the only credible explanation for the observed 9.5-days period radial-velocity variation.
138,hal-00082280.training.tei.xml,21,"As current planetary formation models are still far from being able to account for all the amazing diversity observed amongst the exoplanets discovered thus far, we can only speculate on the true nature of the present object."
139,hal-00082280.training.tei.xml,22,"First, given its location and the characteristics of the central star, it is unlikely that this object was in fact a much more massive giant planet which has lost a large fraction of its envelope over its lifetime. This is supported by the fact that more massive planets exist orbiting much closer to stars with similar characteristics and by calculations by Baraffe et al. (2004) and Lecavelier des Etangs et al. (2004) which show that only planets significanly less massive than Jupiter would evaporate at0.09 AU. Except if outward migration has occurred, we conclude that the mass of this object has always remained small."
140,hal-00082280.training.tei.xml,23,"To understand the consequences of this, it is necessary to recall that in the current paradigm of giant planet formation, a core is formed first through the accretion of solid planetesimals. Once this core reaches a critical mass (m crit ), accretion of gas in a runaway fashion becomes possible and the mass of the planet increases rapidly (e.g. Ida & Lin 2004b). This therefore implies that the current object has never reached the critical mass, for otherwise the planet would have become much more massive. Furthermore, recent giant planet formation models including disk evolution and migration ( Alibert et al. 2004) have shown that these effects greatly shorten the formation time. Hence, it is unlikely that the planet has migrated over large distances before reaching its present location. It was thus probably formed inside the ice radius (∼3.2 AU-Ida & Lin 2004a), and its composition should be dominated by rocky (telluric) material. We note that the high [Fe/H] of µ Ara makes this case possible (Ida & Lin 2004a). Curiously, with 14 M ⊕ and a=0.09 AU, this planet is near the borderline of the massperiod desert defined by Ida & Lin (2004b), where no planets are supposed to exist."
141,hal-00082280.training.tei.xml,24,"The above considerations lead us towards the following scenario for the formation of the present planetary system. The more massive planet, with the present ∼660 days period orbit, begins to form first and migrates inwards while growing in mass. Towards the end of the lifetime of the disk, the smaller planet is formed inside the orbit of the larger one, probably at a distance not exceeding 3 AU. Thus, we expect this object to have a massive, essentially rocky core (as opposed to icy), surrounded by a gaseous envelope with ∼5-10% of its mass. It therefore probably qualifies as a super-Earth and not as a failed ice-giant."
142,hal-00082280.training.tei.xml,25,"The discovery of this extremely low-mass planet represents a new benchmark for planet surveys, and demonstrates the ability of instruments like HARPS to detect telluric planets with just a few times the mass of the Earth. In the future these detections will give the possibility to study the low end of the planetary-mass distribution. This kind of planets may be relatively common, as according to recent simulations (Ida & Lin 2004a), very low-mass planets may be more frequent than the previously found giant worlds. This is further supported by the recent detection of a first neptunian planet in a short period orbit around 55 Cnc ( McArthur et al. 2004) 2. Such planets will be preferential targets for space missions like the photometric satellites COROT and Kepler. Furthermore, the discovery of such low mass planets around stars that have at leastone more giant exoplanet, makes of these systems very interesting cases to understand the processes of planetary formation and evolution."
143,hal-01252076.training.tei.xml,0,"We analyzed data from120 LGG patients treated between 1999 and 2007.  Data included information on tumor size in all patients and on up to three molecular (genetic) characteristics in 77 patients: 1p/19q chromosomal codeletion, p53 overexpression (a surrogate marker for TP53 missense mutations  ), and IDH mutation status. The model was built using the data from these 77 patients. Among these patients, information on all molecular characteristics was available for 42 patients (54.5%), and information on at least one item of molecular characteristic in 35 patients (45.5%). The data from the 43 patients without molecular information were used as an external dataset to evaluate model quality."
144,hal-01252076.training.tei.xml,1,"Tumor sizes were measured manually from printed MRI images and expressed as mean tumor diameter (MTD) in millimeters, according to the formula MTD 5 (2V) 1/3 , where"
145,hal-01252076.training.tei.xml,2,"is the approximated tumor volume with D 1 , D 2 , and D 3 referring to thethree largest perpendicular diameters. The total median duration of the posttreatment follow-up period for a single patient, which included several follow-up observations, was 21 months (5 months at minimum and 9.5 years at maximum). We analyzed a total of 952 MTD observations with, on average, 12 observations per patient (minimum 4 observations and maximum 28). Histology consisted of grade II oligodendrogliomas (56 patients, 73%), oligoastrocytomas (16 patients, 21%), and astrocytomas (5 patients, 6%). All patients received TMZ as first-line treatment. The drug was administered for 5 consecutive days (day 1 to day 5) every 28 days at a daily dose of 200 mg/m 2 ."
146,hal-01252076.training.tei.xml,3,"We recently proposed a mathematical model to describe MTD dynamics in LGG patients before, during, and after chemotherapy. The model, which distinguishes between disease-specific and treatment-specific parameters, relies on the hypothesis that LGG tumors are made up of both quiescent and proliferative cells, and that both cell types are sensitive to treatment.  Chemotherapy is assumed to act by damaging cells' DNA. The DNA damage leads proliferative cells to die, whereas quiescent cells with DNA damage can either repair their lesions and return to a proliferative state or die. Thus, we consider three compartments: proliferative tissue, denoted P; nondamaged quiescent tissue, denoted Q; and damaged quiescent tissue, denoted Q p. The sum of the values attributed to the three compartments (P Ã Þ represents the size of the lesion and is compared to the MTD observations."
147,hal-01252076.training.tei.xml,4,"To allow for MTD increase during TMZ treatment, we extended the previously proposed model by taking into account the possibility that proliferative cells can repair their DNA lesions during the division process, instead of immediately dying, and thus acquire resistance to TMZ. In line with previous works, we implemented acquisition of resistance by assuming that the effect of TMZ concentration on tumor tissues decreases exponentially with the amount of time since the beginning of treatment. Both proliferative and quiescent tissues were considered as having the potential to acquire resistance to TMZ."
148,hal-01252076.training.tei.xml,5,"The model was developed in a population context, and the values of individual-level parameters were assumed to be log-normally distributed; i.e., for a parameter w i corresponding to an individual patient i, w i =w.expg i where w is the ""typical"" (population) value of the parameter, and g i represents the contribution of the individual i. The values of g are normally distributed with mean 0. Population and individual parameter values were estimated with the SAEM (Stochastic Approximation of the Expectation Maximization) algorithm implemented in Monolix 4.2 (Lixoft) using the full MTD time-course in the 77 patients. We assumed a constant error model, with parameter value a. Model selection was carried out according to the usual criteria; in particular, models that achieved lower values of the objective function were considered to provide a better fit to the data."
149,hal-01252076.training.tei.xml,6,"The data from the42 patients with complete molecular status (codeletion 1p/19q, p53, and IDH mutations) was used as a training dataset to obtain statistics on this genetic information. We computed the percentage of patients with a given genetic profile and derived the probability to exhibit it. So for patients with one or two missing covariates, we could compute the probability to exhibit the lacking mutation status(es), knowing the other(s) one(s). We therefore attributed genetic characteristics for patients with missing covariates. In our training dataset, in line with previous literature, 1p/ 19q codeletion and the p53 mutation were mutually exclusive. We incorporated the molecular information into the model and identified the effect of each status on the tumor size kinetic parameters. For a given fixed-effect parameter f, the model used for covariate analysis was of the following form:"
150,hal-01252076.training.tei.xml,7,"where status j corresponds to the value of the characteristic j. As we considered binary variables only, Eq. 1 could be simplified:"
151,hal-01252076.training.tei.xml,8,"where n 0 denotes the population value of the parameter for the reference group of patients (with non-codeleted 1p/19q or wild p53), and n 1 is the population value for the group of patients with mutated covariate. Using this framework, only fixed-effect parameters can depend on the covariate status while (interindividual) variability is not concerned, i.e., parameters n 0 and n 1 will have the same variability."
152,hal-01252076.training.tei.xml,9,"We used a stepwise forward/backward modeling strategy based on progressive inclusion and then exclusion of covariates, where the decision to include or exclude a covariate was dependent on the covariate's effect on the objective function through a log-likelihood ratio test. Empirical Bayes estimates to predict LGG response to temozolomide Following a Bayesian approach in which population parameter estimates constituted prior information, we analyzed each patient's data individually, considering only the MTD observations prior to treatment and the first MTD observations obtained after treatment onset, i.e., the observations obtained in the first 3 months of treatment. If, for a given patient, no observations were available prior to treatment, we used only the MTD observations obtained within the first 3 months after treatment onset. Patients for whom no information was available for the first 3 months of treatment were excluded from the analysis. Overall, data from 45 patients were analyzed. Among these, 31 patients had a single MTD observation during the first 3 months of treatment, and 14 patients had two MTD observations. When possible, MTD observations before treatment onset were used to estimate empirical Bayes estimates (EBEs) of the disease-specific parameters of the model. The MTD observations after treatment start were then used to estimate the treatment-specific parameters of the model. EBEs were calculated using MatLab (MathWorks, Natick, MA) after the population parameters were estimated with Monolix."
153,hal-01252076.training.tei.xml,10,"To evaluate the capacity of the model to predict individual tumor response to TMZ treatment,two clinically relevant metrics were calculated: the duration of response, i.e., the length of time during which MTD decreases, and the minimal tumor size reached as a result of TMZ effect."
154,hal-01252076.training.tei.xml,11,"In the analysis described above, the model's predictive capacity was assessed on patients whose full time-course data were used to estimate the population parameters; these population parameters constituted prior information for the calculation of EBEs. To explore whether this caused a bias, we subsequently performed predictive analysis on the43 ""external"" patients whose data had been excluded from the initial model-building process, owing to a lack of genetic information. In this case, we used early MTD observations as in the original predictive analysis, but without covariates."
155,hal-01252076.training.tei.xml,12,"We then used population parameters to simulate200 new virtual patients, and we made predictions for these patients using the model without covariates in a first step, and we incorporated genetic statuses into the predictions in a second step."
156,hal-01252076.training.tei.xml,13,"Tumor size time-course in patients treated with temozolomide Figure 1 depicts the time-course of tumor size (mean tumor diameter) in the77 LGG patients included in the analysis. Patients received a median of 18 TMZ cycles (minimum 2 cycles, maximum 24). Table 1 shows a summary of the characteristics of the 77 patients. Tumor size increased linearly before treatment.  Median time to tumor progression was 18 months. In 34 patients MTD a reincrease occurred during TMZ treatment while in 24 patients it occurred after TMZ discontinuation. We separated the patients to present tumor profiles in a clear manner, but it did not impact data analysis."
157,hal-01252076.training.tei.xml,14,"Inclusion of a resistance term for the proliferative tissue resulted in a significantly better model fit (drop of200 points in the objective function) compared with exclusion of the resistance term. The model with the inclusion of a resistance term for the quiescent tissue performed worse. However, because quiescent cells have the capacity to repair their DNA lesions, they also contribute to the emergence of resistance by repopulating the proliferative compartment. Thus, the final selected model incorporated a resistance term for the proliferative tissue only:"
158,hal-01252076.training.tei.xml,15,"The term res denotes the resistance parameter. Given the time scale of data collection compared to the time scale of TMZ delivery scheduling, we represented a single TMZ cycle (actually composed offive daily administrations) as a single bolus administration with corresponding concentration, C, assumed to undergo exponential decay at a constant rate KDE (a so-called K-PD approach  ). The parameter c is the constant rate for proliferative tissue death (also referred to as the TMZ efficacy parameter). A schematic view of the model is presented in Figure 2. The full mathematical equations of the model are:"
159,hal-01252076.training.tei.xml,16,"Overall, the model includesseven parameters and two initial conditions. Two parameters are disease-specific and related to tumor growth: the proliferation rate (k P ) and the transfer constant rate (k PQ ) from proliferation to quiescence. The five remaining parameters are related to TMZ action and effect and are called treatment-specific parameters. In particular, the constant rate of death of quiescent cells is denoted d Qp and k Qp P is the constant rate of transition from quiescence to proliferation following repair of TMZ-induced DNA damage. For identifiability reasons, we assumed that the initial drug effect is the same on P and Q. This is consistent with the biology, as TMZ acts on cells regardless of their stage in the division process."
160,hal-01252076.training.tei.xml,17,"The covariate analysis performed with thethree molecular status characteristics showed that p53 mutation could be included as a covariate of the TMZ efficacy parameter (c). On the basis of the stepwise forward/backward analysis, we further determined that chromosomal 1p/19q codeletion could be included as a covariate on the constant rate k Qp P (transition from quiescence to proliferation following repair of TMZ-induced DNA damage. The inclusion of these two model covariates led to a significant drop in the objective function (126 points; P < 0.01, likelihood-ratio test). In the Figure 1 Typical tumor size dynamics for the 77 patients included in the analysis. Top, right: In 21 patients, an ongoing MTD decrease after treatment discontinuation was observed. Bottom, left: Acquired resistance occurs in 34 patients. MTD time-course shows initial decrease followed by progression during TMZ treatment. Mazzocco et al. stepwise analysis procedure, IDH mutation status identified as having an effect on model parameters when tested independently from the two other covariates was not identified as a significant covariate in the presence of p53 and 1p/ 19q information. This is in agreement with the known redundancy of the genetic information and indicates that, in our model, p53 and 1p/19q information intrinsically integrate IDH information. The parameter estimates of the final model (including covariates) are presented in Table 2."
161,hal-01252076.training.tei.xml,18,"Among p53-mutated patients, the value of the TMZ efficacy parameter is45% lower than among p53-nonmutated patients, suggesting that TMZ therapy is almost two times less effective in the former population. This is consistent with preclinical evidence that p53 mutations decrease sensitivity to TMZ in gliomas.  Likewise, among patients with the 1p/19q codeletion, the value of k Qp P is 15% lower than among non-codeleted patients, suggesting that among patients in the former group, DNA-damaged quiescent cells have less capacity to repair themselves. This finding is consistent with the longer duration of response reported in codeleted patients.  Figure 3 shows goodness-of-fit (visual predictive check) plots for the 77 patients included in the model-building Figure 2 Schematic view of the model with model's parameters. P is the proliferative tissue, Q is the nondamaged quiescent tissue, and Q p is the damaged quiescent tissue. The sum of the values corresponding to the three compartments, P Ã , is compared to the MTD observations. Proliferative tissue (P) can become quiescent (Q). TMZ treatment affects both proliferative and quiescent tissues. Damaged quiescent tissue can either repair its DNA lesions and return to a proliferative state or die due to treatment-induced lesions. dataset and for the 43 patients included in an external dataset. These diagnostics indicate good quality of the model, with and without covariates. The proposed model is able to capture the variability in patients' response to TMZ, including prolonged response after therapy discontinuation or emergence of acquired resistance to TMZ during treatment."
162,hal-01252076.training.tei.xml,19,"Prediction of response to TMZ chemotherapy Figure 4 shows predictions regarding individual patients' response durations (left-hand side), represented by KaplanMeier curves, together with observed response durations that fall in the95% confidence interval (CI) for almost 2 years after treatment onset. Beyond 2 years, the model predictions are incorrect, which is not surprising given that only information until month 3 is taken into account. Notably, beyond 2 years predicted times to progression are earlier than the actual times to progression. In this respect, the modeling framework shows a tendency for underestimating the effect of the treatment. The early part of the KaplanMeier curve also indicates a tendency to predict progression at a very early time. Integrating this point in our modeling framework resulted in predicting very early progression. However, removing these four patients resulted in correcting the early part of the Kaplan-Meier curve. Figure 4 also shows predicted vs. observed minimal tumor size (right-hand side). We evaluated prediction bias (mean prediction error) and precision (root mean squared prediction error).  Prediction bias was 1.89 mm (95% CI (-0.22, 3.99)) and precision was 7.18 mm (95% CI (4.52, 9.10)). For 90% of the patients, the observed minimal tumor size was predicted correctly, with less than 25% relative error (relative to tumor size at treatment onset); this corresponds, approximately, to an error of about 1 cm in MTD. Notably, for each of the 14 patients with two MTD observations within the 3 first months of treatment, the minimal tumor size was predicted with an error of less than 15% (about 6 mm)."
163,hal-01252076.training.tei.xml,20,"Individual predictions forsix patients with repeated measurements during the first 3 months of treatment are displayed in Figure 5 as an illustration of the method proposed herein. It shows that the model is also able to predict tumor time-course, especially for these patients. For the 10% of patients for whom predictions of minimal tumor size were incorrect, we observed that, for four of them, tumor size reduction was unexpectedly characterized by two phases: an initial moderate decrease in tumor size within the first 5 months of treatment, followed by a more pronounced decrease after 5 months. For these four patients, the model underevaluated the response. Inclusion of a second MTD observation (obtained after 5 months of treatment) yielded correct minimal tumor size predictions for these patients. Tumor response for one additional patient could not be predicted owing to his/her prolonged response to treatment (more than 40 months). The model may not be able to capture such extreme behavior. We then performed predictive analysis on the 43 ""external"" patients. The model successfully predicted the minimal tumor size for 75% of these patients (bias 24.41 mm (-7.14, 21.67)). To evaluate whether the reduction in predictive capability was due to the lack of genetic information or to the fact that these patients' data were not incorporated as prior information in model-building, we made predictions for 200 virtual patients. Without covariates, the minimal tumor size was successfully predicted for 76% of the virtual patients. When we incorporated genetic statuses, predictive capacity rose to 87% (bias 0.31 mm (-0.86, 1.49)). These observations lead us to believe that the reduction in the model's predictive capacity for the 43 external patients was mainly due to the absence of genetic information."
164,hal-01252076.training.tei.xml,21,"Molecular profiling of tumors is a well-known strategy to personalize anticancer treatments. Another approach is mathematical modeling. Mathematical models of tumor growth and response to treatment allow characterizing quantitatively the efficacy and toxicity of anticancer agents and can be used to predict clinical response. In the present study, we show that combining longitudinal tumor size measurements through the use of quantitative modeling with a tumor's genetic characterization is a promising strategy to personalize treatments in patients with lowgrade gliomas."
165,hal-01252076.training.tei.xml,22,"Using p53 mutation and 1p/19q codeletion as covariates significantly improved the model accuracy. In agreement with the literature, p53 and 1p/19q molecular statuses significantly impacted the dynamics of LGG response to treatment: p53 mutation impaired TMZ efficacy and 1p/19q codeleted tumors had less ability to repair TMZ-induced DNA lesions in quiescent tissue, thus increasing the overall efficacy of treatment. IDH mutation status did not provide useful information on tumor size dynamics beyond the combined information provided by p53 and 1p/19q status. However, it would be relevant to introduce IDH status as a model covariate in cases in which p53 and 1p/19q information is not available for a given patient."
166,hal-01252076.training.tei.xml,23,"Molecular information (p53 and 1p/19q status) and tumor observations obtained during the first3 months after TMZ treatment onset were sufficient to correctly predict the amplitude of response and its duration for almost 2 years, especially in those patients in whom precise assessment of early tumor response was available. An adaptive approach, consisting of updating the model predictions at each new MTD observation, could be implemented to prolong the validity period of the prediction and to enhance the percentage of patients for whom the minimal tumor size is successfully predicted. These results, however, need to be confirmed in a larger set of external patients. It would also be interesting to evaluate the relationship between the predictive performance of the model and the timing of MTD observations used for predictive purposes. In this study, we focused on MTD observations obtained during the first 3 months of treatment because we wanted to obtain a reference or rational value for the predictive potential of the model. In the area of brain tumor, numerous studies have used partial differential equation models integrating both time dynamics and spatial aspects of these highly diffusive malignancies (see refs. 31,32 for examples). It would also be important to apply the same prediction framework with these types of models, which capture more precisely glioma evolution."
167,hal-01252076.training.tei.xml,24,"Emergence of acquired resistance, defined as progression after initial benefits, is a critical issue in clinical oncology and model-based approaches should be used to better understand, characterize, and predict this phenomenon. Mechanisms of acquired resistance can be pharmacological resulting from decreased drug uptake into the cell, intracellular drug inactivation, or repair of drug-induced damages. This is the case of the LGG response to TMZ, for which it has been shown that more than90% of recurrent gliomas show no response to a second treatment with TMZ. One of the key elements in TMZ resistance is MGMT, an enzymatic protein with the faculty to repair the principal O 6meG-alkylation site to TMZ, thus resulting in decreased efficacy of drugs during treatment. MGMT could not be tested as a covariate in our model since for most patients only formalin-fixed paraffin-embedded (FFPE) tissue was available for DNA tests, whereas MGMT methylation testing ideally needs frozen tissue. Of note, resistance to TMZ is also responsible for significant therapeutic failures in melanoma. Following this biological knowledge, we modeled acquisition of resistance by decreasing drug efficacy on LGG proliferative tissue with time following therapy onset. Since all patients received the same TMZ doses with the same scheduling, we believe that modeling resistance as a function of actual drug exposure would not have led to significantly different results. For identifiability reasons, we could not model all transitions between the three compartments, especially the possibility for quiescent cells to directly return into proliferation, as it could occur without treatment. However, during treatment quiescent cells do have the capacity to repair their DNA and become proliferative. Therefore, this model is a relevant tool to characterize tumor response to TMZ but does not aim to faithfully mimic natural tumor growth. Our model is flexible enough to reproduce the variability of patients' response to standard TMZ protocol and has the capacity to mimic both tumor regrowth during treatment as a result of acquired resistance to TMZ and prolonged response to treatment. Finally, our model could constitute a rational tool to optimize the duration of temozolomide therapy in low-grade glioma patients. Up to now, there are no clear rules or guidelines regarding the optimal number of cycles or treatment duration in patients treated with TMZ.  Many neurooncologists prolong the duration of the treatment beyond 12 cycles if an ongoing decrease in tumor size is observed at this time. However, there are downsides to prolonging TMZ treatment, including side effects and costs. An even greater concern is that TMZ therapy might drive the evolutionary path to high-grade glioma. Consequently, determining the optimal duration of TMZ therapy and maximizing the duration of the response is a critical challenge in the management of"
168,hal-01252076.training.tei.xml,25,"LGG. Using our model as a simulation tool to determine the optimal number of TMZ cycles each patient should have received, we found that for47% of the 45 patients analyzed, the model would have recommended, by month 3 after treatment onset, administration of additional TMZ cycles beyond what these patients actually received; i.e., in these patients delivery of additional TMZ could have resulted in tumor shrinkage exceeding what was ultimately achieved. In contrast, the model predicted that 33% of the patients could have benefited if their treatment had been stopped earlier than it actually was."
169,hal-01252076.training.tei.xml,26,Mandonnet et al. suggested that knowledge of the glioma growth rate prior to treatment can be used to optimize patient management and follow-up.Our approach proposes to leverage early tumor dynamical information together with the a tumor's genetic characteristics to predict tumor size response. We believe that this framework can be used as a template for other diseases whose response to treatment is characterized by emergence of acquired resistance.
170,1412.2117.training.tei.xml,0,"The Near-Earth Object Wide-field Infrared Survey Explorer (NEOWISE) mission observed comet C/2013 A1 (Siding Spring)three times at 3.4µm and 4.6µm as the comet approached Mars in 2014. The comet is an extremely interesting target since its close approach to Mars in late 2014 will be observed by various spacecraft in-situ. The observations were taken in 2014 Jan., Jul. and Sep. when the comet was at heliocentric distances of 3.82AU, 1.88AU, and 1.48AU. The level of activity increased significantly between the Jan. and Jul. visits but then decreased by the time of the observations in Sep., approximately 4weeks prior to its close approach to Mars. In this work we calculate Afρ values, and CO/CO 2 production rates."
171,1412.2117.training.tei.xml,1,"Comet C/2013 A1 (Siding Spring) presents both risk and opportunity at its close approach to Mars on2014 Oct. 19. It will pass approximately 135,000km ± 5000km from the planet's center (Farnocchia et al. 2014) -close enough for detailed observations by spacecraft at Mars but also close enough for ejected dust and gas to reach Mars' atmosphere, potentially affecting spacecraft in orbit. The potential risk posed by the dust has been downgraded through careful dynamical modeling. Early studies suggested relatively high amounts of dust would reach the Martian atmosphere (Ye & Hui 2014; Moorhead et al. 2014). Later studies that included lower ejection velocities and radiation pressure effects showed only a low fluence of old, larger dust grains is expected to reach the atmosphere (Tricarico et al. 2014; Kelley et al. 2014). However, it is possible that the gas coma may sufficiently excite the atmosphere of Mars to cause increased drag on orbiting satellites (Yelle et al. 2014). The comet is also interesting in its own right as a long period comet on a near-parabolic retrograde orbit, bringing primitive material in from the Oort Cloud. In this paper we use data from the Near-Earth Object Wide-Field Infrared Survey Explorer reactivation (NEOWISE; Mainzer et al. 2014) mission to characterize the comet's activity on three occasions in 2014 Jan.Jul. and Sep. as the comet approached Mars. We derive dust and gas production rates using near-infrared wide-field images and examine the evolving morphology of the coma."
172,1412.2117.training.tei.xml,2,"The NEOWISE mission utilizes the Wide-Field Infrared Survey Explorer (WISE) spacecraft (Wright et al. 2010), which completed an all-sky survey atfour wavelengths of 3.4, 4.6, 12, and 22µm from January to August 2010 using a 40cm cryogenically-cooled telescope. After the depletion of the spacecraft's solid hydrogen, the mission could still operate at the two shortest wavelengths of 3.6 and 4.6µm. This phase of the mission was renamed NEOWISE (Mainzer et al. 2011) and continued for several months until Feb 1, 2011 after which the spacecraft was placed into hibernation. The spacecraft was then reactivated in Dec 2013 for the start of the NEOWISE reactivation mission (Mainzer et al. 2014 ). The telescope is conducting an all-sky survey in a low- Earth orbit, taking images of the sky every 11s. There is approximately 10% overlap between successive frames, allowing for the linking of moving object detections. The images, which are 47′ on a side, undergo instrumental and astrometric calibration, followed by source extraction at at the Infrared Processing and Analysis Center (IPAC) at the California Institute of Technology (Wright et al. 2010). All data from the WISE prime mission in 2010 have been publicly released via NASA's Infrared Science Archive (IRSA) 1 , and the first data release from the NEOWISE reactivation mission will be in March 2015 through IRSA. The motion of comet Siding Spring meant that it was observed by NEOWISE on three separate occasions, hereafter referred to as "" visits "" . Observing details are given in Table 1."
173,1412.2117.training.tei.xml,3,"We used predicted positions from the Jet Propulsion Laboratory's Horizons service 2 and the Moving Object Search Tool 3 (Cutri et al. 2012) to identify the images covering comet Siding Spring. Trailing during each7.7s exposure was at most under than 1′′, which is significantly less than the FWHM. We coadded the images of the comet in the moving reference frame for each visit using the ICORE image co-addition package that includes outlier-rejection (Masci 2013). This improved the signal-to-noise ratio and resampled the pixel scale from . The coadded images were centered on the predicted position of the comet. Two-band color images of comet Siding Spring from each visit are shown in Figure 1. The comet appeared active at each visit."
174,1412.2117.training.tei.xml,4,"We used circular apertures with radii of11′′ to perform photometry on each coadded image. Counts were converted to fluxes using instrumental zero points using the same method as used for WISE catalog source data (Wright et al. 2010). The background was estimated and subtracted using the modal value in an aperture of at least 1.4square arcminutes located ≥ 1′ from the nucleus, and far from the coma. The dominant source of error is the uncertainty on the absolute calibration. We fit a reflected light model to the observed data at 3.4µm, making the assumption that the flux observed is due to reflected light from dust grains, the grains have a neutral reflectance. The reflected light is thus effectively the solar flux scaled to the 3.4µm signal. We neglected light reflected or emitted by the nucleus, the size of which is unknown at this time because it has been obscured by dust since discovery. If the nucleus is on the order of 1km in radius, it would contribute a few tenths of a µJy to the fluxes observed during the 2014 Jan. visit when the comet was least active and the nucleus likely contributed the largest fraction of the light. This is significantly less than the uncertainty in the flux measurements and we conclude that the nucleus contributes a negligible amount of light. We also computed a theoretical thermal contribution to the flux by calculating the blackbody radiation that would be emitted by the quantity of dust estimated from the 3.4µm flux. We used a Planck function and assumed an emissivity∼0.9, so that the temperature of the grains scaled as 286K × r −1/2 H . It is entirely possible that the grain spectral energy distribution is more complicated, but the singular data point at 3.4µm only allows for a simplified dust model constraint. The measured fluxes for each visit are listed in Table 1 and shown in Figure 2 with the reflected light and predicted thermal light models."
175,1412.2117.training.tei.xml,5,We converted the flux measured at3.4µm to the quantity Af ρ -a proxy for the dust production rate discussed in A'Hearn et al. (1984) -using equation 1.
176,1412.2117.training.tei.xml,6,"where A is the Bond albedo of the dust at the phase angle of observation, f is the filling factor of the dust grains within the aperture, ρ is the aperture size in cm, ∆ and r H are the geocentric and heliocentric distances in cm and AU, respectively, and F comet and F ⊙ are the flux from the comet and the Solar flux at1AU, respectively. As before, we used 11′′ radius apertures. We estimate the uncertainties on our measurements by examining the spread of results obtained when using apertures of radii 9, 11, and 22′′ since, ideally, the quantity calculated should be independent of chosen aperture size. However, it is quite possible that the spread in these values may in part be due to non-ideallized behavior, i.e. that the coma flux does not drop off as ρ −1. Residuals from the removal of inertial background sources from the co-added image may contribute to the Af ρ uncertainty as well. We measure Afρ as 432 ± 21 cm during the 2014 Jan. visit, 726 ± 40 cm during the Jul. visit, and 724 ± 40 cm during the Sep. visit. These Afρ values are provided for comparison with each other. The values are not corrected for phase angle effects since these effects are poorly constrained for infrared wavelengths, nor do the uncertainties include the component from the uncertainty in the absolute photometric calibration. We report these values in Table 1 and show how Afρ varied with heliocentric distance in Figure 3.2. The Afρ value reported by Bodewits et al. (2014) from observations in late May falls between our Afρ values when using the same phase-angle correction method 4 . Because the phase-angle correction at 3.4µm is not wellconstrained for comet dust, we hesitate to make any further direct comparisons between the optical and infrared Af ρ values when they are not simultaneous."
177,1412.2117.training.tei.xml,7,"At4.6µm the observed flux exceeds that predicted by the combined reflected and thermal light models. In 2014 Jan. we find that the excess emission is at the 3 σ level, while it drops to 2.5σ and σ in the Jul. and Sep. visits, respectively. We note that the uncertainties are dominated by systematics that correlate with the signal at 3.4µm and thus the uncertainties at 4.6µm are likely overestimated. The September band-excess detection is notably weak. However, the 3.4µm and 4.6µm relative photometry are better constrained, since the relative uncertainties are ∼ 7% (Cutri et al. 2012), considerably less than the uncertainties listed in Table 1. The NEOWISE bandpass at 4.6µm contains emission features from CO 2 (ν 3 band) at 4.26µm and CO (v=1-0 band) at 4.67µm. Both species have sufficiently long photodissociation lifetimes to be responsible for the observed excess flux. We therefore interpret the excess emission as being due to optically thin gaseous emission and convert the excess flux, F , to an average column density, N , using equation 2:"
178,1412.2117.training.tei.xml,8,"where symbols have the same meanings as in equation 1, λ is the wavelength of observation, h is Planck's constant, c is the speed of light, and g is the fluorescence efficiency for the chosen gas species. F is the excess flux density in the4.6µm bandpass is integrated over the CO/CO 2 band, after the in-band dust signal contribution has been removed, as described in Pittichová et al. (2008). We are unable to distinguish between CO and CO 2 emission with just the NEOWISE data since the bandpass spans both features and therefore present two scenarios in Table 1 where 100% of the excess flux is due to CO emission or it is entirely due to CO 2 emission. We assume fluorescence efficiencies at a heliocentric distance of 1AU for the CO v=1-0 and the CO 2 ν 3 bands of 2.46 × 10 −4s −1 and 2.86 × 10 −3s −1 , respectively (Crovisier & Encrenaz 1983). We then use the average column densities to calculate the production rates, Q in molecules s −1 , using equation 3:"
179,1412.2117.training.tei.xml,9,"in which N and ρ have their previous definitions, v is the ejection velocity of the gas in km s −1 and10 5 is a conversion factor (Pittichová et al. 2008). We take the ejection velocity to be 0.6km s −1 during the 2014 Jan. visit when the comet was at a heliocentric distance of 3.82AU and assume that the velocity scales as √ r H (Delsemme 1982). The production rates calculated are given in Table 1 and show that gas production increased overall by a factor of ∼ 3.5 between 2014 Jan. and Sep.. For these heliocentric distances, this kind of increase, proportional to ∼ r −1.8 is comparable to a Q CO2 ∼ r −2 , possibly coinciding with a fixed source of constant area (Meech and Svoren 2004). There was a decrease between the Jul. visit and the Sep. visit which may be attributable to a decrease in overall activity. Figure 3.2 shows the evolution of the Af ρ quantity and CO 2 production as the comet moves toward perihelion. There are comparatively few published H 2 O values for Siding Spring, but Q H2O was measured to be ∼ 2 × 10 27molecules s −1 when the comet was at ∼ 2.5AU (Bodewits et al. 2014). This would suggest a lower bound of Q CO2 /Q H2O of 15% assuming water production increased with decreasing heliocentric distance from 3.8 to 2.5 AU."
180,1412.2117.training.tei.xml,10,"We have used data from the NEOWISE reactivation mission to monitor the activity of comet Siding Spring over9months as it approaches Mars on a trajectory that will bring it within ∼135,000km of the planet. Our conclusions from this work are as follows:"
181,1412.2117.training.tei.xml,11,"1. NEOWISE observed comet Siding Spring to be active fromJanuary to September, 2014. The quantity Afρ and CO/CO2 production rates initially increased as the comet reached a heliocentric distance of 1.88AU but then decreased slightly even as the comet moved further inwards."
182,1412.2117.training.tei.xml,12,"2. The activity of the comet decreased betweenJuly and September, possibly due to depletion of volatile deposits on or near the surface."
183,1412.2117.training.tei.xml,13,3. The decrease in activity suggests the risk to assets at Mars was reduced since activity diminished as the comet approached Mars.
184,inserm-00508113.training.tei.xml,0,"Background: We investigated whether circulating tumor cells (CTCs) and circulating endothelial cells (CECs) predict clinical outcome of first-line chemotherapy combined with bevacizumab in metastatic breast cancer patients. Patients and methods: In a French substudy of the MO19391 trial, CTC and CEC counts (CellSearch system) at baseline and changes aftertwo cycles of treatment were correlated with time to progression (TtP). Results: CTC and CEC levels were not correlated in the 67 patients included. At baseline, CTC positivity was a significant prognostic marker for TtP at a threshold of 3 CTC/7.5 ml (P < 0.05) but not at 5 CTC/7.5 ml (P = 0.09). Baseline CEC levels (median 17 CEC/4 ml, range 1-769) were associated with age ‡45 years (P = 0.01), elevated lactate dehydrogenase (P < 0.01) and not with TtP at any threshold. Changes of CTC count during treatment were not a surrogate of TtP, with any of the model tested (threshold based or relative decrease in percent). However, increase in CEC count was associated with improved TtP, at the threshold of 20 CEC/4 ml (P < 0.01). Conclusion: Bevacizumab combined with first-line chemotherapy may modify the predictive value of CTC during treatment possibly due to impaired tumor cells intravasation through vessels endothelium. Variations in CEC levels appear to be a promising early surrogate marker of TtP under antiangiogenic treatment."
185,inserm-00508113.training.tei.xml,1,"Several methods for the detection of cancer cells disseminated in the bone marrow and/or circulating in the peripheral blood of patients have been developed recently for the most common epithelial tumors. Beyond biological characterization of the metastatic process, disseminated tumor cells (DTCs) and circulating tumor cells (CTCs) detection can be used in a few clinical settings as a prognostic factor and/or as an early surrogate marker of treatment response. In patients with metastatic breast cancer, it has been reported that DTC detection was not clinically relevant, whereas CTC detection was of prognostic significance . Currently, the most commonly used detection system in this setting is the automated and standardized CellSearchÒ assay , which has a validated reproducibility . In a pivotal study reported by Cristofanilli et al. in 2004 , CTCs were screened by this system in patients who received hormonal treatments or chemotherapy as first-or second-line treatment of metastatic breast cancer. In patients under standard chemotherapy, CTC positivity at baseline was an independent prognostic factor for progression-free survival (PFS; 2.7 versus 7.0 months; P < 0.001) and overall survival (OS; 10.1 versus >18 months; P < 0.001). CTC positivity was defined in a training set (n = 102 patients) and validated in another set of patients (n = 75 patients). Three weeks after the start of the treatment, CTC status was also independently associated with PFS (2.1 versus 7.0 months; P < 0.001) and OS (8.2 versus >18 months; P < 0.001). In the same series, other studies reported that CTC detection was poorly correlated to the CA 27-29 serum marker nor to the tumor burden . The threshold of 5 CTC/7.5 ml for CTC positivity and its association with clinical outcome have been confirmed in 80 patients treated with standard chemotherapy at the European Institute of Oncology (Milan, Italy) , and in another 74 American patients . However, the clinical benefit of using CTC count as an early surrogate marker of response and/or time to progression (TtP) under treatment has not yet been demonstrated prospectively, explaining why American Society of Clinical Oncology has not recommended its use ."
186,inserm-00508113.training.tei.xml,2,"In this specific setting of first-line treatment of metastatic breast cancer patients, it has been demonstrated that the antiangiogenic agent bevacizumab, when given in combination with first-line chemotherapy, significantly improves PFS and response rates as compared with chemotherapy alone. The impairment of vascular endothelial growth factor-A actions (endothelial cell proliferation and migration, endothelial cell apoptosis inhibition, extracellular matrix remodelling, vasodilatation, increase of vascular permeability) explains bevacizumab antitumoral effects together with its main side-effects. There is currently no validated prognostic or predictive marker of response to the combination of bevacizumab and standard chemotherapy. In a French substudy of the phase IIIb international multicentric MO19391 trial evaluating the safety of bevacizumab with first-line taxane-based chemotherapy in patients with metastatic breast cancer, we investigated whether CTC and circulating endothelial cell (CECs) counts were associated with the outcome (TtP, as defined in the MO19391 trial)."
187,inserm-00508113.training.tei.xml,3,This article has been written in accordance with the REporting of tumor MARKer studies criteria.
188,inserm-00508113.training.tei.xml,4,"The single-arm, prospective, multinational, observational MO19391 study enrolled >2000 patients with metastatic breast cancer to better understand the safety profile of first-line bevacizumab in combination with taxanebased therapy. Briefly, MO19391 inclusion criteria were age ‡18 years, metastatic (mBC) or nonresectable locoregional recurrence (LR) of breast cancer adenocarcinoma, HER2-negative tumors (or HER2 positive if they have progressed after previous adjuvant trastuzumab), Eastern Cooperative Oncology Group performance status of zero to two and adequate liver, kidney and hematopoietic functions. Exclusion criteria included prior chemotherapy for LR or mBC and evidence of central nervous system metastasis. Bevacizumab regimen was 10 mg/kg every 2 weeks or 15 mg/kg every 3 weeks, combined with the physician's choice of taxane regimen (or investigator's standard of care, excluding anthracyclines). Treatment with bevacizumab was continued until disease progression, unacceptable toxicity or patient withdrawal. Our ethically approved companion study was prospectively conducted in 10 centers in France between June 2007 and June 2008. All samples were obtained with the patient's written informed consent, after approval by the regional ethics committee. Neither patients nor clinicians were informed of the results of CTC and CEC analysis. Lactate dehydrogenase (LDH) and CA 15-3 levels were also investigated before the first treatment cycle (baseline) and before cycle 3 (C3). There was no prior publication on this cohort of patients."
189,inserm-00508113.training.tei.xml,5,"Briefly,two CellSaveÒ tubes of blood (one tube for CTC and the other for CEC analysis) were drawn at baseline and before C3, i.e. 6 weeks after the start of the treatment, at the time of physician consultations. Samples were maintained at room temperature and processed within 96 (CTC samples) or 72 (CEC samples) h after collection. All evaluations were carried out with no knowledge of the patient's clinical status. The standardized CellSearchÒ technique has been reported previously for CTC  and CEC  detection. Briefly, CTCs expressing the epithelial cell adhesion molecule were immunomagnetically enriched and stained with 4,2diamidino-2-phenylindole dihydrochloride (DAPI) (+), cytokeratin 8,18,19 (+) and CD45 (2). CECs expressing CD146 were immunomagnetically enriched and stained with DAPI (+), CD105 (+) and CD45 (2). CTC and CEC morphology was confirmed in all cases. Quantitative results were expressed as per 7.5 ml blood and per 4 ml blood for CTCs and CECs, respectively."
190,inserm-00508113.training.tei.xml,6,"criteria and statistical analyses CTC and CEC detection was carried out whenever possible, without any target statistical power. According to MO19391 study end points, TtP was measured from the time treatment began to the time documented progression occurred (i.e. not taking into account deaths occurring before tumor progression). Tumor response to treatment was evaluated as per standard of care and included complete and partial responses. Initially, the ‡5-CTC/7.5 ml threshold was used to define CTC positivity (for Table 1). The chi-square test or Fisher's exact test was applied for dichotomous and categorical data, and the Student's t-test or analysis of variance was used to compare continuous variables between groups. For comparison of quantitative parameters, if the normality hypothesis was not fulfilled, nonparametric tests were used as Wilcoxon test or Kruskall-Wallis test. Signed rank test was used to compare quantitative parameter between baseline and before C3. Correlation between CTCs and CECs was studied using linear regression. Kaplan-Meier survival curves were used to analyze TtP for each group of interest defined by CEC or CTC thresholds, and Cox regression model was used to provide the hazard ratio and its 95% confidence interval (CI). To correlate CTC and CEC changes under treatment with tumor response, two models were used: threshold-based and relative change-based models, the latter considering CTCs and CECs as continuous variables. Threshold-based models divided patients into four classes . For TtP analysis, in the threshold-based models, Wald test (provided by Cox model regression) compared TtP of patients classified according to their positivity status before C3, this analysis being stratified on patient's baseline status. Relative change-based model divided patients into two classes: patients with relative change (between baseline and before C3) above a threshold and patients with relative change below this threshold. For TtP prediction by CTC/CEC counts, this article reports every test made. Two-tailed P values <0.05 were regarded as statistically significant. Statistical analyses were carried out using SASÒ 9.1.3 software. results patients Sixty-seven patients have been included, of whom 63 patients (94%) had a metastatic disease and 4 patients (6%) a nonoperable locoregional relapse. Fifty-five patients (82%) received prior chemotherapy for localized breast cancer. The patient characteristics are shown in Table 1. Only two patients had a HER2-positive breast cancer, as targeted anti-HER2 treatment was not authorized in this study. Main metastatic sites were bone (n = 37, 55%), liver (n = 24, 36%) and lungs (n = 21, 31%). Chemotherapy regimens associated with bevacizumab, were mostly docetaxel and paclitaxel as single agents (n = 27 and 16 patients, respectively). The median number of bevacizumab cycles received was 10 (range 1-27); however, 11 patients (19%) discontinued bevacizumab before the fourth cycle. After a median duration of follow-up of 8.8 months (range 0.4-14.7), the median TtP was 11.9 months (95% CI 8.3 to not reached). Twenty-eight of 50 patients (56%) patients achieved an objective response and 10 patients (17%) had died at the time of analysis."
191,inserm-00508113.training.tei.xml,7,"The median CTC count was7 CTC/7.5 ml (range 0-496) at baseline (n = 65 patients assessed), dropping to 0 (range 0-71) before C3 (n = 42 patients assessed). CTC count change between baseline and before C3 was statistically significant (P < 0.001). A decrease in CTC count was observed in 32 patients (76%), in 9 patients (22%) values remained stable (at 0 CTC/7.5 ml) whereas an increase was seen in 1 patient (2%). According to the ‡5 CTC/7.5 ml threshold previously defined by Cristofanilli et al. , 35 patients (54%) were CTC positive at baseline and only 5 patients (12%) were CTC positive before C3. Detection of CTC at baseline was correlated with high tumor burden (more than 3 metastatic sites; P = 0.03), elevated LDH (P = 0.04) and elevated serum marker CA 15-3 (P < 0.001) (Table 1). Also, CTC levels ‡5 CTC/7.5 ml were highly associated with bone (P < 0.001) and liver (P = 0.003) metastatic sites."
192,inserm-00508113.training.tei.xml,8,The median CEC count was17 CEC/4 ml (range 1-769) at baseline (n = 59 patients assessed) and 26 CEC/4 ml (range
193,inserm-00508113.training.tei.xml,9,2-335) before C3 (n = 42 patients assessed). CEC count change between baseline and before C3 was statistically significant (P = 0.013). CEC levels at baseline were associated with age ‡45 years (P = 0.01) and elevated LDH (P < 0.01) (Table 1). No association was seen between CEC count and metastatic sites (type and number). We found no association between CEC levels and CTC levels or positivity (according to the ‡5 CTC/7.5 ml threshold).
194,inserm-00508113.training.tei.xml,10,"The patient characteristics at baseline were not associated with tumor response or TtP (Table 1). At baseline, CTC prognostic value was assessed using different thresholds to define CTC positivity (Table 2). In this serial, using the ‡5 CTC/7.5 ml threshold to define CTC positivity, CTC status was not predictive of tumor response (P = 0.11) nor of TtP (P = 0.09). However, among the different thresholds tested (Table 2), a threshold of ‡3 CTC/7.5 ml was a significant predictor of both tumor response and TtP (Figure 1; P < 0.05). At baseline, CEC count was not associated with tumor response or TtP, at any threshold (Table 2). We further studied whether CTC and CEC count changes under treatment may be correlated with tumor response and used as an early surrogate of TtP. Results are shown in Table 3. Strikingly, focusing on CTC count, all threshold-based and relative change-based models failed to accurately predict tumor response or TtP. On the contrary, TtP was significantly associated with CEC changes, using the threshold-based model, at the threshold of ‡20 CEC/4 ml (P < 0.01; Table 3; see also Figure 2A). More simply, TtP was also associated with the CEC status before C3, using the ‡20-CEC/4 ml threshold ( Figure 2B; P = 0.003). Among relative change-based models, a relative increase of 50% of CEC count during treatment was trended toward significance (P = 0.06). In comparison, neither LDH and CA 15-3 serum levels before C3 nor their changes between baseline and C3 were associated with TtP or to tumor response (data not shown). We checked that these results, based on patients assessed twice, were not biased by a significant difference in TtP between patients assessed only at inclusion or twice (P = 0.59 and P = 0.85 for CTCs and CECs, respectively)."
195,inserm-00508113.training.tei.xml,11,This exploratory prospective substudy is the first to report both CTC and CEC counts in metastatic breast cancer patients who were homogeneously treated by first-line chemotherapy (mostly taxanes) combined with the antiangiogenic agent bevacizumab.
196,inserm-00508113.training.tei.xml,12,"Our analysis does not confirm the clinical relevance of CTC counts reported previously by Cristofanilli et al. and Nole et al. , using the same ‡5 CTC/7.5 ml threshold as a threshold for poor outcome. In line with these results, we observed that (i) at baseline CTC was of prognostic significance for TtP and (ii) was also predictive of tumor response, using a lower threshold of ‡3 CTC/7.5 ml. However, the most striking result was that 6 weeks after the start of the treatment, irrespective of the model tested, CTC count was not associated with TtP or tumor response."
197,inserm-00508113.training.tei.xml,13,"Concerning these finding for CTC counts, a lack of power should be discussed at first, as our analysis included67 patients. By comparison, the successful validation set of the study of Cristofanilli et al.  included 75 patients, of whom about a third were treated only by hormone therapy, a clinical setting Time to progressive disease (months)"
198,inserm-00508113.training.tei.xml,14,"Wald test : p <0,05 original article"
199,inserm-00508113.training.tei.xml,15,"in which CTC count seemed to have less significance. The study of Nole et al. , which was also successful, included 80 patients: 70 patients received chemotherapy (in combination with trastuzumab for 27 patients), but only 33 patients had not received previous chemotherapy for their metastatic disease ."
200,inserm-00508113.training.tei.xml,16,The sample size in our study is therefore of similar size to that of Cristofanilli's group(validation set) and Nole et al.
201,inserm-00508113.training.tei.xml,17," and also showed a similar CTC detection rate at baseline ( ‡5 CTC/7.5 ml). Moreover, the fact that we do describe in our series the significant correlations of CTC positivity ( ‡5 CTC/ 7.5 ml) with high CA 15-3 serum marker and with bone metastatic site, which have been also reported by Nole et al.  and De Giorgi et al. , does not support a lack of power of our study. The lack of clinical relevance of CTC changes during treatment might also be due to the difference in the CTC count schedule, as it was before C2 (i.e. theoretically at day 21) in the study by Cristofanilli et al. , at day 30 in the study of Nole et al.  and before C3 (i.e. theoretically at day 42) in our study. However, a recent retrospective study of the series by Cristofanilli's group showed that later CTC count (i.e. between day 56 and day 91) retained a strong  prognostic relevance in patients treated by standard chemotherapy alone . We therefore favor the hypothesis that the lack of CTC significance during treatment is directly related to the systematic use of bevacizumab in our homogeneously treated series of patients. In our study, CTC counts were in line with previous studies at baseline but unusually low after two cycles of treatment compared with previous studies (without bevacizumab): 12% of patients had ‡5 CTC/7.5 ml here (day 42) versus 30% (day 30)  or 22% (day 42-56) . Besides a hypothetical direct anti-CTC effect of bevacizumab, two possible explanations should be discussed. First, bevacizumab gained its worldwide regulatory approval after the demonstration of a significant prolongation in PFS, leading to its use as a standard for first-line treatment . Importantly, it is highly probable that the intrinsic properties of CTC positivity, defined by a precise threshold (here, ‡5 CTC/7.5 ml was initially tested) and used as an early surrogate test of PFS, will strongly depend on the length of the PFS obtained by treatments, which are continuously improved. In other words, the ‡5 CTC/7.5 ml threshold which successfully separated early (median: 2.1 months) from late (7.0 months) progressive disease under standard chemotherapy will necessarily be challenged by the improvement of PFS due to new treatments such as bevacizumab (or trastuzumab, not used here and not reported in the series by Cristofanilli et al.). Accordingly, our results indicate that the CTC positivity threshold should be ‡3 CTC/7.5 ml rather than ‡5 CTC/7.5 ml, when used for prognostic purpose at baseline in our setting. Secondly, we also suggest that the use of bevacizumab may change the intravasation abilities of cancer cells, due to its effect on the vessels endothelium. For comparison purposes, in colorectal cancer studies, 56% and 100% of patients were treated with bevacizumab in the studies by Cohen et al. (n = 430 patients)  and Tol et al. (n = 467 patients)  studies, respectively. These two studies exhibited a similar baseline CTC positivity rate. However, CTC positivity rates reported under treatment were rather different (12% versus 5% at 5-6 weeks), inversely correlated with bevacizumab use. Intravasation impairment is a hypothesis to explain why CTC counts dropped off under treatment. This hypothesis may explain how CTC count (i) retains a significant prognostic value at baseline, as already described in metastatic colorectal cancer patients treated with bevacizumab combined with chemotherapy ; but (ii) has less individual significance as an early surrogate marker of TtP when assessed under treatment. The impact of bevacizumab on tumoral vessels has been reported in many ways since the original report of bevacizumab use in rectal cancers but our analysis is the first, to our knowledge, to indicate that it may impair tumor cell intravasation in patients."
202,inserm-00508113.training.tei.xml,18,"Published data about CECs are highly heterogeneous, obtained by various CEC-sorting techniques, in many different tumor types, and commonly reported in a small sample size. To further complicate the survey on CECs, these cells can be divided into mature CECs and progenitor CECs, whereas some chemotherapy regimens (metronomic chemotherapy) are believed to exert antiangiogenic effects beyond their cytotoxic effects. More recently, it has also been shown that some standard chemotherapies (especially taxanes) have a stimulating effect on an endothelial progenitor which supports their combined use with an antiangiogenic agent . CEC sorting by the CellSearchÒ system is based on a CD146(+), CD105(+), DAPI (+) and CD45 (2) phenotypes of CECs . It has been also validated through gene expression profiling which verified that expression of endothelial genes such as vascular endothelial cadherin was present in the population of sorted cells . Our analysis is the first to report that CEC count, by the CellSearchÒ automated system, could be a significant early surrogate marker of TtP for breast cancer patients treated by bevacizumab combined with standard chemotherapy. Among many models tested, three could be compared in a validation study: (i) a threshold-based model ( ‡20 CEC/4 ml); (ii) a relative variation model (+50%), both using the two CEC counts (baseline and day 42); and (iii) a threshold-based model at day 42 ( ‡20 CEC/4 ml). Interestingly, the barely lower statistical significance of the nearby thresholds of ‡25 CEC/4 ml and ‡15 CEC/4 ml (P < 0.05 and P = 0.07, respectively) strongly indicated that the statistical significance of the ‡20 CEC/4 ml threshold was not due to a random multiple testing artifact. Our results are in line with previous reports (using other CEC-sorting techniques) which indicated that tumor progression under antiangiogenic treatment combined with chemotherapy was associated with a significant CEC decrease . In this exploratory, hypothesis-generating study, we did not attempt to establish a combinatory model using both CTC and CEC levels."
203,inserm-00508113.training.tei.xml,19,"Finally, if our results are confirmed by another independent prospective study, CTC counts by the Food and Drug Administration-approved CellSearchÒ system should be interpreted carefully by clinicians when treating metastatic breast cancer patients with first-line chemotherapy combined with bevacizumab: clinically relevant baseline prognostic threshold may be different in this setting, and CTC changes during treatment may not be an early surrogate marker of survival. Contrarily, CECs are a promising early surrogate marker of TtP in such patients, and a validation study is required to further investigate the different CEC threshold proposals."
204,inserm-00508113.training.tei.xml,20,This study has been funded by Roche. The funding source had no role in study design.
205,inserm-00508113.training.tei.xml,21,We thank Mustapha Khazour (Institut Curie) for technical assistance and Delphine Etienne (Lincoln biostatistics) for statistical analyses.
206,hal-01078051.training.tei.xml,0,"The past decades have seen a great number of discoveries of planets around stars other than our Sun ( Schneider et al. 2011). As some of these planets are of terrestrial nature, the hope of identifying Earth analogs has lead to considerable advances toward the detection of possibly habitable worlds (Borucki 2011;Fordetal.2012). Even though quite frequent in the solar neighborhood (Kiseleva-Eggleton & Eggleton 2001), not many attempts have yet been made to specifically target binary stars in this endeavor. Nonetheless, more than60 planets have already been found in and around such systems (Haghighipour 2010;Doyle et al. 2011;Welsh et al. 2012;Roell et al. 2012;O r o s z et al. 2012a, 2012bDumusque et al. 2012). Although several P-type (circumbinary) planets orbiting both stars of a close binary have also been discovered ( Doyle et al. 2011;W e l s h et al. 2012;Orosz et al. 2012aOrosz et al. , 2012b), most planets are in the so-called S-type ( Rabl & Dvorak 1988) configuration where the planet orbits only one of the binary's stars. A prominent example of an S-type system is α Centauri AB which hosts a terrestrial planet around the fainter binary component, α Cen B ( Dumusque et al. 2012)."
207,hal-01078051.training.tei.xml,1,"The reason for the general reluctance to include binary systems in the search for terrestrial, habitable planets lies in the assumption that the additional interactions with a massive companion will make planets harder to find. That is primarily because the gravitational interaction between the second star and a planet may alter the planet's orbit significantly and complicate the task of interpreting the planetary signal. One aim of this work is therefore to show that changes in the planet's orbit can actually enhance its detectability (see Section 5). Of course, the orbit of a binary as well as its stellar parameters have to be well determined in order to be able to identify signals from additional terrestrial planets. Sensing the need for a better understanding of binary star systems, efforts have been intensified to improve physical as well as orbital data for nearby binaries (e.g., Torres et al. 2010) and to evolve existing data analysis methodologies ( Chauvin et al. 2011;Haghighipour 2010;Pourbaix 2002;Pourbaix et al. 2002)."
208,hal-01078051.training.tei.xml,2,"Understanding the complex interactions between a stellar binary and a planet is essential if a system's potential habitability is to be evaluated. For instance, one of the main assumptions of classical habitability, as introduced by Kasting et al. (1993), is that the planet moves around its host star on a circular orbit. This may not be a valid assumption for planets in a binary star system where the gravitational perturbation of the secondary can excite the eccentricity of the planet's orbit (Marchal 1990;Georgakarakos 2002;Eggl et al. 2012). Eggl et al. (2012) found that except for S-type systems where the secondary star is much more luminous than the planet's host star, variations in planetary orbit around the planet-hosting star are the main cause for changes in insolation. Even though Eggl et al. (2012)g a v ea n analytic recipe for calculating the boundaries of the habitable zones (HZs) in S-type binaries, it remains to be seen whether an Earth-like planet in the HZ of a system withtwo Sun-like stars will in fact be detectable."
209,hal-01078051.training.tei.xml,3,"In order to answer this question, we considerthree techniques, namely, radial velocity (RV), astrometry (AM), and transit photometry (TP), and discuss whether the current observational facilities are capable of detecting habitable planets in such systems. We provide analytical formula for estimating the strength of RV and AM signals for habitable, Earth-like planets, and show that the planet-binary interaction can enhance the chances for the detection of these objects."
210,hal-01078051.training.tei.xml,4,"The rest of this article will be structured as follows. In Sections 2 and 3 analytic estimates of the maximum and root mean square (rms) of the strength of an RV and an AM signal that an Earth-like planet produces in an S-type binary configuration will be derived. Section 4 will deal with the consequences of such a setup for TP. We will then briefly recall the different types of HZs for S-type binaries established in Eggl et al. (2012), and use their methodology to identify similar habitable regions in the α Centauri system (Section 5). This system has been chosen because first, it has inspired many studies on the possibility of the formation and detection of habitable planets around its stellar components (Forgan 2012;Guedes et al. 2008;Thébault et al. 2009) and second, Dumusque et al. (2012) have already discovered an Earth-sized planet in a short-period orbit around its secondary star. Therefore, we will compare our RV estimates to the actual signal of α Cen Bb, and study its influence on an additional terrestrial planet presumed in α Cen B's HZ. Finally, in Section 6, the projected RV, AM, and TP trace that terrestrial planets will leave in the HZ of the α Centauri system are analyzed, and the results are discussed within the context of the sensitivity of the current observational facilities."
211,hal-01078051.training.tei.xml,5,"To estimate the RV signal that an Earth-like planet produces in an S-type binary system, we will build upon the formalism presented by Beaugée ta l . ( 2007). We assume that the nonplanetary contributions to the host star's RV signal (such as the RV variation caused by the motion of the binary around its center of gravity) are known and have been subtracted, leaving behind only the residual signal due to the planet. The motion of the planet around its host star then constitutes a perturbedtwo-body problem, where the gravitational influence of the secondary star is still playing a role and is mirrored in the forced variations of the planet's orbit."
212,hal-01078051.training.tei.xml,6,"In practice, the extraction of the planetary signal is all but a trivial task. Even after subtraction of the binary's barycentric and proper motion, the residual will contain contributions from the binary's orbital uncertainties as well as from non-gravitational sources which could be orders of magnitude larger than the star's reflex signal, such as the Rossiter-McLaughlin effect in transiting systems, for example ( Ohta et al. 2005). The discovery of α Cen Bb showed, however, that a substantial reduction of non-planetary RV interference is possible if the respective binary star has been studied in sufficient detail."
213,hal-01078051.training.tei.xml,7,"The amplitude of the planet induced RV signal of the host star, V r , is given by"
214,hal-01078051.training.tei.xml,8,where K is equal to
215,hal-01078051.training.tei.xml,9,"In Equation (2), μ = m 1 /(m 0 + m 1 ) is the planet to star mass ratio with m 1 and m 0 being the masses of the planet and host star, respectively. The planet's mean motion, n p = 2π/P p ,i s given by n p = 񮽙 κ/a 3 with κ = G(m 0 + m 1 ), and P p and G being the planet's orbital period and gravitational constant. The quantities a, e, i, f, and ω in Equations (1) and (2) denote the planet's semimajor axis, eccentricity, orbital inclination relative to the plane of the sky, true anomaly, and argument of periastron, respectively."
216,hal-01078051.training.tei.xml,10,"Our goal in this section is to identify the range of the possible peak amplitudes that a terrestrial planet in an S-type binary configuration can produce. We note that the gravitational influence of the second star causes the planet's orbital elements to vary, thus inducing additional time-dependent changes in the RV signal V r (Lee & Peale 2003). While we know from secular perturbation theory that a does not change significantly with Beaugé et al. 2007). The planet is at1 AU with ω = 45 • when e 񮽙 = 0. Bottom: the amplitude variations of the primary's radial velocity signal due to an Earth-like planet that is subject to the gravitational perturbations of a second star. Both stars are Sun-like with a separation of 20 AU and an orbital eccentricity of 0.5. The planet's initial orbit was circular with a semimajor axis of 1 AU. Our analytically estimated maximum amplitude V max r is also shown. (A color version of this figure is available in the online journal.) time for hierarchical systems such as the one under consideration (Marchal 1990;Georgakarakos 2003), ω becomes a function of time. We assume coplanar orbits of the planet and the binary star which result in the planet's inclination to the plane of the sky (i) to remain constant. In contrast, the planetary eccentricity will vary between zero and a maximum e max , where the latter value can be expressed as a function of the system's masses and the binary's orbital parameters ( Eggl et al. 2012). This is important, because the reflex RV signal (V r ) of a star can be increased significantly by planetary orbital eccentricities (Figure 1). Using Equation (1), we identify the global maximum of V r at f = ω = 0, when e = e max. This leads to"
217,hal-01078051.training.tei.xml,11,where
218,hal-01078051.training.tei.xml,12,"Equation (3) presents a fully analytic estimate of the expected maximum RV signal that a terrestrial planet produces in an S-type binary configuration. As an example for the influence of a double star on a planetary RV signal, the induced variations in the RV of the planet's host star are presented in the bottom panel of Figure 1. The host star is a constituent of a solar-type binary with a semimajor axis of 20 AU and an orbital eccentricity of 0.5. Changes in the amplitude of V r are due to variations in the planet's eccentricity."
219,hal-01078051.training.tei.xml,13,"Since we do not know the state of the planet's orbital eccentricity at the time of observation, we consider a range for the maximum possible amplitudes of its RV"
220,hal-01078051.training.tei.xml,14,"Although the range of the amplitude of the host star's RV signal, as given by Equation (5), can be used to identify the ""best case"" detectability limits, the maximum values of the RV signal due to the planet will be ""snapshots"" that are reached only during brief moments. As a result, their values for assessing the precision needed to trace fingerprints of an Earth-like planet are rather limited. In such cases, expressions for the rms of the astrometric signal are preferable."
221,hal-01078051.training.tei.xml,15,"Since rms values are by convention time-averaged, we substitute f by the mean anomaly M = n p t in all corresponding functions using the equation of the center expansion up to the sixth order in planetary eccentricities (see Appendix A) and average over M and ω. The vastly different rates of change of these quantities ( ˙ M ≫˙ ω) make it possible to consider ω to remain constant duringone cycle of M, so that independent averaging can be performed. In order to eliminate short-term variations in the RV signal, we first average over M. Averaging over ω as well might be desirable if for example the initial state of ω is unknown, or if observations stretch beyond secular evolution timescales of the planets argument of pericenter. We, therefore, define two different types of rms evaluations for a square-integrable function F:"
222,hal-01078051.training.tei.xml,16,and
223,hal-01078051.training.tei.xml,17,"Using Equations (6) and (7), the rms values of V r are then given byLarger signals are possible, if the terrestrial planet has a considerable initial eccentricity after its formation and migration phase. Yet, due to the eccentricity dampening in protoplanetary disks, this seems unlikely (Paardekooper & Leinhardt 2010)."
224,hal-01078051.training.tei.xml,18,with
225,hal-01078051.training.tei.xml,19,Here we have considered 񮽙a񮽙 M = a since ˙ a ≃0 (Marchal 1990; Georgakarakos 2003). Also
226,hal-01078051.training.tei.xml,20,"as indicated in Appendix B. It is noteworthy that the averaging over ω causes the rms value of the RV signal to become independent of e so that its difference with the peak signal in the circular case (V circ r ) becomes a mere factor of1/ √ 2. Thanks to their intricate relation to power spectra, rms values can also be valuable for orbit fitting. The choice of singly or doubly averaged rms relations for this purpose will depend on how many planetary orbital periods are available in the data set. In the case of α Cen Bb, there are order-of-magnitude differences in the rates of change of the mean anomaly ( ˙ M) and the argument of pericenter ( ˙ ω). It would therefore make more sense to assume ω to be constant and add it as a variable in the fitting process. If stronger perturbations or additional forces act on the planet, the periods can be considerably shorter, so that the fully averaged equations might come in handy."
227,hal-01078051.training.tei.xml,21,"In order to derive the maximum and rms values for an astrometric signal, we will use the framework presented in Pourbaix (2002). We again assume that the non-planetary contributions have been subtracted from the combined signal of the host star and planet. The projected motion of the planet on the astrometric plane is then given by"
228,hal-01078051.training.tei.xml,22,"where x E and y E are the Cartesian coordinates of the projected orbit, e is the planet's orbital eccentricity, E is the eccentric anomaly, and A, B, F, and G are the modified Thiele-Innes constants given by"
229,hal-01078051.training.tei.xml,23,"In these equations, d is the distance between the observer and the observed system in units of the planetary semimajor axis a. We can rewrite Equations (10) in terms of the true anomaly f as"
230,hal-01078051.training.tei.xml,24,"In these equations, r = a(1 − e 2 )/(1 + e cos f ) represents the planet's radial distance to its host star. Because the motion of the planet itself cannot be traced, we translate these equations into the apparent motion of the host star by the application of Newton's third law. That is"
231,hal-01078051.training.tei.xml,25,"Here, X and Y are the projected coordinates of the center of mass of the planet-star system, and μ denotes the planet-star mass ratio as defined for Equation (2). Assuming without the loss of generality that the barycenter of the star-planet system coincides with the origin of the associated coordinate system, the distance of the projected stellar orbit to the coordinate center will be equal to"
232,hal-01078051.training.tei.xml,26,"The right-hand side of Equation (14) is independent of Ω and has a global maximum at f = π, ω =0 when e = e max .This translates into a maximum astrometric amplitude given by"
233,hal-01078051.training.tei.xml,27,where
234,hal-01078051.training.tei.xml,28,The planetary maximum AM signal will again lie between ρ circ and ρ max. A remarkable feature of ρ max and ρ circ is their independence of the system's inclination i. This is visualized in Figure 2. The same figure also shows the time evolution of the AM signal due to an Earth-like planet orbiting α Cen B at a distance of1 AU.
235,hal-01078051.training.tei.xml,29,The astrometric rms values are given by
236,hal-01078051.training.tei.xml,30,and
237,hal-01078051.training.tei.xml,31,"Details regarding the derivation of Equations (17) and (18) can be found in Appendix B. In contrast to the doubly averaged equations for the rms of an RV signal, Equation (18)s h o w sa dependence on the binary's eccentricity. In cases where the planetary inclination i coincides with the inclination of the binary itself, analytic expressions for 񮽙e 2 񮽙 M,ω are available (Georgakarakos 2003(Georgakarakos , 2005).The analytic expressions given in these articles are also averaged over initial phases, i.e., different relative starting positions of the planet and the binary stars."
238,hal-01078051.training.tei.xml,32,", due to an Earth-like planet orbiting its Sunlike host star. The planet's orbital elements are a =1A U ,e = e max = 0.5, ω = 0, Ω = 111 •. As shown here, the maximum distance from the origin of the coordinate system is independent of the system's inclination with respect to the plane of the sky (i). Bottom: evolution of the astrometric signal (x f ,y f ) caused by an Earth-like planet in a binary star system. The planet is orbiting α Cen B at a distance of 1 AU. The evolution of the astrometric signal is shown for 3750 periods of α Centauri AB. Since the system is coplanar, the changes in orientation and shape of the projected ellipse are due to variations in the planet's eccentricity (e) and argument of pericenter (ω)."
239,hal-01078051.training.tei.xml,33,"In TP, signal strength is equivalent to the relative depth of the dint the planet produces in the stellar light curve during its transit. Assuming that the star-planet configuration allows for occultations, and excluding grazing transits, the fractional depth of the photometric transit (TD) produced by an Earth-like planet is simply given by the proportion of the luminous area of the disk of the star that is covered by the planet as the planet moves between the observer and the star. Ignoring limb darkening, that means, TD ≃ R 2 p /R 2 ⋆ where R p is the radius of the planet and R ⋆ is the stellar radius. The overall probability to observe a transit Table 1 Physical and Orbital Parameters of the α Centauri ABb System ( Kervella et al. 2003;Guedes et al. 2008;Pourbaix et al. 2002;Dumusque et al. 2012)"
240,hal-01078051.training.tei.xml,34,
241,hal-01078051.training.tei.xml,35,231.65 ± 0.076
242,hal-01078051.training.tei.xml,36,204.85 ± 0.084
243,hal-01078051.training.tei.xml,37,is given by (Borucki & Summers 1984)
244,hal-01078051.training.tei.xml,38,"In Equation (19), r T is the radial distance of the planet to the star during the transit. For an eccentric planetary motion, the planet-star distance during transit can be expressed as et al. 2008), where ¯ ω denotes the argument of pericenter measured from the line of sight.In analogy to Sections 2 and 3, the maximum and averaged transit probability for a planet perturbed by the secondary star in a planar configuration can be calculated by substituting for r T in Equation (19) and averaging over ¯ ω. This will result in"
245,hal-01078051.training.tei.xml,39,and
246,hal-01078051.training.tei.xml,40,"Equations (20) and (21) indicate that the increase in the eccentricity of the planet due to the perturbation of the secondary increases the probability of transit. In deriving these equations, we have ignored the occultation of the planet by the second star. However, depending on the period ratio between the secondary and the planet, such conjunctions are either scarce or short-lived. Consequently, their contribution to the probability of witnessing a planetary transit is negligibly small."
247,hal-01078051.training.tei.xml,41,"In this section, we will show that the previously derived analytic expressions produce results that are in good agreement with the current observations of α Cen Bb. We will also present numerical evidence that the presence-or the absence-of an additional terrestrial planet in the HZ of α Cen B cannot be derived easily from the orbit evolution of α Cen Bb. Consequently, we argue that an independent detection of additional Notes. Coplanarity of the system was assumed. The formal uncertainties have been derived assuming Gaussian error propagation of the uncertainties given in Table 1. The maximum predicted planetary eccentricity for α Cen Bb is e max =0.003. When taking general relativity into account, however, the orbit of α Cen Bb will remain practically circular (see Figure 6). terrestrial companions might be difficult, but more promising. For this purpose, we will determine the HZ of α Cen B, as well as the RV, AM, and TP signatures of an Earth-like planet orbiting in the HZ of α Cen B. Since there is no a priori reason why the brighter component of α Centauri could not be hosting a terrestrial planet as well, we perform a similar analysis for α Cen A. We will also study the behavior of Equations (1) (21) for a broad range of binary eccentricities."
248,hal-01078051.training.tei.xml,42,"The planet discovered around α Cen B offers a perfect opportunity to compare the RV amplitude predictions derived in Section 2 with actual measurements. The planet's known orbital parameters are given in Table 1. In Table 2, we present the analytic estimates of Section 2 applied to the α Centauri ABb system. This is not surprising, since the planetary parameters were derived from an RV signal using the same methodology in reverse. While still well within measurement uncertainties, the deviation of the maximum RV amplitude (V max r ) from the observed value is larger than that of V circ r. On the one hand, this might indicate that the planet is currently in an orbital evolution phase where its eccentricity is almost zero. On the other hand, the planet may be too close to its host star for our model to predict V max r correctly. In fact, we show in Section 5.3 that the latter explanation is more likely, since the influence of general relativity (GR) cannot be neglected in this case. Estimates based on Newtonian physics exaggerate the actual eccentricity of α Cen Bb. Its orbit remains practically circular despite the interaction with the binary star (see Section 5.3 for a detailed discussion). This justifies the assumption of a circular planetary orbit made by Dumusque et al. (2012)."
249,hal-01078051.training.tei.xml,43,"Since we are especially interested in additional habitable planets, however, it is worthwhile to ask whether predictions on the orbital evolution of α Cen Bb can be used to exclude the presence of other gravitationally active bodies in the system. In other words, could an Earth-like planet still orbit in the HZ of α Cen B or would the accompanying distortions of the orbit of α Cen Bb be significant enough to detect them immediately? Before we try to answer these questions, we need to briefly recall some important aspects regarding HZs in binary star systems."
250,hal-01078051.training.tei.xml,44,"Combining the classical definition of an HZ ( Kasting et al. 1993) with the dynamical properties of a planet-hosting double reacts strongly to enhanced binary eccentricities (top row, curved, solid lines) whereas in contrast, 񮽙񮽙V r 񮽙񮽙 M,ω is independent of the binary's eccentricity (bottom row, straight, vertical lines). The straight, vertical lines in the top row correspond to RV amplitudes for circular orbits (V circ r ). See Section 5.4 for details. star system, Eggl et al. (2012) have shown that one can distinguishthree types of HZ in an S-type binary system."
251,hal-01078051.training.tei.xml,45,"The permanently habitable zone (PHZ) where a planet always stays within the insolation limits (S I , S O ) as defined by Kasting et al. (1993) and Underwood et al. (2003). In other words, despite the changes in its orbit, the planet never leaves the classical HZ. The total insolation the planet receives will vary between the inner (S I ) and outer (S O ) effective radiation limits as S I S tot S O where, for a given stellar spectral type, S I and S O are in units of solar constant (1360 W m −2 ). The extended habitable zone (EHZ) where, in contrast to the PHZ, parts of the planetary orbit may lie outside the HZ due to the planet's high eccentricity, for instance. Yet, the binary-planet configuration is still considered to be habitable when most of the planet's orbit remains inside the boundaries of the HZ. In this case, 񮽙S tot 񮽙 t + σ S I and 񮽙S tot 񮽙 t − σ S O where 񮽙S tot 񮽙 t denotes the time-averaged effective insolation from both stars and σ 2 is the effective insolation variance. The averaged habitable zone (AHZ). Following the argument of Williams & Pollard (2002) that planetary eccentricities up to e<0.7 may not be prohibitive for habitability as long as the atmosphere can act as a buffer, the AHZ is defined as encompassing all configurations which support the planet's time-averaged effective insolation to be within the limits of the classical HZ. Therefore,"
252,hal-01078051.training.tei.xml,46,"Analytic expressions for the maximum insolation, the average insolation (񮽙S tot 񮽙), and insolation variance that a planet encounters in a binary system have been derived in Eggl et al. (2012). We refer the reader to that article for more details. Figures 3 and 4 show the application of the proposed habitability classification scheme to the α Centauri system. In these figures, blue denotes PHZs, green shows EHZs, and yellow corresponds to AHZs. The red areas in Figures 3 and 4 are uninhabitable, and purple stands for dynamically unstable regions. Table 1 shows the physical properties of the system. We used the formulae by Underwood et al. (2003) to calculate S I and S O for the given effective temperatures of α Cen A and B. In general, these formulae allow for extending the analytic estimates for HZs, as given by Eggl et al. (2012), to main-sequence stars with different spectral types. Runaway greenhouse and maximum greenhouse insolation limits were used to determine the inner and outer boundaries of HZs, respectively. In the bottom panels, the vertical lines represent areas of equal rms amplitudes, 񮽙񮽙ρ񮽙񮽙 M,ω. One can see that planetary orbits with dynamically enhanced eccentricities can have smaller semimajor axes and still produce similarly high astrometric amplitudes as circular orbits which are more distant from the host star. As shown in Figures 3 and 4, the locations of the HZs and the detectability of habitable planets in those regions depend strongly on the eccentricity of the binary (e b ). The actual eccentricity of the α Centauri system is denoted by a horizontal line at e b =0.5179. The values for the borders of the different HZs using α Centauri's actual eccentricity are listed in Table 3. As shown here, both stars permit dynamical stability for habitable, Earth-like planets. Due to the difference in stellar luminosities, the HZs around α Cen A are larger and farther away from the host star compared to α Cen B. Since the binary's mass ratio is close to 0.45, the gravitational influence of α Cen B is more pronounced on the PHZ of α Cen A. This is a consequence of the larger injected planetary eccentricities (e p ) as can be seen from the top row of Figure 5. The relatively larger gravitational influence of α Cen B onto the HZ of α Cen A is also mirrored in the fact that the region of dynamical instability (meshed, purple online) reaches toward lower binary eccentricities. The change in the range and configuration of HZs with the change in planetary semimajor axis and eccentricity of the binary is pronounced. A clear shrinking trend for PHZ and EHZ can be observed for high values of the binary's eccentricity. While as shown by Eggl et al. (2012), the AHZ in general expands slightly when the eccentricity is enhanced, Figures 3 and 4 show that in the α Centauri system, this HZ depends only weakly on e b , making it the closest approximation to the classical HZ as defined by Kasting et al. (1993). Comparing these results with the existing studies on the HZs for α Cen B such as Guedes et al. (2008) and Forgan (2012), one can see that the values of the inner boundaries of the HZs around α Cen B as given in Figures 3 and 4 coincide well with the previous studies. Forgan (2012) even found a similar shrinking trend with higher planetary eccentricity. Yet, Forgan (2012) did not take the actual coupling between the planet's eccentricity and the binary's orbit into account. The limits for the outer boundaries of HZ in our model are different from the ones in Forgan (2012) since different climatic assumptions were made. In this work, we used insolation limits for atmospheric collapse assuming a maximum greenhouse atmosphere ( Kasting et al. 1993) whereas Forgan (2012) focused on emergence from snowball states."
253,hal-01078051.training.tei.xml,47,"While the classification of HZs presented in the previous section is globally applicable to binary star systems, the analytic estimates to calculate their extent ( Eggl et al. 2012) are only strictly valid forthree-body systems, e.g., the binary star and a planet. Additional perturbers will influence the shape and size of the HZs. It is thus necessary to investigate which effect the already discovered planet around α C e nBw o u l dh a v eo na n additional terrestrial planet in α Cen B's HZ."
254,hal-01078051.training.tei.xml,48,"If the mutual perturbations were large, the HZ boundaries given in Table 3 would have to be adapted, but α Cen Bb's orbital evolution could also contain clues on the presence-or the absence-of an additional planet. Should the interaction between the inner planet and an additional terrestrial body in the HZ be small, then the HZ boundaries would hold. However, a detection of the habitable planet via its influence on α Cen Bb's orbit would become difficult."
255,hal-01078051.training.tei.xml,49,"In Figure 6, results of numerical investigations on the coupled orbital evolution of an additional terrestrial planet and α Cen Bb are presented. The top row of Figure 6 shows the eccentricity evolution of α Cen Bb altered by an additional Earth-like planet at the inner and outer edge of α Cen B's AHZ. Figure 6. Additional terrestrial planet in α Cen B's HZ affects the orbit of α Cen Bb (top row) and vice versa (bottom row). In the top left panel, the numerically computed evolution of the eccentricity of α Cen Bb in the Newtonianthree-body problem (3BP) consisting of the binary α Cen AB and the planet α C e nB bi s compared with different four-body problem scenarios (4BP). In one scenario, an additional Earth-sized body orbits α Cen B at the inner edge of its AHZ (see Table 3)."
256,hal-01078051.training.tei.xml,50,"In the other scenario, the terrestrial planet is assumed to be at the outer edge of α Cen B's AHZ. The analytic estimate for the maximum eccentricity (e max )i nt h e3BP is presented as well (horizontal line). The top right panel shows the exact same setup, only with general relativity (GR) taken into account. The orbit of α Cen Bb becomes practically circular. While the influence of an additional planet at the outer edge of the HZ is barely noticeable in the eccentricity evolution of α Cen Bb, a planet at the inner edge of the AHZ would cause distinct features. In contrast, neither GR nor α Cen Bb will influence the eccentricity evolution of planets (e p )in the HZ significantly as is shown in the bottom left panel. Also, the semimajor axes evolution of additional planets in the HZ is negligible (bottom right panel). Here, Δa p /a p denotes the normalized difference between the 4BP+GR and the 3BP semimajor axis evolution of planets at the inner and outer edges of α Cen B's AHZ, respectively."
257,hal-01078051.training.tei.xml,51,"(A color version of this figure is available in the online journal.) reference curve (dashed, blue online) represents α Cen Bb's eccentricity influenced only by the binary α Cen AB. The top left panel of Figure 6 shows the results in Newtonianthree (3BP) and four (4BP) body problems. The top right panel depicts similar analysis with GR  included. The difference between the two approaches is quite pronounced, as GR clearly prevents the secular rise in Cen Bb's eccentricity predicted in the classical setup ( Blaes et al. 2002;Fabrycky & Tremaine 2007). Thus, the orbit of α Cen Bb stays circular, even when tidal forces are neglected. The variations in semimajor axis (Δa)forα Cen Bb are not shown, because they remain below 10 −8 AU for all cases. GR was introduced by numerically solving the Einstein Infeld Hoffman equations ( Einstein et al. 1938) for the respective systems."
258,hal-01078051.training.tei.xml,52,"A possible method to search for additional companions is to measure variations in α Cen Bb's orbital period. Yet, the small Δa values make this approach difficult, since ΔP p ∝ P 1/3 p Δa. Disentangling the effects of GR and perturbations due to other habitable planets on α Cen Bb's period would require precisions several orders of magnitude greater than currently available. The top right panel in Figure 6 shows that the perturbations an additional planet at the inner edge of α Cen B's AHZ causes in α Cen Bb's eccentricity are, in principle, distinguishable from the nominal signal. Unfortunately, it is also clear from this graph that neither the required precision nor the observational timescales necessary to identify the presence of an additional Earth-sized companion via observations of α Cen Bb's eccentricity seem obtainable in the near future. For habitable planets at the outer edge of α Cen B's AHZ the chances for indirect detection seem even worse, as their influence on α Cen Bb's orbit is negligible."
259,hal-01078051.training.tei.xml,53,"In order to confirm that the interaction between α Cen Bb and Earth-like planets in the HZ is small, as well as to further study the influence of the GR on the dynamics of the system, we examined the orbital evolution of a fictitious habitable planet in that region. The results are shown in the bottom row of Figure 6. The left panel depicts the eccentricity evolution of additional terrestrial planets positioned at the inner and outer edges of α Cen B's AHZ. The secular variations in the eccentricity (bottom left panel) and semimajor axis (bottom right panel) of the habitable planet were computed numerically, taking the influence of the binary α Cen AB, the planet α Cen Bb, as well as GR into account. When comparing the analytic estimates of e max with the evolution of the habitable planet's eccentricity in the full system, it is evident that neither GR nor α CenBbalter the results for planets in α Cen B's HZ significantly. Also, the deviation in the habitable planet's semimajor axis due to GR and α CenBb(Δa p ) remains below0.1% and 0.5% for planets at the inner and outer edge of α Cen B's AHZ, respectively."
260,hal-01078051.training.tei.xml,54,"We conclude that the interaction between additional terrestrial planets in α Cen B's HZ and α Cen Bb is indeed small. Thus, our estimates for the HZs of the α Centauri system remain valid. The existence of additional terrestrial planets on the other hand cannot be determined easily from observing the orbital evolution of α Cen Bb."
261,hal-01078051.training.tei.xml,55,"The presented results are, strictly speaking, only valid for a coplanar configuration, i.e., the binary and both planets are in the same orbital plane. Mutually inclined configurations can exhibit much more involved dynamics such as Kozai resonant behavior (see, e.g., Correia et al. 2011). A detailed study of such effects lies beyond the scope of this work. Nevertheless, the arguments presented in this section suggest that the search for an additional coplanar planet in the HZ around α Cen B will most likely have to be performed without relying on observations of α Cen Bb. We will therefore investigate whether habitable planets can actually be detected independently in Sun-like binary star configurations using current observational facilities."
262,hal-01078051.training.tei.xml,56,"We apply our methodology, as derived in Sections 2 and 3, to a fictitious terrestrial planet in the HZ of binary systems similar to α Centauri AB but with a broadened range of binary eccentricities. In addition to the habitability maps discussed in Section 5.2, Figures 3 and 4 show the results regarding the peak and rms strength of the RV and astrometric signals. Here, the aim is to illustrate how the different types of HZs presented in Section 5.2, as well as the maximum and rms signal strengths defined in Section 2 vary with the binary's eccentricity (e b ) and planetary semimajor axis (a p ). The left column of almost coincide. The same holds true for ρ max and ρ circ in this case. Yet, V max r and ρ max grow with the binary's eccentricity. The corresponding contour lines indicate that for high binary eccentricities even small planetary semimajor axes can produce similar AM peak signal strengths. Similarly, planets with larger distances to their host stars can still cause similar RV amplitudes if the binary's eccentricity is sufficiently large. If a fixed detection limit is set, e.g., V r =9.5m/s, planets with semimajor axes up to 1.5A U could still be found around stars similar to α Cen A, assuming a binary eccentricity of e b = 0.7. To produce a similarly high RV amplitude, a circular planet has to orbit its host star at roughly 0.8 AU (Figure 3). In other words, high binary eccentricities lead to excited planetary eccentricities which in turn increase the peak signal strengths suggesting that binary-planet interactions can actually improve the chances for detecting terrestrial planets. Naturally, if the planet's eccentricity happens to be close to zero at the time of observation, this advantage is nullified."
263,hal-01078051.training.tei.xml,57,The slight curvature of the contour lines representing the rms signal in Figure 4 indicates this behavior. A summary of RV and AM signal strengths for an Earth-like planet at the boundaries of α Centauri's HZ is presented in Table 3.
264,hal-01078051.training.tei.xml,58,"We illustrated in this section that the dynamical interactions between a terrestrial planet and the secondary star can produce large peak amplitudes which may enhance the detectability of the planet with the RV and AM methods considerably. The rms values of the planet's AM and RV signals, on the other hand, remain almost unaffected by the gravitational influence of the secondary star."
265,hal-01078051.training.tei.xml,59,"To assess the detectability of a terrestrial planet in the HZ of α Centauri AB (and similar binaries) through TP, we calculated the relative transit depths that an Earth-like planet would produce during its transit. If such a system hosted a transiting terrestrial planet, TD values would range around55 ppm for α Cen A, and 115 ppm for α Cen B. Such transit depths are detectable by NASA's Kepler telescope for instance-stellar and instrumental sources included-as the spacecraft's median noise level amounts to ≈29 ppm ( Gilliland et al. 2011). Therefore, Earth-like planets could in theory be found around α Centauri stars. However, Kepler was not designed to observe stars with apparent magnitudes between 0 and 3 such as those of α Centauri. The Transiting Exoplanets Survey Satellite mission, for instance, will aim for TP of brighter stars ( Ricker et al. 2010). Nevertheless, the example of Kepler suggests that the detection of transiting habitable planets in S-type systems would be possible using current technology. In fact, very much similar to the cases discussed in the previous section, the orbit forcing that an Earth-like planet experiences in a binary star system may enhance its possibility of detection via TP (Kane & von Braun 2008;Kane et al. 2012;Borkovits et al. 2003;Schneider 1994; also see Figure 7). Assuming α Centauri was a transiting system,  a comparison of the transit Graphs of the ratio of transit probabilities (p T /p Tc ) with p T ≡ p T | e=e max and p Tc ≡ p T | e=0 , in a binary similar to the α Centauri system. The graphs show the transit probabilities in terms of the planet's argument of pericenter ( ¯ ω), as measured from the line of sight for component A (top) and component B (bottom). The curved full lines correspond to the planet starting at the inner border of AHZ and dashed-curved line represents planets that started at the outer edge, for each star (see Table 3). (A color version of this figure is available in the online journal.) probabilities of actual planetary orbits to circular orbits shows that an 18% increase in p T values seems possible for terrestrial planets at the outer edge of α Cen A's AHZ (Figure 7). Given the right orbital configuration, it may be more likely to identify a transiting habitable terrestrial planet around a stellar component of a binary than around a single star assuming similar initial planetary eccentricities."
266,hal-01078051.training.tei.xml,60,The increase in transit probability for planets in double star systems is less dramatic when the equations are averaged over all possible configurations of the argument of pericenter as in Equation (21).
267,hal-01078051.training.tei.xml,61,"Comparing the quantitative estimates of RV, AM, and TP signals, TP seems to be the best choice for finding Earth-like planets in the HZs of a coplanar S-type binary configuration with Sun-like components. Even for a system as near as α Centauri, AM peak signals only measure μas. Unfortunately, neither ESO's very long baseline interferometry with PRIMA nor ESA's Gaia mission will be able to deliver such precision in the near future ( Quirrenbach et al. 2011). Gaia's aim to provide μas AM will most likely not be achieved until the end of the mission ( Hestroffer et al. 2010). Also, from an astrometric point of view, Earth-like planets would be easier to find around α Cen A than α Cen B. That is because the HZ around star A is more distant from this star. Naturally, the opposite is true for RV detections. Due to the difference in the stellar masses, α Cen B offers a better chance of finding a terrestrial planet there using RV techniques. The recent discovery of an Earth-sized planet around this star supports our results. The observed planetary RV signal was reproduced excellently by our analytic estimates for circular planetary orbits."
268,hal-01078051.training.tei.xml,62,"Our prediction of RV amplitudes for terrestrial planets around α Cen B are also in good agreement with those presented by Guedes et al. (2008). Thefour terrestrial planets used in the RV model by these authors produce almost exactly four times the predicted rms amplitude given in Figure 3. Guedes et al. (2008) claim that Earth-like planets in the α Centauri are detectable even for signal-to-noise ratios of single observations below 0.1. However, obtaining sufficient data to reconstruct the planetary signal requires a great amount of dedicated observing time (approximately five years in their example). Validating this statement, it took Dumusque et al. (2012) about four years of acquired data to detect α Cen Bb. The data published by Dumusque et al. (2012) also allow a glimpse on the current performance of the HARPS spectrograph revealing a precision around 50-80 cm s −1. Given the fact that the RV signal of a habitable planet around α Cen B would be still half an order of magnitude smaller (Figure 3), considerably more observation time would be required to identify habitable companions. HIRES measurements are currently yielding precisions around 1ms −1. Identifying RV signals of habitable worlds around α Cen B therefore seems even more unlikely when using HIRES. The previous examples show that some development of observational capacities is still necessary to achieve the RV resolution required for discovering habitable planets in the α Centauri system."
269,hal-01078051.training.tei.xml,63,"The success of NASA's Kepler space telescope in identifying countless Earth-sized planetary candidates (e.g., Borucki 2011) that require follow-up observations might provide the necessary momentum to develop instruments capable of resolving RV signals in the range of cm s −1. Focusing on less massive binaries would have the advantage of having greatly enhanced RV signals as the HZs will be situated closer to the planet's host stars. How far this might simplify the task of finding habitable worlds will be the topic of further investigations."
270,hal-01078051.training.tei.xml,64,"In regard to TP, both Kepler and CoRoT telescopes have proven that it is possible to find terrestrial planets around Sunlike stars (e.g., Léger et al. 2009;Borucki et al. 2012). The combination of proven technology and the presented argument that the dynamical environment in binary star systems will enhance transit probabilities makes photometry currently the most promising method for finding Earth-like planets in the HZs of S-type binary star systems."
271,hal-01078051.training.tei.xml,65,"In this work, we provided an analytic framework to estimate the detectability of a terrestrial planet using RV, AM, as well as TP in coplanar S-type binary configurations. We have shown that the gravitational interactions between the stars of a binary and a terrestrial planet can improve the chances for the planet's detection. The induced changes in the planet's eccentricity enhance not only RV and AM peak amplitudes, but also the probability to witness a planetary transit. Next to the presented ""best case"" estimates, we offered rms/averaged expressions which are deemed to be more suited to determine the longterm influence of the second star on planetary fingerprints in S-type systems. In contrast to peak amplitudes, the rms of a planet's AM signal is only modified slightly by the additional gravitational interaction with the second star. A similar behavior can be seen in planetary transit probabilities. The rms values of RV signals are altogether independent of the secondary's gravitational influence, assuming that the system is nearly coplanar."
272,hal-01078051.training.tei.xml,66,"After defining the permanent, extended, and average habitable zones for both stellar components of the α Centauri system, we investigated the possible interaction between the newly discovered α Cen Bb and additional terrestrial companions in α Cen B's HZ. Our results suggest that α C e nB bi so na n orbit with very low eccentricity which would not be influenced significantly by habitable, terrestrial companions. Conversely, α Cen Bb's presence would also not affect Earth-like planets in the HZ of α Cen B."
273,hal-01078051.training.tei.xml,67,"We estimated the maximum and rms values of the RV as well as AM signal for a terrestrial planet in the α Centauri HZs. The peak and rms amplitudes of the RV signal ranged between4 and 12 cm s −1. Astrometric signals were estimated to lie between 1 and 5 μas. Given the current observational facilities, enormous amounts of observing time would be required to achieve such precisions. If the α Centauri was a transiting system, however, a habitable planet could be detectable using current technologies. It seems that the detection of Earth-like planets in circumstellar HZs of binaries with Sun-like components via astrometry and RV is still somewhat beyond our grasp, leaving photometry to be the only current option in this respect."
274,hal-01078051.training.tei.xml,68,"S.E. and E.P.-L. acknowledge support from FWF through projects AS11608-N16 (EP-L and SE), P20216-N16 (SE and EP-L), and P22603-N16 (EP-L). S.E. acknowledges support from the University of Vienna's Forschungsstipendium 2012. N.H. acknowledges support from the NASA Astrobiology Institute under Cooperative Agreement NNA09DA77A at the Institute for Astronomy, University of Hawaii, and NASA EXOB grant NNX09AN05G. S.E. and E.P-L. also thank the Institute for Astronomy and NASA Astrobiology Institute at the University of Hawaii-Manoa for their kind hospitality during the course of this project. The authors are thankful to Nikolaos Georgakarakos for his valuable suggestions and to the anonymous referee for constructive comments."
275,hal-01078051.training.tei.xml,69,The equation of the center providing a direct relation between the true anomaly f and the mean anomaly M is presented up to the sixth order in eccentricity e: The averaging integrations over M and ω in Equations (17) and (18)From the definition of averaging given by Equation (6)
276,hal-01078051.training.tei.xml,70,A similar procedure has been applied to derive Equation (8).
277,hal-00924047.training.tei.xml,0,"Due to the lack of macromolecular fossils, the enzymatic repertoire of extinct species has remained largely unknown to date. In an attempt to solve this problem, we have characterized a cyclase subunit (HisF) of the imidazole glycerol phosphate synthase (ImGP-S), which was reconstructed from the era of the last universal common ancestor of cellular organisms (LUCA). As observed for contemporary HisF proteins, the crystal structure of LUCA-HisF adopts the ()8-barrel architecture, one of the most ancient folds. Moreover, LUCA-HisF i) resembles extant HisF proteins with regard to internal twofold symmetry , active site residues, and a stabilizing salt bridge cluster, ii) is thermostable and shows a folding mechanism similar to that of contemporary () 8-barrel enzymes, iii) displays high and specific catalytic activity, and iv) forms a stable and functional complex with the glutaminase subunit (HisH) of an extant ImGP-S. Furthermore, we show that LUCA-HisF binds to a reconstructed LUCA-HisH protein with high affinity. Our findings suggest that the evolution of highly specific enzymes and enzyme complexes has already been completed in the LUCA era, which means that sophisticated catalytic concepts such as substrate tunneling and allosteric communication existed already 4 billion years ago."
278,hal-00924047.training.tei.xml,1,"Modern enzyme complexes are elaborate molecular machineries that have been optimized in the course of evolution for the efficient and specific processing of their substrates. One prominent example is the imidazole glycerol phosphate synthase (ImGP-S), a bi-enzyme complex which belongs to the family of glutamine amidotransferases and constitutes a branch-point connecting amino acid and nucleotide biosynthesis. ImGP-S consists of the cyclase subunit HisF and the glutaminase subunit HisH. HisF binds the substrate PRFAR and performs a cycloligase/lyase reaction that generates ImGP and AICAR, which are further used in histidine and de novo purine biosynthesis, respectively  ( Figure 1). The ammonia molecule required for this transformation is produced by the glutaminase subunit HisH and transported to the active site of HisF through an extended molecular channel. This channeling hampers diffusion of ammonia into bulk solvent and thus presumably prevents its protonation to the non-productive ammonium ion. Another special feature of the HisF/HisH complex is the tight coordination of the two enzymatic activities: Binding of PRFAR (or its analogue N′[(5′-phosphoribosyl) formimino]-5-aminoimidazole-4carboxamide-ribonucleotide; ProFAR) to HisF results in an allosteric signal that leads to a several hundred-fold stimulation of the glutaminase activity of HisH  . This property precludes the hydrolysis of glutamine by HisH in the absence of an acceptor substrate at the active site of HisF "
279,hal-00924047.training.tei.xml,2,"We were interested to find out whether the characteristics of modern HisF enzymes were already present in those species that colonized earth in a very early phase of biological evolution. A straightforward answer to this question is difficult due to the lack of macromolecular fossils. However, computational techniques of amino acid sequence reconstruction make it possible to travel back in time and to study extinct proteins . In extreme cases, these algorithms enable us to study enzymes from the last universal common ancestor of cellular organisms (LUCA), which preceded the diversification of life and existed in the Paleoarchean era, i. e. at least 3.8 billion years (Gyr) and presumably 4.5 Gyr ago ."
280,hal-00924047.training.tei.xml,3,"Along these lines, we have previously computationally reconstructed the amino acid sequence of HisF from the LUCA era (LUCA-HisF) . To this end, a set of 87 extant HisF and HisH proteins from the seven phylogenetic clades Crenarchaeota, Actinobacteria, Chlorobi, Cyanobacteria, Firmicutes, Proteobacteria, and Thermotogae has been used to determine a phylogenetic tree t HisF_HisH based on the CAT model  ( Figure 1). After having rooted this tree between the superkingdoms Archaea and Bacteria, we reconstructed a predecessor of HisF from Bacteria and Crenarchaeota as described  . Thus, although the precise lineage of the three-2 Figure 1. Reaction catalyzed by the heterodimeric ImGP synthase complex. The synthase subunit HisF catalyzes the reaction of N`-[(5`phosphoribulosyl)formimino]-5-aminoimidazole-4-carboxamide-ribonucleotide (PRFAR) with ammonia (NH 3 ) to imidazole glycerol phosphate (ImGP) and 5-aminoimidazole-4-carboxamide ribotide (AICAR). ImGP is further utilized in the synthesis of histidine, whereas AICAR is an intermediate in de novo purine biosynthesis, rendering HisF a branch-point enzyme of amino acid and nucleotide biosynthesis. The ammonia molecule required for the HisF reaction is produced by the glutaminase subunit HisH (catalytic triad residues are depicted as spheres) and subsequently channeled to the active site of HisF (catalytic aspartate residues are depicted as spheres). In the absence of HisH, HisF can also use external ammonia that is added as ammonia salts. HisF adopts the () 8-barrel fold, an ubiquitous and catalytically versatile protein architecture  , which is considered one of the three most ancient protein folds  . HisH adopts the  hydrolase fold  . superkingdoms is still under debate  , LUCA-HisF is among the oldest so far reconstructed proteins, if not the oldest hitherto calculated predecessor  . Among the 87 descendants of LUCA-HisF used for reconstruction, 78 of the 250 residues are less than 50 % conserved, whereas 49 residues are strongly conserved. Accordingly, LUCA-HisF differs in 55 amino acids (22 %) from the closest BLAST match , which is HisF from Thermovibrio ammonificans. The nucleotide and amino acid sequences of LUCA-HisF are given in the Supporting Information."
281,hal-00924047.training.tei.xml,4,"We have now produced LUCA-HisF in Escherichia coli, and analyzed its crystal structure, conformational stability, folding mechanism, and catalytic activity. The observed molecular characteristics of LUCA-HisF turned out to be similar to contemporary HisF proteins. Moreover, LUCA-HisF activates an extant HisH protein and thus comprises all elements required for allosteric interaction. Finally, we have also reconstructed and produced a LUCA-HisH protein and could show that it binds to LUCA-HisF with high affinity. Taken together, our results suggest that the protein inventory of the LUCA already contained elaborate enzyme complexes."
282,hal-00924047.training.tei.xml,5,"Cloning, expression, and purification of LUCA-HisF. The gene coding for LUCA-HisF was optimized for its expression in E. coli, synthesized (GeneArt), and cloned into the vector pET24a(+)(Stratagene) using the terminal restriction sites for NdeI and XhoI. Since the addition of a C-terminal hexahistidine tag to LUCA-HisF might influence its interaction with HisH proteins, a stop codon was integrated at the end of the gene. The gene was expressed in E. coli T7-Express cells (New England Biolabs) transformed with pET24a(+)LUCA-hisF. To this end,four liters of Luria Broth (LB) medium supplemented with 75 µg/ml kanamycin were inoculated with a pre-culture and incubated at 37 °C. After an OD 600 of 0.6 was reached, the temperature was lowered to 30 °C. Expression was induced by adding 0.5 mM IPTG, and growth was continued overnight. Cells were harvested by centrifugation (Sorvall/RC5B, GS3, 15 min, 4000 rpm, 4 °C), washed with 50 mM potassium phosphate, pH 7.5, and centrifuged again. The cells were suspended in the same buffer, lysed by sonification (Branson Sonifier W-250D, 2 x 2 min in 15 sec intervals, 45% pulse, 0 °C), and centrifuged again (Sorvall/RC5B, SS34, 30 min, 13.000 rpm, 4 °C) to separate the soluble from the insoluble fraction of the cell extract. In a first step, the soluble supernatant was subjected to ion exchange chromatography using a MonoQ column (HR 16/10, 20 ml, Pharmacia), which had been equilibrated with 50 mM potassium phosphate, pH 7.5. The column was washed with equilibration buffer, and bound LUCA-HisF was eluted by applying a linear gradient of 0-1.5 M NaCl. Proteincontaining fractions were pooled, dialyzed against 50 mM potassium phosphate, pH 7.5, and subjected to ammonia sulfate precipitation. After 80 % saturation with ammonia sulfate, precipitated protein was centrifuged (Sorvall/RC5B, SS34, 30 min, 13.000 rpm, 4 °C), dissolved in 50 mM potassium phosphate, , pH 7.5300 mM potassium chloride, and finally purified via size exclusion chromatography. For this purpose a Superdex200 column (HiLoad 26/60, 320 ml, GE Healthcare) was operated with 50 mM potassium phosphate, pH 7.5, 300 mM potassium chloride at 4 °C. Fractions with pure protein were pooled and dialyzed against 50 mM Tris/HCl, pH 7.5. According to SDS-PAGE (12.5% acrylamide), LUCA-HisF was more than 95% pure. About 30 mg of protein were obtained per liter of culture."
283,hal-00924047.training.tei.xml,6,"In order to determine the binding properties of LUCA-HisF to HisH proteins via fluorescence titration, all tryptophan residues of LUCA-HisF were replaced by tyrosines. Hence, LUCA_hisF_W138Y+W156Y was generated via overlap extension PCR 28 using pET24a(+)-LUCA-hisF as a template (see Supporting Information for oligonucleotide sequences), and subsequently cloned into pET24a(+) via the terminal restriction sites for NdeI and XhoI. Expression and purification were performed as described for LUCA-HisF, yielding a comparable amount and purity of LUCA-HisF_W138Y+W156Y."
284,hal-00924047.training.tei.xml,7,"Cloning, expression, and purification of zmHisH. Genomic DNA of Zymomonas mobilis (DSM424) was ordered from the Leibniz Institute DSMZ. In order to remove the internal restriction site for NdeI, the zmhisH gene was amplified by overlap extension PCR (see Supporting Information for oligonucleotide sequences) and cloned into pET24a(+) using the terminal restriction sites for NdeI and XhoI. After transformation of E. coli strain BL21(DE3) (Stratagene), expression was carried out at 30 °C overnight in four liters of LB medium, supplemented with 75 µg/ml kanamycin. Protein purification was performed as described for LUCA-HisF including-3ion exchange chromatography using 50 mM Tris/HCl, pH 9, as buffer, ammonia sulfate precipitation, size exclusion chromatography, and final dialysis against 50 mM Tris/HCl, pH 7.5. According to SDS-PAGE (12.5% acrylamide), zmHisH was more than 95% pure. About 8 mg of protein were obtained per liter of culture."
285,hal-00924047.training.tei.xml,8,"Sequence reconstruction, cloning, expression and purification of LUCA-HisH. As for LUCA-HisF, the reconstruction of LUCA-HisH was based on the tree t HisF_HisH (Figure S1), which is close to an accepted organism phylogeny. In comparison to the multiple sequence alignment (MSA) of extant HisF sequences, the87 extant HisH sequences exhibit a significantly higher variability. In fact, 140 of 226 residues are less than 50 % conserved. Furthermore, the MSA (HisH ext ) contains several gaps. Recently, it has been shown that a novel algorithm for the phylogeny-aware gap placement named PRANK  improves MSA quality. This is why we used PRANK with the option-showanc to deduce LUCA-HisH from the MSA HisH ext under the control of t HisF_HisH ( Figure S2). The nucleotide and amino acid sequences of LUCA-HisH are given in the Supporting Information. The protein shares 123 of 226 residues (54%) with the closest BLAST match, which is HisH from Syntrophothermus lipocalidus."
286,hal-00924047.training.tei.xml,9,"The gene coding for LUCA-HisH was optimized for its expression in E. coli, synthesized (GeneArt), and cloned into the vector pET24a(+)(Stratagene) using the terminal restriction sites for NdeI and XhoI. (The gene encodes a Cterminal hexa-histidin tag; see Supporting Information). Subsequently, pET24a(+)-LUCA-hisH was used to transform E. coli strain BL21-Gold (DE3) (Stratagene). Protein expression, harvesting of cells, and cell lysis were performed as described for LUCA-HisF. As LUCA-HisH showed a high thermal stability, most of the host proteins could be removed by heat denaturation (70 °C, 15 min) followed by centrifugation (Sorvall/RC5B, SS34, 30 min, 13.000 rpm, 4 °C). For further purification, the supernatant of the heat step was loaded onto a HisTrapFF crude column (5 ml; GE Healthcare), which had been equilibrated with 50 mM potassium phosphate, pH 7.5, 300 mM potassium chloride and 10 mM imidazole. After washing with equilibration buffer, the bound protein was eluted by applying a linear gradient of 10375 mM imidazole. Fractions with pure protein were pooled, and LUCA-HisH was dialyzed against 10 mM potassium phosphate, pH 7.5. As judged by SDS-PAGE, the protein was more than 95 % pure. About 26 mg LUCA-HisH were obtained per liter of culture."
287,hal-00924047.training.tei.xml,10,"Crystallization, data collection, and refinement of LUCAHisF. Crystallization trials were carried out using the PEG/Ion screen (Hampton Research). The hanging drop vapor diffusion method was performed in96-well plates (Greiner) at 291 K. Drops contained 300 nl of the respective reservoir buffer mixed with 300 nl of LUCA-HisF (13.9 mg/ml) in 10 mM potassium phosphate, pH 7.5. In each well equilibration was performed against 100 µl of reservoir buffer. Crystals were obtained with 0.2 M sodium phosphate monobasic monohydrate, pH 4.7 and 20% wt/vol PEG 3350. After flash freezing in liquid nitrogen, data of single crystals were collected at the synchrotron beamlinePX2 (SLS) at 100 K. Data were processed using XDS  and the data quality assessment was done using phenix.xtriage  . Molecular replacement was performed with MOLREP within the CCP4i suite  . A homology model of LUCA-HisF with HisF from Thermotoga maritima (tmHisF) (PDB ID 1THF) was built with MODELLER  and served as a search model. Initial refinement was performed using REFMAC . The model was further improved in several refinement rounds using automated restrained refinement with the program PHENIX  and interactive modeling with Coot  . The data collection and refinement statistics are summarized in Table S1. The final model was analyzed using the program MolProbity ."
288,hal-00924047.training.tei.xml,11,"Analysis of the thermal stability of LUCA-HisF and LUCAHisH. Differential T DSC). Thermal denaturation traces of LUCA-HisF and LUCA-HisH in50 mM potassium phosphate, pH 7.5, were monitored with a JASCO J-815 circular dichroism (CD) spectrometer in a 0.1 cm cuvette by following the loss of ellipticity at 220 nm. Unfolding was induced by raising the temperature in 1 °C increments at a ramp rate of 1 °C min"
289,hal-00924047.training.tei.xml,12,"The thermodynamic stability of LUCA-HisF was determined by GdmClinduced equilibrium unfolding transitions. The loss of tertiary structure was probed by protein fluorescence, the loss of secondary structure by far-UV CD. Samples with2 µM protein were prepared in 50 mM Tris/HCl buffer (pH 7.5) containing different concentrations of GdmCl. GdmCl (ultrapure) was purchased from MP Biomedicals (Illkrich, France) and its concentration was determined by the refractive index of the solution  . To reach equilibrium, LUCAHisF was preincubated at the indicated concentration of GdmCl for 24 h at 25 °C."
290,hal-00924047.training.tei.xml,13,"The fluorescence emission signal at320 nm (bandwidth 5 nm) after excitation at 280 nm (bandwidth 3 nm) was monitored with a JASCO model FP-6500 spectrofluorimeter. The equilibrium unfolding transition of LUCA-HisF obtained by monitoring fluorescence is shown in Figure S4A in comparison to tmHisF and its artificially designed precursors Sym1 and Sym2, which were constructed by duplication and fusion of the C-terminal half-barrel HisF-C followed by the optimization of the initial construct . The transitions were analyzed according to the two-state equilibrium model, assuming a linear dependency of the free-energy of unfolding on the GdmCl concentration Table S2."
291,hal-00924047.training.tei.xml,14,"The far-UV circular dichroism (CD) signal at225 nm was monitored using a JASCO model J815 CD spectrophotometer (path-length 5 mm; bandwidth 1 nm). The equilibrium unfolding/refolding transitions of LUCA-HisF obtained by Kinetics of refolding of LUCA-HisF in Figure S4B were obtained by following the far-UV CD signal for 200 seconds in manual mixing experiments at various concentrations of GdmCl and extrapolating the exponential curve to zero time. The observed amplitude was plotted as function of the GdmCl concentration and is shown in Figure S4B as closed triangles. It was significantly lower than the amplitude observed in the refolding equilibrium transitions, indicating that the major part of the CD change occurred within the dead time of the manual mixing experiment. This is interpreted with the formation of a compact burst-phase refolding intermediate with a high content of secondary structure."
292,hal-00924047.training.tei.xml,15,"Fluorescence titration of zmHisH and LUCA-HisH with LUCA-HisF. Fluorescence titration was used to determine the binding stoichiometry and affinity of the LUCAHisF/zmHisH and LUCA-HisF/LUCA-HisH complexes, as in the course of complex formation a tryptophan residue lying at the HisH-interface is shielded from the solvent . Hence, when titrating either 7 µM zmHisH or 5 µM LUCA-HisH in 50 mM potassium phosphate, pH 7.5 with LUCAHisF_W138Y+W156Y, the emission maxima shifted from 345 nm to 325 nm and from 345 nm to 329 nm, respectively (excitation at 295 nm). The decreases in fluorescence emission at 318 nm were plotted against the added amounts of LUCA-HisF_W138Y+W156Y and the resulting curves were analyzed with a quadratic fit. Both titrations were performed in triplicate."
293,hal-00924047.training.tei.xml,16,"Analysis of enzymatic activity in vitro, and in vivo. In vitro enzymatic activities were determined by steady-state kinetics. The ammonia-and glutamine-dependent conversions of PRFAR into ImGP and AICAR (HisF reaction) were measured spectrophotometrically at300 nm as previously described ."
294,hal-00924047.training.tei.xml,17,"At25 °C, entire progress curves at four different PRFARconcentrations were recorded either in 50 mM Tris-acetate, pH 8.5 in presence of 100 mM ammonium acetate (ammoniadependent cyclase reaction) or in 50 mM Tris-acetate, pH 8.0 in presence of 15 mM glutamine and 2 µM zmHisH (glutamine-dependent cyclase reaction). In both cases an excess of HisA from T. maritima was added in order to synthesize PRFAR in situ from ProFAR"
295,hal-00924047.training.tei.xml,18,", and0.5 µM of LUCA-HisF were used to initiate the measurements. Data were analyzed with the integrated form of the Michaelis-Menten equation using the program COSY 45 to obtain k cat and K M PRFAR. In case of the LUCA-HisF/LUCA-HisH complex (10 µM), no glutamine-dependent cyclase activity could be determined in presence of 10 mM glutamine and 100 µM ProFAR. The glutaminase activity of zmHisH (1 µM) in complex with liganded LUCA-HisF (2 µM; 40 µM ProFAR) was measured in a coupled enzymatic assay as previously described  . At 25 °C, produced glutamate was oxidized by a molar excess of glutamate dehydrogenase (Roche) in 50 mM Tricine hydroxide, pH 8.0. Thus, the reduction of the coenzyme NAD + to NADH could be monitored spectrophotometrically at 340 nm. Three glutamine saturation curves were recorded and fitted with the Michaelis-Menten equation to obtain k cat and K M Gln. In an identical setup, no glutaminase activity (12 mM glutamine) could be detected for LUCA-HisH (20 µM) in complex with ligand-bound LUCA-HisF (20 µM; 200 µM ProFAR). The extent to which LUCA-HisF liganded with ProFAR activates zmHisH had to be determined in a discontinuous assay, since NAD + also exhibits a stimulating effect on glutaminase activity  . To this end, 10 mM glutamine were incubated at 25 °C with 0.5 µM zmHisH and 5 µM LUCA-HisF either in the absence of ProFAR or in the presence of 40 µM ProFAR. 150 µl aliquots of the reaction mixture were collected after , , 153045 and 60 (only in the absence of ProFAR) minutes and spun through a 10 kDa filter (Roth) to remove the enzymes. The stimulation factor is the quotient k cat (ProFAR saturated)/ k cat (without ProFAR) v max was calculated from the linear increase of glutamate production with time, which was determined with the help of 1 mg/ml glutamate dehydrogenase and 0.7 mM APAD + (Sigma) (the reaction mixture was diluted 1:7.5, and absorption was measured at 363 nm). All measurements were performed in triplicates. ProFAR to PRFAR isomerisation activity (HisA reaction) was measured with the enzymatic assay described for the ammonia-dependent HisF reaction, however in presence of an excess of HisF 46 . PRA to CdRP isomerisation activity (TrpF reaction) was followed at 25 °C by a fluorimetric assay (excitation at 350 nm, emission at 400 nm)  . The substrate PRA was generated in situ by 1 µM yeast anthranilate phosphoribosyl transferase from anthranilate and PRPP, which was provided in a 30-fold molar excess. Moreover, 2.5 µM indole-3-glycerol phosphate synthase from T. maritima were added to prevent product inhibition. To test for enzymatic activity in vivo, the gene coding for LUCA-HisF was subcloned into the pTNA vector, which allows for constitutive expression in E. coli  . The resulting pTNA-LUCA-hisF plasmid was used to transform cells of auxotrophic hisF, hisA or trpF E. coli strains  . These strains lack the hisF, hisA, or trpF gene on their chromosome and are, therefore, unable to grow on medium without histidine or tryptophan, respectively. Growth experiments and controls were performed as described ."
296,hal-00924047.training.tei.xml,19,"Structure determination of LUCA-HisF. The gene coding for LUCA-HisF was synthesized, cloned into a plasmid, and expressed in E. coli. The LUCA-HisF protein was predominantly found in the soluble fraction of the host cell extract, and purified in athree step process using ion exchange chromatography, ammonia sulfate precipitation, and size exclusion chromatography. Purified LUCA-HisF was crystallized and its three-dimensional structure was determined at 1.48 Å resolution by molecular replacement based on the structure of tmHisF  ( Figure 2, Table S1). LUCA-HisF (PDB ID 4EVZ) adopts the conserved () 8-barrel structure observed in the three extant HisF proteins from Pyrobaculum aerophilum (PDB ID 1H5Y), Thermus thermophilus (PDB ID 1KA9), and T. maritima (PDB ID 1THF), for which crystal structures have been previously determined. The superposition of LUCA-HisF with each of these structures by means of STAMP  resulted in an overall root-mean-square deviation (RMSD) ranging from 1.14 Å to 1.43 Å. In agreement with the postulated evolution of the () 8-barrel fold from a () 4half-barrel  , LUCA-HisF displays a clear two-fold symmetry: The superposition of its N-terminal and Cterminal halves yielded an RMSD of 1.68 Å, which is similar to the corresponding values for the three extant HisF proteins (1.27 Å for 1H5Y, 1.52 Å for 1KA9, and 1.69 Å for 1THF). Consistent with the internal symmetry, the two catalytically important aspartate residues  are found on opposite sides of the active site at the C-terminal ends of β-strand 1 and β-strand 5. Likewise, the two co-crystallized phosphate groups, which represent the two phosphate groups of the substrate PRFAR (Figure 1), are anchored by the C-terminal ends of β-strands 3 and 4, and β-strands 7 and 8, respectively (Figure 2A). Moreover, a stabilizing salt-bridge cluster at the N-terminal end of the β-barrel, which contains four charged and invariant residues, which form the gate to the cyclase ammonia channel , is also present in LUCA-HisF ( Figure 2B)."
297,hal-00924047.training.tei.xml,20,"Stability and folding mechanism of LUCA-HisF. The thermal stability of LUCA-HisF was determined by differential scanning calorimetry (DSC), which monitors overall unfolding, and the heat-induced decrease of the far-UV CD signal, which indicates the loss of secondary structure. The combination of both methods showed that thermal unfolding of LUCA-HisF is atwo-step process with apparent transition Figure 3. GdmCl-induced equilibrium unfolding/refolding transitions of LUCA-HisF. The transitions were followed by Trp/Tyr fluorescence (excitation at 280 nm; emission at 320 nm) in 50 mM Tris/HCl buffer, pH 7.5. Closed symbols represent the unfolding experiment, started with folded protein, and open symbols represent the refolding experiment, started with protein that was previously unfolded in 6.0 M GdmCl. The continuous line represents a fit to the unfolding transition on the basis of the two-state model. The dashed lines indicate the baselines for the pure N and U states. The thermodynamic parameters deduced from the analysis are given in the text and listed in Table S2. midpoints of about 70 °C and 100 °C ( Figure S3A+B). These results characterize LUCA-HisF as an enzyme with a high resistance to heat. Interestingly, even higher denaturation temperatures were previously observed for enzymes from the common ancestors of Bacteria, Archaea and Archaea/Eukaryota . These findings and our results are in agreement with rRNA and protein sequence analyses which have provided independent support for the increase of thermotolerance from the LUCA to the ancestors of Bacteria and Archaea-Eukaryota"
298,hal-00924047.training.tei.xml,21,". Furthermore, the conformational stability of LUCA-HisF was analyzed by GdmCl-induced equilibrium unfolding and refolding transitions. The loss or gain of tertiary structure was probed by protein (Tyr/Trp) fluorescence. The equilibrium unfolding and refolding curves superpose well, which proves the reversibility of unfolding (Figure 3). Moreover, the transitions are adequately described by thetwo-state model  , indicating that no significant amounts of stable equilibrium intermediates are populated. "
299,hal-00924047.training.tei.xml,22,", a transition midpoint ( [D]1/2) at1.2 M GdmCl, and a free energy of unfolding in the absence of denaturant (G D ) of 18 kJ mol-1. LUCA-HisF has a lower G D but a comparably high mvalue as tmHisF and its artificially designed precursors Sym1 and Sym2  , indicating that it is comparably compact as these proteins but less stable ( Figure S4A, Table S2). Folding and unfolding kinetics followed by Tyr/Trp fluorescence, showed that the reduced stability of LUCA-HisF is due to strongly increased unfolding rates of LUCA-HisF in comparison to tmHisF ( Figure S5A). The comparison of the refolding kinetics of LUCA-HisF, tmHisF, Sym1, and Sym2 followed by fluorescence and far-UV CD (Supporting Information) showed that all four proteins share a common sequential folding mechanism including a non-productive burst-phase intermediate ( Figure S4B) and two productive intermediates ( Figure S5). The rate-limiting step that synchronizes folding is conserved ( Figure S6). (Table 1). As ancient enzymes have been proposed to be less specific (more promiscuous) than their modern descendants 57 , we tested LUCA-HisF for its ability to catalyze related metabolic reactions. The homologous enzyme HisA, which precedes HisF in the histidine biosynthesis pathway, catalyzes the Amadori rearrangement of ProFAR to PRFAR. HisA shares with HisF the overall () 8-barrel fold as well as the location of the two symmetry-related catalytic aspartate residues and phosphate binding sites  . Phosphoribosyl anthranilate (PRA) isomerase (TrpF) catalyzes an Amadori rearrangement in tryptophan biosynthesis analogous to HisA in histidine biosynthesis . Remarkably, a single amino acid exchange in the HisA and HisF proteins from T. maritima leads to TrpF activity, suggesting that these three phosphate-binding () 8-barrel proteins have evolved from a common precursor  . We examined LUCA-HisF for the isomerisation activity towards ProFAR and PRA. However, no substrate turnover could be detected, even in the presence of 50 µM protein. These findings were complemented by assessing catalytic activity in vivo using metabolic selection. For this purpose, a plasmid harboring the LUCA-HisF gene was used to transform auxotrophic E. coli strains lacking either the intrinsic hisF, hisA, or trpF gene. When plated on minimal medium without histidine or tryptophan, the hisF cells formed visible colonies within 24 hours, whereas the hisA and trpF cells did not grow within one week. Taken together, these results demonstrate that LUCA-HisF is a monofunctional enzyme."
300,hal-00924047.training.tei.xml,23,"whether LUCA-HisF contains all structural elements required for complex formation, substrate channeling and allosteric communication, we assayed its functional interaction with the extant zmHisH enzyme from Zymomonas mobilis. For this purpose, zmHisH was produced in E. coli and purified. The binding of zmHisH to LUCA-HisF was analyzed via fluorescence titration 5 , which showed that thetwo proteins form a stoichiometric complex with a thermodynamic dissociation constant (K D ) of 113 nM ( Figure 4A)."
301,hal-00924047.training.tei.xml,24,"The steady-state kinetic constants k cat and K M PRFAR of LUCAHisF in presence of zmHisH and saturating concentrations of glutamine (glutamine-dependent cyclase activity) compare well with the above reported ammonia-dependent cyclase activity (Table 1). This outcome confirms the functionality of the LUCA-HisF/zmHisH complex, as ammonia produced at the active site of HisH by means of glutamine hydrolysis is used as efficiently by LUCA-HisF as externally added ammonia. Moreover, this finding suggests that ammonia is transported from HisH to the active site of the synthase through a molecular channel formed by the central -barrel of HisF Figure 4. Fluorescence titration curve of zmHisH and LUCAHisH with LUCA-HisF and activation of zmHisH by LUCAHisF. (A) LUCA-HisF_W138Y+W156Y was added to7 μM zmHisH in 50 mM potassium phosphate, pH 7.5, 25 °C. Fluorescence emission at 318 nm was determined following excitation at 295 nm. Quadratic fits of the obtained data points resulted in a K D value of 113 (± 1) nM. The stoichiometry is slightly deviating from a 1:1 complex, indicating that a small fraction of zmHisH is not active. (B) Glutaminase activity of the LUCA-HisF/zmHisH complex in the absence (circles) and presence (triangles) of ProFAR was tested in a discontinuous assay (see Supplementary Materials for detailed information). Mean values and standard deviations of triplicate measurements are shown. 10 mM glutamine were incubated with 0.5 µM zmHisH and 5 µM LUCA-HisF at 25 °C in both cases. Glutaminase activity is enhanced 13-fold in the presence of ProFAR (see Table 1). (C) Titration of 5 µM LUCAHisH with LUCA-HisF_W138Y+W156Y was performed and analyzed analogous to (A), yielding a stoichiometric complex with a K D value of 4 (± 2) nM."
302,hal-00924047.training.tei.xml,25,"LUCA, as observed for extant HisF enzymes . Furthermore, glutamine hydrolysis by zmHisH in presence of LUCAHisF and saturating concentrations of the substrate analogue ProFAR (glutaminase activity) is as efficient as glutaminase activity of HisH from T. maritima (tmHisH) in complex with ProFAR-liganded tmHisF  (Table 1). The comparison of the zmHisH activity in the presence and absence of ProFAR indicates a 13-fold stimulation by the HisF-ligand in this nonnative complex ( Figure 4B), which is 23-fold lower than the stimulating effect of ProFAR in the native tmHisF/tmHisH complex (Table 1)."
303,hal-00924047.training.tei.xml,26,"Following the characterization of LUCA-HisF, we also reconstructed the amino acid sequence of the corresponding glutaminase LUCA-HisH. Again, we used the tree t HisF_HisH , but opted for the phylogeny-aware gap placement of PRANK to deduce LUCA-HisH from the MSA HisH ext , which contains several insertions and deletions. The gene coding for LUCAHisH was synthesized, cloned into a plasmid, and expressed in E. coli. The produced protein was soluble and could be purified by a combination of heat denaturation and Ni 2+ affinity chromatography. As observed for LUCA-HisF, LUCAHisH exhibits a high thermotolerance. Unfolding followed by CD resulted in a single transition with a midpoint of about 79 °C ( Figure S3C). Complex formation between LUCA-HisH and LUCA-HisF was probed by fluorescence titration  . Both proteins interacted stoichiometrically with very high affinity as demonstrated by a K D value of 4 nM ( Figure 4C). However, when testing the LUCA-HisF/LUCA-HisH complex for glutamine-dependent cyclase activity or LUCA-HisH for the hydrolysis of glutamine in the presence of LUCA-HisF and saturating concentrations of ProFAR, no enzymatic turnover could be determined. Thus, other than LUCA-HisF, LUCAHisH is catalytically inactive. As outlined in the following, uncertainties in the reconstruction process are probably responsible for this finding."
304,hal-00924047.training.tei.xml,27,"The evolutionary models underlying reconstruction consider each residue-position independently of all other positions. Thus, the reliability of a given reconstruction is not limited by sequence length, but by the composition of the MSA and the topology of the deduced phylogenetic tree. In the case of LUCA-HisF,49 out of 250 residues are strictly conserved, among them are the two active site aspartate residues  and amino acids contributing to the central ammonia channel  ( Figure 2). Furthermore, the four central nodes of t HisF_HisH ( Figure S1) possess posterior probabilities ≥ 0.88. Taken together, these features suggest that tree topology and choice of the most likely residues for the corresponding predecessors and LUCA-HisF is largely unambiguous. In contrast, in the case of LUCA-HisH, only 21 out of 226 residues are strictly conserved, which makes the reconstruction much more prone to uncertainties."
305,hal-00924047.training.tei.xml,28,"Even more than a certain conservation of amino acid sequence composition, conservation of sequence length is an important prerequisite for a valid reconstruction. Along these lines, the sequence lengths of extant and reconstructed thioredoxins, which is the only other example for a fully functional enzyme from the LUCA era , are very similar. In contrast, MSA HisH ext contains several gaps. Nonetheless, the phylogeny-aware gap placement by means of PRANK did allow us to reconstruct a stable LUCA-HisH protein with a fully functional protein-protein interface, albeit lacking enzyme activity. Obviously, t HisF_HisH was sufficiently informative to reconstruct ancestral residues at positions whose role did not change during evolution such as the catalytic triad  and residues involved in binding of the substrate glutamine,-8as deduced from the structure of tmHisH . In contrast, reconstruction seems to have failed at residue-positions that underwent frequent changes during evolution due to insertions and deletions. It has to be shown that highly articulated phylogenetic trees will enable us to reconstruct the correct series of indels and to further improve reconstruction for such difficult cases."
306,hal-00924047.training.tei.xml,29,"Taken together, LUCA-HisF, which presumably existed about4 billion years ago, is similar to extant HisF proteins with respect to structure, stability, folding, and activity. Since similar results were obtained for predecessors of thioredoxin,  .experimental evidence accumulates for the existence of highly effective and non-promiscuous enzymes in the LUCA era. In addition, LUCA-HisF forms a stable complex with LUCA-HisH and a functional enzyme complex with the extant glutaminase zmHisH. It is therefore plausible to assume that the evolution of the ImGP-S complex, including ammonia channeling and allosteric communication, has been completed in the LUCA era. Thus, our experimental findings are in line with the hypothesis that the LUCA had already a rather diverse metabolism, which was as sophisticated as are the metabolisms of its archaeal and bacterial successors ."
307,hal-00924047.training.tei.xml,30,"Nucleotide and amino acid sequences of LUCA-HisF and LUCA-HisH. Oligonucleotides used for the construction of LUCA_hisF_W138Y+W156Y and for the genomic amplification of zmHisH. Comparison of the folding mechanism of LUCA-HisF, T. maritima HisF, and its artificial precursors Sym1 and Sym2. Crystal structure determination of LUCAHisF (PDB ID 4EVZ): Data collection and refinement statistics (Table S1). Thermodynamic unfolding parameters of LUCA-HisF, T. maritima HisF, Sym1, and Sym2 (Table S2). Phylogenetic tree t HisF_HisH used for the reconstruction of LUCA-HisF and LUCA-HisH ( Figure S1). PRANK output for the reconstruction of LUCA-HisH ( Figure S2). Thermal denaturation of LUCA-HisF and LUCA-HisH ( Figure S3). Equilibrium unfolding transitions of LUCA-HisF, T. maritima HisF, Sym1, and Sym2, and formation of a burst-phase intermediate by LUCA-HisF ( Figure S4). Apparent rate constants () and amplitudes of refolding and unfolding kinetics of LUCAHisF ( Figure S5). Unifying folding mechanism for LUCAHisF, T. maritima HisF, and Sym1 and Sym2 ( Figure S6). References for Supporting Information.This material is available free of charge via the Internet at http://pubs.acs.org."
308,hal-00924047.training.tei.xml,31,Rainer.Merkl@ur.de and Reinhard.Sterner@ur.de
309,hal-00924047.training.tei.xml,32,The firsttwo authors contributed equally. All authors have given approval to the final version of the manuscript.
310,hal-00924047.training.tei.xml,33,The authors declare no competing financial interests.
311,hal-00924047.training.tei.xml,34,"B.R. and J.S. were supported by fellowships from the Cusanuswerk and the Fonds der Chemischen Industrie, respectively."
312,1404.4640.training.tei.xml,0,"Comet C/2013 A1 (siding Spring) will experience a high velocity encounter with Mars onOctober 19, 2014 at a distance of 135,000km ± 5000km from the planet center. We present a comprehensive analysis of the trajectory of both the comet nucleus and the dust tail. The nucleus of C/2013 A1 cannot impact on Mars even in the case of unexpectedly large nongravitational perturbations. Furthermore, we compute the required ejection velocities for the dust grains of the tail to reach Mars as a function of particle radius and density and heliocentric distance of the ejection. A comparison between our results and the most current modeling of the ejection velocities suggests that impacts are possible only for millimeter to centimeter size particles released more than 13 au from the Sun. However, this level of cometary activity that far from the Sun is considered extremely unlikely. The arrival time of these particles spans a 20-minute time interval centered at October 19, 2014 at 20:09 TDB, i.e., around the time that Mars crosses the orbital plane of C/2013 A1. Ejection velocities larger than currently estimated by a factor > 2 would allow impacts for smaller particles ejected as close as 3 au from the Sun. These particles would reach Mars from 43 to 130 min after the nominal close approach epoch of the purely gravitational trajectory of the nucleus."
313,1404.4640.training.tei.xml,1,"Comet C/2013 A1 (Siding Spring) was discovered onJanuary 2013 at the Siding Spring observatory (McNaught et al. 2013). Shortly after discovery it was clear that C/2013 A1 was headed for a close encounter with Mars on October 19, 2014. C/2013 A1 is on a near parabolic retrograde orbit and will have a high relative velocity with respect to Mars of about 56km/s during the close approach. If the comet has no significant nongravitational perturbations, the trajectory of the nucleus consistent with the present set of astrometric observations rules out an impact on Mars. However, comet orbits are generally difficult to predict. As the comet gets closer to the Sun cometary activity can result in significant nongravitational perturbations (Marsden et al. 1973) that in turn can lead to significant deviations from the purely gravitational ( "" ballistic "" ) trajectory. In the case of C/2013 A1, cometary activity was already visible in the discovery observations, when the comet was at more than 7 au from the Sun."
314,1404.4640.training.tei.xml,2,"Beside the effect of nongravitational perturbations, dust grains in the tail of the comet could reach Mars and possibly damage spacecrafts orbiting Mars, i.e., NASA's Mars Reconnaissance Orbiter , NASA's Mars Odyssey, ESA's Mars Express, NASA's MAVEN, and ISRO's MOM. Vaubaillon et al. (2014) and Moorhead et al. (2014) show that dust grains can reach Mars if they are ejected from the nucleus with a sufficiently high velocity. The modeling of the ejection velocity is in continuous evolution. As the comet gets closer to the inner solar system we have additional observation that provide constraints to the ejection velocities of dust grains. In particular, by making use of observations from HST/WFC3, Swift/UVOT, and WISE, Farnham et al. (2014) and Tricarico et al. (2014) find ejection velocities lower than those derived by Vaubaillon et al. (2014) and Moorhead et al. (2014), thus significantly reducing the hazard due to dust grains in the comet tail."
315,1404.4640.training.tei.xml,3,"In this paper we study the trajectory of C/2013 A1's nucleus, including the contribution of nongravitational perturbations. We also present an analysis of the required ejection velocities for the dust grains to reach Mars. This analysis can be used as a reference as the understanding and the modeling of the dust grain ejection velocities evolve."
316,1404.4640.training.tei.xml,4,"We examined all available ground-based optical astrometry (Right Ascension and Declination angular pairs) as ofMarch 15, 2014. To remove biased contributions from individual observatories we conservatively excluded from the orbital fit batches of more than four observations in the same night with mean residual larger than 0.5′′, and batches of three or four observations showing mean residual larger than 1 ′′. We also adopted the outlier rejection scheme of Carpino et al. (2003) with χ rej = 2. To the remaining 597 optical observations we applied the standard one arcsecond data-weights used for comet astrometry. Figure 1 shows the residuals of C/2013 A1's observations against our new orbit solution (JPL solution 46). [Figure 1 about here.] Our force model included solar and planetary perturbations based on JPL's planetary ephemerides DE431 1 , the gravitational attraction due to the 16 most massive bodies in the main asteroid belt, and the Sun relativistic term. No significant nongravitational forces were evident in the astrometric data and so the corresponding JPL orbit solution is ballistic, identified as number 46. Table 1 contains the orbital elements of the computed solution."
317,1404.4640.training.tei.xml,5,"Table 2 provides information on the close encounter between C/2013 A1 and Mars. C/2013 A1 passes through the orbital plane of Mars69minutes before the close approach epoch, while Mars passes through the orbital plane of C/2013 A1 99minutes after the close approach. The Minimum Orbit Intersection Distance (MOID) is the minimum distance between the orbit of the comet and the orbit of Mars (MOID, Gronchi et al. 2007). The MOID points on the two orbits are not on the line of nodes. Mars arrives at the minimum distance point 101min after the close approach epoch, while C/2013 A1 arrives at the minimum distance point 70min before the close approach, which means that the comet is 171min early for the minimum distance encounter. [Table 2 about here.] A standard tool to analyze planetary encounters is the b-plane (Kizner 1961; Valsecchi et al. 2003 ), defined as the plane passing through the center of mass of the planet and normal to the inbound hyperbolic approach asymptote. The coordinates on the b-plane described in Valsecchi et al. (2003) are oriented such that the projected heliocentric velocity of the planet is along −ζ. Therefore , ζ varies with the time of arrival, i.e., a positive ζ means that the comet arrives late at the encounter while a negative ζ means that the comet arrives early. On the other hand ξ is related to the MOID. The b-plane is used on a daily basis for asteroid close approaches to the Earth and computing the corresponding impact probabilities (Milani et al. 2005).  The projection of the velocity of Mars on this plane is oriented as −ζ, while the Mars-to-Sun vector projection is on the left side, at a counterclockwise angle of 186 ° with respect to the ξ axis. The negative ζ coordinate of the center of the ellipse corresponds to the 171min time shift between Mars and C/2013 A1. [Figure 2 about here.]"
318,1404.4640.training.tei.xml,6,Comet trajectories can be significantly affected by nongravitational perturbations due to cometary outgassing. We use the Marsden et al. (1973) comet nongravitational model:
319,1404.4640.training.tei.xml,7,"where g(r) is a known function of the heliocentric distance r, and A i are free parameters that give the nongravitational acceleration at1 au in the radial-transverse-normal reference frame defined byˆrbyˆbyˆr, ˆ t, ˆ n."
320,1404.4640.training.tei.xml,8,"The observational dataset available for C/2013 A1 does not allow us to estimate the nongravitational parameters A i . Still, nongravitational accelerations could cause statistically significant deviations at the close approach epoch. To deal with this problem, we analyzed the properties of known nongravitational parameters in the comet catalog. Figure 3 shows the known A 1 and A 2 in the catalog. A 3 values have an order of magnitude similar to that of A 2 . Figure 4 contains scatter plots of nongravitational parameters showing the correlation between these parameters. For comets with an orbit similar to that of C/2013 A1, i.e., with large orbital period (>60 yr) and high eccentricity (> 0.9), values of A 1 are on average ∼ 10 −8 au/d 2, but they can be as large as ∼ 10 −6 . A 2 and A 3 are generally one order of magnitude smaller, i.e., on average they are ∼ 10 −9 au/d 2 but can be as large as ∼ 10 −7 au/d 2. We can see that A 1 is generally one order of magnitude larger than A 2 and A 3 , which makes sense since the radial component is usually the largest for nongravitational accelerations. [ According to the properties of the comet population we considered three different scenarios as described in Table 3: the ballistic scenario corresponds to JPL solution 46; the "" reference "" scenario uses typical values of the nongravitational parameters; the "" wide "" scenario assumes extreme values of the nongravitational parameters.  For A 2 and A 3 the nominal value is 0 au/d 2 since these components can be either positive or negative, while A 1 can only be positive."
321,1404.4640.training.tei.xml,9,"Figure 5 shows the position difference among thethree scenarios compared to the position uncertainty of the ballistic solution. The available observations put a strong constraint the trajectory of C/2013 A1 for heliocentric distances between 3 au and 8 au from the Sun. Outside of this distance range we have no observations and therefore the uncertainty increases. Because of the fast decay of the g(r) function in Eq. (1) the contribution of nongravitational accelerations for large heliocentric distances is well within the uncertainty and so the trajectory of C/2013 A1 in the past is not significantly affected. It is worth pointing out that the function g(r) represents water sublimation while distant activity is not driven by water and therefore may be inaccurate a large distances. However, for such large distances the position uncertainty is large enough to make this possible discrepancy irrelevant. Finally, for smaller heliocentric distances nongravitational perturbations become relevant and can affect the predictions for the Mars encounter, especially in the wide scenario."
322,1404.4640.training.tei.xml,10,"[Figure 5 about here.] For thethree different scenarios, Table 4 gives the close approach information while Fig. 6 shows the projection of the orbital uncertainties on the b-plane. The ballistic and reference solutions provide very similar predictions, from which we conclude that nongravitational perturbations will not significantly affect the orbit unless they are larger than expected. The wide solution, which has to be regarded as an extreme case, produces a significantly different nominal prediction and quite a large uncertainty. In all three scenarios, the nominal close approach distance is more than 130,000km from Mars and therefore there is no chance of an impact between the nucleus of C/2013 A1 and Mars."
323,1404.4640.training.tei.xml,11,"The predictions and the uncertainty provided so far are based on the optical astrometry available as ofMarch 15, 2014. At the time of submission of this paper (April 2014), comet C/2013 A1 was difficult to observe because of the low solar elongation. On June 18, 2014 the solar elongation becomes larger than 60 ° and we therefore expect observations to resume, which will help in further constraining the trajectory of C/2013 A1. To quantify the effect of future optical astrometry, we simulated geocentric optical observations, with two observations every five nights."
324,1404.4640.training.tei.xml,12,Figure 8 shows the 3σ uncertainty evolution for the close approach epoch. The ballistic and reference scenarios have a current uncertainty of3min and this uncertainty decreases to less than 0.2min right before the close approach. 
325,1404.4640.training.tei.xml,13,"As already discussed in Sec. 3, the wide solution produces predictions significantly different from the ballistic and reference solutions. Thus, at some point observations will reveal whether or not the nongravitational perturbations are behaving as in the wide scenario. Figure 9 shows the uncertainty in A 1 when estimated from the orbital fit as a function of time. When this uncertainty becomes smaller than a given value of A 1 , the observation dataset reveals such A 1 value if it is real. By comparing the uncertainty evolution to the nominal values of A 1 assumed for the different scenarios, we can see that large nongravitational accelerations to the level assumed in the wide scenario are detectable about90days before the close encounter. On the other hand, the reference solution becomes distinct from the ballistic solution only a couple of weeks before the encounter. [Figure 9 about here.] Some skilled observers are capable of gathering comet observations even for solar elongations smaller than 60 °. Therefore, we also simulated observations using 40 ° as a lower threshold for the solar elongation, which makes it possible to collect new observations for C/2013 A1 starting on May 7, 2014. However, the improvement in the uncertainties discussed above is a factor of 1.3 or less and is therefore not relevant."
326,1404.4640.training.tei.xml,14,"Though an impact the nucleus of C/2013 A1 on Mars is ruled out, there is a chance that dust particles in the tail could reach Mars and some of the orbiting spacecrafts. Due to their small size, the motion of dust particles is strongly affected by solar radiation pressure. It is therefore to use the β parameter (Burns et al. 1979), i.e., the non-dimensional number corresponding to the ratio between solar radiation pressure and solar gravity. In terms of physical properties, β is proportional to the area-to-mass ratio and inversely proportional to both the density and to the radius of the particle:"
327,1404.4640.training.tei.xml,15,"where a is the particle radius in µm, ρ is the density in g/cc, and Q is the solar radiation pressure efficiency coefficient."
328,1404.4640.training.tei.xml,16,"For each ejected particle, the location on the b-plane for the Mars encounter is determined by the β parameter, the heliocentric distance r at which the particle is ejected (or the ejection epoch), and the ejection velocity ∆v. Figure 10 shows the typical behavior using as an example β =0.01 and ∆v = |∆v| = 10m/s. For each given β we have a curve on the b-plane corresponding to zero ejection velocity. This curve can be parameterized by the heliocentric distance at which the ejection takes place. Finally, the ejection velocity ∆v yields dispersion around the curve: the larger the ∆v the wider the dispersion."
329,1404.4640.training.tei.xml,17,"The ejection velocity depends on the particle size and density, as well as the heliocentric distance at which the particle is ejected (Whipple 1951). Since cometary activity is very hard to predict, modeling the ejection velocities is a complicated task and is subject to continuous updates as additional observations are available. Therefore, we decided to adopt a different approach: for given ejection distance r and β parameter we computed the minimum ∆v required to reach Mars. In mathematical terms we look for the tridimensional ∆v that is a minimum point of ∆v 2 = |∆v| 2 under the constraint that the particle reaches Mars, i.e., (ξ, ζ)(r, β, ∆v) = (0, 0). This problem is a typical example of finding the minima of a function subject to equality constraints. Thus, we can solve this problem by means of the Lagrange multipliers, i.e., the ∆v we are looking for must satisfy the following system of equations:"
330,1404.4640.training.tei.xml,18,"where λ 1 and λ 2 are free parameters. To solve this system, we first tested the linearity of (ξ, ζ) in ∆v and then linearized system (3) around ∆v =0, thus obtaining the following linear system:"
331,1404.4640.training.tei.xml,19,"To compute the required ∆v, we followed these steps:"
332,1404.4640.training.tei.xml,20,• We sampled β in log-scale from10−6 to 1 and r from 1.
333,1404.4640.training.tei.xml,21,"• For each couple (r, β) we computed the b-plane coordinates (ξ, ζ) obtained without ejection velocity as well as a finite difference approximation of the (ξ, ζ) partials with respect to ∆v;"
334,1404.4640.training.tei.xml,22,• We solved system (4). We scaled the resulting ∆v to account for the size of Mars and the 3σ uncertainty of the particle projection on the b-plane. For this analysis we used the ballistic solution as reference trajectory.
335,1404.4640.training.tei.xml,23,"Figure 11 shows the required ∆v needed to reach Mars as a function of the heliocentric distance at which the ejection takes place for different values of β. On the right side of the plot the required velocities are almost the same. This behavior makes sense as the closer we get to Mars the less time is available for solar radiation pressure to affect the trajectory. Therefore, the required ejection velocity is almost independent of the particle size and density. For β =1.43 × 10 −4 we can see that the required velocity goes to zero for heliocentric distances around 22.5 au. The reason for this is that the curve on the b-plane defined by this particular value of β passes through the center of Mars. Thus, if ejected at the right distance, i.e., 22.5 au, the particle reaches Mars under the action of solar radiation pressure, with no ejection velocity at all. It is also worth noticing that the β = 0.1 curve does not go all the way back to 30 au because, for such a high β, solar radiation pressure is extremely strong and the particle does not even experience the close encounter with Mars if ejected too far in advance. [Figure 11 about here.] The results obtained so far can be used to assess the possibility that particles of a given size could reach Mars for a given ejection velocity model. For instance, the best fit for the ejection velocity according to Farnham As shown in Fig. 12, we can scale the required velocity to β = 1 and make a comparison to the velocity given by (5). We can see that, according to this ejection velocity model, impacts are possible only for particles with β ∼ 2 × 10 −4 or smaller ejected at more than ∼ 16 au from the Sun. We conclude that impacts are possible only in one of these two unlikely cases: • millimeter to centimeter dust grains are ejected from the nucleus more than 13 au from the Sun; • the ejection velocities are larger than current estimates by a factor > 2. To analyze the risk posed by dust grains in the tail, we computed the required ejection velocities as a function of the heliocentric distance at which the particle is ejected and the particle's β parameter, i.e., the ratio between solar radiation pressure and solar gravity. By comparing our results to the most updated modeling of dust grain ejection velocities, impacts are possible only for β of the order of 10 −4 , which, for a density of 1g/cc, corresponds to millimeter to centimeter particles. However, the particles have to be ejected at more than 13 au, which is generally considered unlikely. See Kelley et al. (2014) As the comet gets closer to the inner solar system, new observations will be available and will allow better constraints on the dust grain ejection velocity profile. Our analysis can be used as a reference to quickly figure out what particles can reach Mars and the heliocentric distance at which they would have to have been ejected. In the unlikely case that future astrometry reveals unexpectedly large nongravitational perturbations, the required velocity to reach Mars for particles ejected within 2  au from the Sun can change and the presented analysis will need to be refined. Figures 1 Scatter plot of the astrometric residuals in Right Ascension and Declination with respect to JPL solution 46. Crosses correspond to rejected observations, while dots correspond to the observations included in the fit."
336,hal-01278907.training.tei.xml,0,"The main benefit of an interstellar mission is to carry out in-situ measurements within a target star system. To allow for extended in-situ measurements, the spacecraft needs to be decelerated. One of the currently most promising technologies for deceleration is the magnetic sail which uses the deflection of interstellar matter via a magnetic field to decelerate the spacecraft. However, while the magnetic sail is very efficient at high velocities, its performance decreases with lower speeds. This leads to deceleration durations ofseveral decades depending on the spacecraft mass. Within the context of Project Dragonfly, initiated by the Initiative of Interstellar Studies (i4is), this paper proposes a novel concept for decelerating a spacecraft on an interstellar mission by combining a magnetic sail with an electric sail. Combining the sails compensates for each technologys shortcomings: A magnetic sail is more effective at higher velocities than the electric sail and vice versa. It is demonstrated that using both sails sequentially outperforms using only the magnetic or electric sail for various mission scenarios and velocity ranges, at a constant total spacecraft mass. For example, for decelerating from 5% c, to interplanetary velocities, a spacecraft with both sails needs about 29 years, whereas the electric sail alone would take 35 years and the magnetic sail about 40 years with a total spacecraft mass of 8250 kg. Furthermore, it is assessed how the combined deceleration system affects the optimal overall mission architecture for different spacecraft masses and cruising speeds. Future work would investigate how operating both systems in parallel instead of sequentially would affect its performance. Moreover, uncertainties in the density of interstellar matter and sail properties need to be explored."
337,hal-01278907.training.tei.xml,1,"The concept of manned and unmanned interstellar missions has been examined in different contexts by many authors within the pastdecades . The main obstacle connected to the design of such a mission, is the necessity for an advanced propulsion system which is able to accelerate the spacecraft towards the target system within a reasonable time span. To overcome the vast interstellar distances, propulsion systems with high specific impulses, like the fusion based engines in the ICARUS and Daedalus projects have been proposed . Other methods rely on propellant-less systems like laser-powered light sails, as described in [4]."
338,hal-01278907.training.tei.xml,2,"Accelerating a probe to high speeds and reaching the target system within short duration using advanced propulsion systems would be a big achievement for mankind. However, the scientific gain of an interstellar mission would be immensely increased with an extensive scientific payload. In order to produce valuable scientific results, the deceleration of the probe is required since it enables the study of star and planetary systems in detail. For a more detailed analysis of exoplanets, involving surface operations, a deceleration down to orbital speeds is necessary."
339,hal-01278907.training.tei.xml,3,"Therefore, apart from the acceleration propulsion system, a further crucial mission component which is often overlooked, is the deceleration system of an interstellar mission. This has to demonstrate equally effective ∆v capabilities as the propulsion system. For that reason, methods utilizing propellant are not preferred, since they would induce large amounts of mass, which are inert during the acceleration and cruising phases of the mission."
340,hal-01278907.training.tei.xml,4,Two attractive concepts rely on utilizing magnetic and electric fields in order to deflect incoming ions of the interstellar space and thereby decelerate effectively.
341,hal-01278907.training.tei.xml,5,"These systems called Magnetic Sail (Msail) and Electric Sail (Esail) were first proposed by Zubrin and Janhunen  respectively. Since each one of those systems has a different design point and velocity application regime in which it performs optimally, the combination of the two can induce great flexibility in the mission design as well as better performance. To demonstrate these points, the example of a mission to Alpha Centauri is analyzed. This star system was chosen because it is the closest one to the earth at a distance of 4.35 light years and because the deceleration concept described in this paper, was inspired by the Dragonfly Competition of the i4is, which involved a light-powered light sail mission to Alpha Centauri ."
342,hal-01278907.training.tei.xml,6,"Before the comparison of the different deceleration methods takes place, the properties of each sail will be shortly analyzed and the assumptions used in the simulation of their performance will be explained."
343,hal-01278907.training.tei.xml,7,"The Msail consists of a superconducting coil and support tethers which connect it to the spacecraft and transfer the forces onto the main structure. The current through the coil produces a magnetic field. When the spacecraft has a non-zero velocity, the stationary ions of the interstellar medium are moving towards the sail in its own reference frame. The interaction of ions with the magnetosphere of the coil leads to a momentum exchange and a force on the sail, along the direction of the incoming charged particles."
344,hal-01278907.training.tei.xml,8,The force on the sail is calculated according to Equation 1.
345,hal-01278907.training.tei.xml,9,"where m p is the mass of the proton, n o the number density of interstellar ions, µ the free space permeability, I the current through the sail, R its radius and v its speed. Values for n o are proposed in [10] in the case of a space probe traveling to Alpha Centauri. In this work, a rather conservative value was implemented, with n o =0.03 cm −3 corresponding to the expected values in the Local Bubble ."
346,hal-01278907.training.tei.xml,10,"The main structural component introducing extra mass into the system is the sail itself, as well as its shielding and its deployment mechanism. The mass of the sail is defined by the maximal current density that can be achieved with the superconducting material, since this dictates the minimal cross sectional area for a specific current. According to Zubrin, the current densities of superconductors can reach up to j max = 2 · 10 10 A/m 2 and this is the value used in the analysis. For the material of the sail, the density of common superconductors like copper oxide (CuO) and YBCO was used, with ρ Msail = 6000 kg/m 3 ."
347,hal-01278907.training.tei.xml,11,The shielding mass required to protect the sail was modeled according to. This mass vaporizes due to collisions with interstellar atoms and ions and the total mass vaporized after time T is given by Equations 2 and 3:
348,hal-01278907.training.tei.xml,12,"In Equation 3, A ion represents the cross sectional area of the coil, as seen from the direction of the incoming ions, ∆H is the vaporization enthalpy of the shielding material and β = v/c. Graphite was chosen as a shielding material since it combines a low density with high vaporization enthalpy. The shielding mass is calculated separately for each configuration, since its calculation requires knowledge of the time-dependent profile for β. For that reason, its calculation is carried out with an iterative procedure."
349,hal-01278907.training.tei.xml,13,"For the tether and support structures, a mass equal to15 % of the sail mass was used."
350,hal-01278907.training.tei.xml,14,"It is evident from the formula in Equation 1, that the magnetic sail is efficient for higher current values and larger dimensions. In the analyses presented in this work, the radius of the Msail was limited to50 km. Although even larger dimensions can demonstrate better performance, it was thought that the deployment of bigger radii is far from the current or near-future technological capabilities and was therefore excluded from the analyses."
351,hal-01278907.training.tei.xml,15,"The main disadvantage of the magnetic sail is also evident when taking the force formula into account. At lower speeds, the force keeps getting reduced asymptotically, and hence the effect of the Msail at these velocities becomes negligible. This has as consequence that reaching orbital speeds (10-100 km/s) requires long deceleration duration. A magnetic sail would therefore be optimal for missions where no orbital insertion or surface operations in planetary systems are required but where a deceleration for prolonged measurements in the target system is sufficient. Its inefficiency in lower speeds indicates the need for a secondary system able to bring the velocity down to orbital values."
352,hal-01278907.training.tei.xml,16,"Similar to the Msail, where a magnetic field deflects incoming ions, the Esail uses an electric field to change the trajectories of the interstellar protons. The sail consists of extended tethers which are charged with a high positive voltage."
353,hal-01278907.training.tei.xml,17,The force on the Esail demonstrates a more complex dependency on the velocity compared to the Msail. The force can be described by Equation 4.
354,hal-01278907.training.tei.xml,18,"with N standing for the number of tethers, L their length, V o the voltage of the sail, e the charge of the electron, r w the wire radius and r o the double Debye length λ D , given by Equation 5:"
355,hal-01278907.training.tei.xml,19,"In the Debye length definition, o is the electric permittivity of vacuum, k b the Boltzmann constant and T e the electron temperature of the interstellar plasma. T e was estimated according to [10], so for the present analysis the value T e =8000 K was used. The wires were designed according to [11], with radius r w = 5 µm and density 1500 kg/m 3 It is evident from Equation 4, that the force increases proportionally to the number and length of the tethers as well as for a higher voltage. The dependency of the Esail force on the velocity of the probe however, displays a more complex character than the one for the Msail. Figure 1 demonstrates this effect qualitatively for a constant total length of the tethers. It follows that the Esail is effective only within a region close to its maximal force. In order to decelerate a probe efficiently from high cruising speeds (≥ 0.04 c) down to orbital values, a very high voltage is required according to Figure 1, or an increased total length of the tethers. However an increase in tether length and voltage does not only imply a higher mass of the wires, but also a bigger power supply system. The positively charged tethers collide with the interstellar plasma electrons, which leads to a decrease of the voltage. In order to maintain the positive voltage of the wires, an electron gun has to be placed on board, leading to an additional mass for the power supply subsystem. The required power is described by Equation 6 :"
356,hal-01278907.training.tei.xml,20,"with m e being the mass of the electron. The total mass of the Esail is put together from the mass of the tethers and the power system required for the operation of the electron gun. In the present work, the power system for the Esail was modeled with a specific power supply of50 W/kg. Although the details of the power system were not part of this analysis, photovoltaic cells could be used, utilizing the laser beam power in combination with radioisotope thermoelectric generators and batteries. Another option is the use of electromagnetic tethers as an energy source, by means of electromagnetic induction as described in [12]."
357,hal-01278907.training.tei.xml,21,"It becomes clear that the Esail has a disadvantage when dealing with high speeds, because of the very high voltage and consequently system mass needed. For that reason, an additional system would be necessary for the initial deceleration from the high cruising speeds until the point where an optimally designed Esail can take over."
358,hal-01278907.training.tei.xml,22,"After establishing the properties and the disadvantages of the individual sails in Section 2, the benefits of combining thetwo subsystems for an effective deceleration in interstellar missions become clear."
359,hal-01278907.training.tei.xml,23,"Missions to neighboring star systems require high cruising speeds in order to reduce the total trip duration. There have been proposals based on fusion propulsion that aim to keep the total mission duration underneath100 years , which means that an average speed bigger than 0.0435 c is necessary in the case of Alpha Centauri . The present analysis focuses on missions with the objective of performing scientific measurements in the target system, hence requiring orbital insertion around a star or a planet. In this context, the combination of Msail and Esail seems to be an elegant solution."
360,hal-01278907.training.tei.xml,24,"Starting the deceleration phase of the mission with the use of a magnetic sail is beneficial as mentioned in Section 2.1, due to the high forces produced in the large velocity range. As the velocity decreases, the force drops also and the Msail starts being ineffective. At this moment (which has to be optimally chosen as described later), the Msail can be switched off and detached from the spacecraft and the Esail can start operating. The electric sail must be designed to perform optimally in this velocity region and can decrease the velocity of the spacecraft further, until the required value for orbital insertion is achieved. The high flexibility of the tandem system comes in the expense of additional optimization effort. Thetwo subsystems are dependent on each other and have to be designed simultaneously and an extra optimization parameter influences their design, namely the velocity value at which the start of operation for the Esail takes place."
361,hal-01278907.training.tei.xml,25,"This idea resembles the concept of staging in conventional launchers with chemical engines. As soon as the first stage is done burning, it is detached, and the second stage, which has been optimally designed to operate in the higher altitude, is ignited. Similarly, as soon as the Msail reaches its weak performance point, it is dropped off and the Esail, which has been optimally designed to decelerate the remnant mass, starts operation."
362,hal-01278907.training.tei.xml,26,"The switching method presented in this paper is only one of the alternatives that can be realized with a combination of Msail and Esail. A further option would be that the Esail starts operation simultaneously with the Msail even at higher speeds, where it is not so effective. One would expect that this extra bit of braking force could improve the overall performance. This idea was not implemented in the present analysis, because the Esail tethers can be used for energy production according to [12] for the velocities that are far from their optimal design point. This way, instead of spending electric power for the operation of the Esail, which only has a small effect on the overall deceleration, the Esail can serve as a significant power supply source."
363,hal-01278907.training.tei.xml,27,"Additionally, allowing the Msail to operate even at the velocity regime where it has lost its efficiency in parallel to the Esail instead of detaching it, would increase the decelerating force. However, the mass being decelerated would also increase and hence the magnitude of acceleration would not necessarily improve. A complete optimization model could include the start of operation of the Esail and the detachment of the Msail astwo separate events. This brings some additional complexity to the model since it requires the optimization of a further parameter. However, it was examined for a single test case which is not in the scope of this paper and the obtained results showed a < 5% performance improvement, so it was ignored in this analysis."
364,hal-01278907.training.tei.xml,28,"An extra benefit of ceasing the use of the Msail when the Esail starts operating, lies in utilizing the magnetically stored energy of the superconductor for the operation of the Esai. The current through the Msail could be discharged into batteries used for the power system of the electron gun before detachment, thereby turning the Msail to a Superconducting Magnetic Energy Storage."
365,hal-01278907.training.tei.xml,29,"These considerations explain why a tandem switching method was preferred to a method where both systems run in parallel. It is easy to understand that the switching point should occur at a speed value where the acceleration with the Msail is equal to the acceleration with the Esail. Switching at a lower speed would imply that there is a time span where the probe is decelerating with a force smaller than what it could achieve by switching to the Esail and would become less effective. The same issue occurs for switching at higher speeds, since it means that the magnetic sail did not reduce the kinetic energy by the amount it was optimally designed to."
366,hal-01278907.training.tei.xml,30,"The consideration of the optimal switching point between Esail and Msail can be qualitatively seen in Fig- ure 2. In this image, a fixed design point for the Esail is chosen and an optimal design for the Msail is searched for. It is obvious, that the choice of an overly dimensioned Msail, like in the case of the design point A, is not very efficient. The intersection point of the acceleration profiles for Esail and Msail lies at velocities smaller than the point of the maximum Esail deceleration. Therefore, after the switch, the magnitude of acceleration would keep dropping and the highest Esail force would never be utilized. Although the acceleration magnitude would be bigger than what the Msail could have produced, the full potential of the Esail would still remain unused."
367,hal-01278907.training.tei.xml,31,"In the case of profile B, the Msail is under dimensioned, hence leading to a high velocity for the switching point. At this regime, the Esail demonstrates a very low force and therefore does not reduce the speed of the probe efficiently. A significant time period has to elapse until the velocity reaches the optimal design point of the Esail, where the acceleration value is big enough to produce an effective braking of the spacecraft."
368,hal-01278907.training.tei.xml,32,"Finally, case C seems to produce a better deceleration profile. The switching point lies in speeds higher than the optimal design point of the Esail. The Esail acceleration starts increasing immediately after the detachment of the Msail and is close to the optimal value, therefore utilizing the full potential of the electric sail, before starting to drop again."
369,hal-01278907.training.tei.xml,33,"The combination of thetwo sails requires the optimization of the individual parameters for Msail and Esail (radius and current of superconductive loop, voltage, number and length of tethers) as well as of the velocity at which the operation of the Msail ceases."
370,hal-01278907.training.tei.xml,34,The optimization problem that was solved to come up with the optimal design of the deceleration system can be expressed as the minimization of the total deceleration duration T decel .
371,hal-01278907.training.tei.xml,35,"To determine T decel for a given sail configuration, the mass of the system and the force profile over time are necessary. The parameters N,L,V o ,R and I allow for the determination of m Esail and m Msail. The combination of Msail and Esail requires the additional parameter of the switching velocity v switch which results to the force profile. Combining the mass and the force leads to the acceleration capabilities of the system. This way, the optimization parameters of the mathematical problem are summarized in Table 1 for thethree deceleration methods."
372,hal-01278907.training.tei.xml,36,"For a given acceleration dependency on the velocity, a(v), the total duration of the deceleration period (the cost function) is given by the expression in Equation 7:"
373,hal-01278907.training.tei.xml,37,"In the case of tandem deceleration, this takes the form of Equation 8:"
374,hal-01278907.training.tei.xml,38,"and the objective of the minimization problem is summarized in: Since the acceleration part of the mission is not captured in this analysis, the absence of any further constraints would shift the optimal solution to very high deceleration system masses. Since the performance of the system increases with increasing mass, an overly dimensioned Msail and Esail with infinite mass would minimize the cost function T decel. When combined with the acceleration system however, such a large system would be inefficient since it would pose a large inert mass during the acceleration phase. For that reason, an additional constraint was introduced, namely an upper bound for the maximal deceleration mass. Hence this extra constraint was introduced as in Equation 10:"
375,hal-01278907.training.tei.xml,39,with C being a predefined upper mass limit and
376,hal-01278907.training.tei.xml,40,"This constraint is directly applied in the definition of the cost function T decel , since it sets the limits of the integral calculation."
377,hal-01278907.training.tei.xml,41,"In the case of the Msail and Esail combination, the switching velocity is to be modeled as well. One constraint for v switch is already present in Equation 8, since it is set as the limit of the integral to be evaluated. Moreover, it has to be made sure, that the acceleration at the switching point between Msail and Esail remains continuous, as described in Section 3. Mathematically this yields:"
378,hal-01278907.training.tei.xml,42,"where m s/c stands for the spacecraft mass. Moreover, as explained in Section 3, the switching point has to take place for velocities larger than the optimal operation point of the Esail and therefore:"
379,hal-01278907.training.tei.xml,43,"Finally, the total deceleration distance r decel poses a further constraint. It has to be ensured, that there is sufficient distance available for the spacecraft to decelerate completely before it reaches Alpha Centauri. For that reason this should remain shorter than4.35 light years."
380,hal-01278907.training.tei.xml,44,"At the same time, there has to be some finite distance available for the acceleration and cruising phases, which are not part of the optimization and this was estimated equal to1.5 light years. For that reason, the constraint was defined as in Equation 14:"
381,hal-01278907.training.tei.xml,45,"The cost function to be minimized (T decel ) is highly non-linearly dependent on the optimization parameters, and therefore linear programming methods were not useful. Moreover, due the lack of knowledge of the function gradient, the optimization took place with a pattern search method similar to the ""direct search"" proposed by Hooke. This is the method utilized for all analyses in the present paper."
382,hal-01278907.training.tei.xml,46,"After obtaining the optimal deceleration duration, the velocity and acceleration profiles as a function of time were calculated by means of numerical integration. A time propagation was implemented using a 4th order Runge-Kutta scheme, which served as a validation of the optimization results and provided a complete time profile of the spacecraft trajectory."
383,hal-01278907.training.tei.xml,47,"Using the optimization method in Section 4, the performance ofthree separate deceleration methods was compared and the resuls are shown in this section. The three deceleration architectures are the following:"
384,hal-01278907.training.tei.xml,48,"In this test case, the mass of the spacecraft m s/c was chosen to be approximately equal to the launch mass of Voyager 1, so equal to750 kg. Voyager is a space probe which was launched to perform flybys of Jupiter, Saturn and Titan and continued to explore the boundaries of the outer heliosphere . Since it is the only manmade probe so close to entering the interstellar space , it was considered relevant to calculate how its deceleration would look like in the case of a mission to another star system, requiring a deceleration phase."
385,hal-01278907.training.tei.xml,49,"Only the deceleration phase of the mission was examined, so a cruising speed v cruise =0.05 c was chosen. The target speed was set to be equal to v target = 35 km/s. This would correspond approximately to the orbital speed at a distance of 1 AU around Alpha Centauri A, which has a mass of 1.1 M ."
386,hal-01278907.training.tei.xml,50,"For each one of thethree deceleration methods, an optimal design point was calculated in order to minimize the total deceleration duration T decel. The mass of the deceleration system was restricted to be underneath 7500 kg, which corresponds to the tenfold spacecraft mass. A direct comparison is thereby possible, since all systems have the same effect on the acceleration phase and hence the overall mission design."
387,hal-01278907.training.tei.xml,51,"At this point it has to be noted, that the restriction of the Msail radius described in Section 2.1 produces very week forces in the low speed limit (close to v target ), thereby resulting in duration close to300 years. It was therefore dismissed from the calculations of pure Msail deceleration. The results shown here required a sail radius of 1000 km, which was considered to be unrealistic but was still included for completion. This demonstrates once again that the Msail as a standalone component is not sufficient for missions requiring orbital insertion in the target system."
388,hal-01278907.training.tei.xml,52,"The acceleration and velocity profiles over time are shown in Figure 3 and 4 respectively. Note that the curves in Figure 3 represent the magnitude of the acceleration, since the numeric values of acceleration are negative during the braking phase. The combination of thetwo sails requires 28.8 years as opposed to the 39.7 years of the Msail and the 34.9 years of the Esail. In the acceleration profile of the dual system, the discontinuity in the gradient represents the point where the switch between Msail and Esail takes place. This occurs after 13.67 years and at a speed equal to approximately 0.03 c according to Figure 4. This change is not detectable in the velocity profile, since the acceleration shows no discontinuity during the switch from the one system to the other, leading to a smooth velocity curve."
389,hal-01278907.training.tei.xml,53,"Initially, the acceleration of the Msail method is the highest. This makes sense because the magnetic sail used in the tandem method is smaller than in the pure Msail method, in order to satisfy the equal mass requirement. After some time however, the magnitude of the acceleration in the tandem method becomes larger and eventually leads to a smaller duration."
390,hal-01278907.training.tei.xml,54,"At this point, it is also important to mention that the pure Msail method is the optimal solution when a higher target speed is needed. Figure 4 demonstrates this effect since the velocity curve of the Msail is lower than the othertwo for the whole duration apart from the lower velocity range, where it flattens. The absence of orbital insertion (leading to v target being an order of magnitude larger), would therefore make the Msail the most effective solution. This test case demonstrates the potential that a combination of Msail and Esail has in the design of an interstellar mission, since it outperforms each individual system in particular mission configurations. However, during a complete mission design, the minimal deceleration duration is not the only parameter to be optimized and the interaction of the deceleration system with the other components (influence on acceleration, effect of deceleration distance) has to be taken into account."
391,hal-01278907.training.tei.xml,55,"After having established that the method of tandem deceleration with Msail and Esail can bring benefits to the total duration of the deceleration phase before orbital capture, it is interesting to determine how this sys-tem interacts with the acceleration and cruising phases."
392,hal-01278907.training.tei.xml,56,"In Section 3, a single value for the cruising speed was examined. In this section, the effect of a variable cruising speed on the design characteristics of the tandem deceleration system is presented. For this analysis,two different spacecraft masses are compared. Apart from the Voyager-like spacecraft introduced in Section 3, the profile of a heavier vehicle with m s/c = 4000 kg is calculated. This value was chosen since it is approximately equal to the launch mass of the Mars Science Laboratory (MSL). This robotic space probe was sent to Mars and included a rover with a landing system and instruments for biological, geochemical and geological measurements on the surface of the planet . Since a similar mission to an exoplanet would be of high scientific value , an MSLlike spacecraft was used. The restriction for the total mass of the deceleration system being maximally ten times the spacecraft mass was maintained. 6 show the dependency of the deceleration duration and distance on the cruising speed. It is intuitive that a larger initial speed requires a larger deceleration duration, since the total ∆v that has to be provided by the deceleration system increases."
393,hal-01278907.training.tei.xml,57,"The same occurs for the deceleration distance, as Figure 6 demonstrates. A higher spacecraft mass also increases the inertia of the system during deceleration and hence the time and distance required. An important indirect result stemming from Figure 6 is that high cruising speeds are not always optimal for a minimal mission duration. In the case of the4000 kg spacecraft, a 0.1 c cruising speed leads to a deceleration distance close to 2.5 light years. When taking into account that the distance to Alpha Centauri is 4.35 light years, one deduces that there are only 1.85 light years available for the acceleration and cruising phases. However, the buildup of such a high speed could require a larger acceleration distance depending on the propulsion system. Therefore, reaching such a high speed in a mission to Alpha Centauri may not be necessary or useful, due to the extreme deceleration distance connected to it."
394,hal-01278907.training.tei.xml,58,The mass and velocity change distribution between Msail and Esail are also interesting to examine as a function of the cruising speed. Figure 8 shows the ratio of the Msail mass m Msail to the Esail mass m Esail and Figure 7 the ratio of the velocity changes ∆v Msail and ∆v Esail at the optimal configuration for each crusing speed.
395,hal-01278907.training.tei.xml,59,"The velocity change ratio demonstrates a nearly linear profile in Figure 7, which increases with the cruising speed. This can be explained with the good performance of the Msail in higher speeds. Since the Msail is efficient in the high speed regime, it is logical that it will also take over most of the deceleration. Moreover, the results show that a higher spacecraft mass leads to a lower ∆v-ratio."
396,hal-01278907.training.tei.xml,60,"Since the velocity changes are proportional to the mass of each subsystem, it is expected that the mass ratio also increases with the cruising speed, as shown in Figure 8. In this case however, the increase in mass ratio tends to be slower and resembles a logarithmic growth."
397,hal-01278907.training.tei.xml,61,The results show a general preference towards the Msail deceleration for higher cruising velocities which is reflected in the ∆v and mass distribution of the decel
398,hal-01278907.training.tei.xml,62,"The deceleration system is an integral part of the mission design and cannot be analyzed independently of the acceleration phase when an interstellar mission is being developed. The main effect that the deceleration system has on the acceleration phase is its mass, which needs to be accelerated as well. Therefore, a deceleration system which is as light as possible but still produces the necessary ∆v change in short amount of time and in short distance is required."
399,hal-01278907.training.tei.xml,63,"The effect of the tandem deceleration system mass on its performance was examined. In the previous sections, the requirement of the deceleration system mass being smaller thanten times the spacecraft mass was utilized. This boundary condition was introduced so that an easier comparison between different configurations could take place. In the present analysis however, the ratio between deceleration system mass and spacecraft mass was varied. The two spacecraft masses described in Section 6.1 as well as two different cases for the cruising speed were compared to each other. Figure 9 shows the results. An increased mass of the deceleration system leads, as expected, to a shorter deceleration duration. It is however notable, that the curves tend to saturate for larger masses. This implies that a larger deceleration mass, although having a great impact on the design of the acceleration phase because of additional inertia, only provides a small benefit to the overall deceleration performance. Quantitatively, taking the example of the 4000 kg spacecraft with 0.08 c cruising speed in Figure 9, one observes that a mass ratio of 10 leads to a minimal duration equal to 53.83 years whereas a mass ratio equal to 4 results in 55.90 years. Hence an increase of 150 % in the mass of the deceleration system, produces only a 3.7 % increase in the performance of the system. This trend is maintained for all configurations and it is evident, that when the complete mission is designed and all mission phases are optimized simultaneously, deceleration system masses are preferred, which are further from the saturation limit and still produce sufficient performance."
400,hal-01278907.training.tei.xml,64,"Magnetic and electric sails have been proposed as propulsion systems for interstellar and interplanetary missions. In the case of interstellar missions with short trip duration and need for orbital insertion around a target system, each one of these sails demonstrates some disadvantages: Msails fail to produce sufficient forces in the low speed limit and Esails require very large masses in order to decelerate from the high cruising speeds of interstellar missions."
401,hal-01278907.training.tei.xml,65,"The present paper demonstrated that a combination of thetwo systems in tandem (initial deceleration with Msail and following braking with Esail) can have a better performance in certain configurations. Small unmanned missions were examined in this context and a generalization of this method for manned missions with larger spacecraft masses would be interesting since it would show the applicability limits of the system. The combination of the two systems in series is not the only method that could improve the deceleration characteristics. Although this was the main architecture analyzed in the paper, operation of the two sails in parallel should also be further examined and controlled for additional increase in performance."
402,hal-01278907.training.tei.xml,66,"The overall design of an interstellar mission requires the optimization of the deceleration system not as a standalone component, but simultaneously with the main propulsion system of the acceleration phase and with the design of the cruising phase. The flexibility of the combination of thetwo sails includes further optimization parameters in the mission architecture, since the switching point between Msail and Esail deceleration has to be also optimized for maximal performance."
403,hal-01278907.training.tei.xml,67,"Finally, the technical design of each sail, including the chosen density of the materials, power system, shield masses etc. as well as parameters with uncertainty, like the properties of the interstellar plasma, influence the optimal solution and should be carefully treated when an interstellar mission is being designed, because they directly affect the deceleration performance and consequently the overall mission architecture."
404,1404.7168.training.tei.xml,0,"Comet C/2013 A1 (Siding Spring) will have a close encounter with Mars onOctober 19, 2014. We model the dynamical evolution of dust grains from the time of their ejection from the comet nucleus to the Mars close encounter, and determine the flux at Mars. Constraints on the ejection velocity from Hubble Space Telescope observations indicate that the bulk of the grains will likely miss Mars, although it is possible that a few-percent of grains with higher velocities will reach Mars, peaking approximately 90–100minutes after the close approach of the nucleus, and consisting mostly of millimeter-radius grains ejected from the comet nucleus at a heliocentric distance of approximately 9 AU or larger. At higher velocities, younger grains from sub-millimeter to several millimeter can reach Mars too, although an even smaller fraction of grains is expected have these velocities, with negligible effect on the peak timing. Using NEOWISE observations of the comet, we can estimate that the maximum fluence will be of the order of 10 −7 grains/m 2. We include a detailed analysis of how the expected fluence depends on the grain density, ejection velocity, and size-frequency distribution, to account for current model uncertainties and in preparation of possible refined model values in the near future."
405,1404.7168.training.tei.xml,1,"Comet C/2013 A1 (Siding Spring) will have a close approach (hereafter c/a) with Mars onOctober 19, 2014 at approximately 18:29 UT, reaching a minimum distance of approximately 134,000km from the center of Mars, according to JPL solution #46 (Farnocchia et al. 2014). This orbital solution was retrieved from the JPL Horizons system (Giorgini et al. 1996), on , and include all the observations before the low solar elongation period. The comet is on a hyperbolic retrograde orbit (129 ° inclination), and the encounter with Mars will be at a relative velocity of approximately 56km/s. Observed from Mars, the comet reaches a minimum solar elongation of approximately 72 °. After the close approach between the two bodies, Mars continues to move closer to the orbit of the comet, crossing the comet orbit plane approximately 100minutes after c/a and then reaching a minimum distance of approximately 27,000km from the comet orbit approximately 102minutes after c/a. A detailed trajectory analysis by Farnocchia et al. (2014) indicates that the current uncertainty on the c/a distance, projected on the target plane normal to the velocity of the comet nucleus relative to Mars, is an ellipse approximately 1600 × 700km (1σ) with the long axis 20 ° from the line connecting the comet nucleus and Mars, and the c/a time uncertainty is about 1minute (1σ). These uncertainties exclude any collision of the comet nucleus with Mars, yet warrant a detailed investigation of the possibility that the dust coma of the comet may be able to reach Mars."
406,1404.7168.training.tei.xml,2,"C/2013 A1 was discovered at a heliocentric distance of7.2 AU (McNaught et al. 2013 ), suggesting hyper-volatile activity (i.e., driven by CO or CO 2 ) was likely causing long-term build-up of larger grains in the coma. Hubble Space Telescope (HST) observations by Li et al. (2014) allow us to estimate the ejection velocity of the dust grains at the µm radius scale, or more precisely grains with β ≃ 1, where β is the ratio between the radiation pressure force and the gravitational force due to the Sun on the grain (Burns et al. 1979). Observations were performed on October 29, 2013, on Jan 21, 2014, and on March 11, 2014. By using a technique based on the sunward turn back distance in the continuum (e.g., Mueller et al. 2013 ) we determined a velocity approximately between 28 and 46 m/s at a heliocentric distance of 4.59 AU, then between 44 and 74 m/s at 3.77 AU, and between 48 and 80 m/s at 3.28 AU, all for β = 1 dust grains, see Figure 1. The ranges are relatively broad because of the inherent difficulty of determining a single value for the turn back distance in a continuous brightness distribution."
407,1404.7168.training.tei.xml,3,"NEOWISE obtained infrared observations of C/2013 A1 onJan 16, 2014 (Mainzer et al. 2014), when the comet was at 3.82 AU from the Sun. We used these observations to model the dust production. Adopting the techniques in Bauer et al. (2011) yields a dust production rate of approximately 10kg/s at the grain velocities for that heliocentric distance, and for grains with radius between 1.7 µm and 2.3 µm (Mainzer et al. 2014)."
408,1404.7168.training.tei.xml,4,"In this paper we use the HST and NEOWISE observations to anchor the grain ejection velocity and production rate. In the next Section we discuss how we use these constraints to model the velocity of larger dust grains ejected at larger heliocentric distances, which are in the relevant range for delivery of dust grains to Mars. In Section 3, we present the results from numerical simulations where we model the dynamical evolution of dust grains after ejection from the comet nucleus to their close approach with Mars. In Section 4 we discuss the consequences of our modeling choices and how they likely affect the outcomes of the simulations, and compare our results with earlier works which characterize the delivery of dust grains to Mars (Moorhead et al. 2014; Vaubaillon et al. 2014; Ye & Hui 2014). Conclusion are then drawn in Section 5."
409,1404.7168.training.tei.xml,5,"The trajectories of the comet nucleus and of the dust grains are determined numerically by integrating the equations of motion, using an efficient numerical integrator with variable timestep (Everhart 1974), and that includes gravitational perturbations and solar radiation pressure (Burns et al. 1979). The state vectors of the Sun and the planets are obtained from the DE405 planetary ephemeris (Standish 1998). The initial conditions of the comet nucleus are chosen at the time of the c/a, as retrieved from the Horizons JPL system (Giorgini et al. 1996), and are then integrated backward in time to the epoch when the comet was at10 AU, i.e., the heliocentric distance that we assume as the beginning of the activity of the comet. The maximum heliocentric distance of 10 AU is chosen based on pre-discovery images of the active comet at approximately 8 AU, and considering that the region of the sky where the comet was between 8 and 10 AU was not covered by surveys (T. Spahr, MPC, priv. comm.). The comet nucleus is considered massless, and grains are assigned a radius and density in order to compute the effects of radiation pressure, and are integrated individually. As a trade-off between simulation performance and accuracy, gravitational interactions are computed only with the Sun, Jupiter, and Mars, and limit the nominal accuracy of the numerical integrator. This causes a significant speed-up of simulations compared to including all planets, and causes a difference in c/a distance of dust grains of the order of a few hundred kilometers. This is relatively small compared to the few thousand kilometers uncertainty on the comet c/a distance, especially considering the statistical nature of this study. Dust grains are ejected isotropically from the nucleus, at a velocity that is determined using an empirical scaling law, based on the results in Whipple (1951). This choice is due to the difficulty of developing a detailed self-consistent physical model of grain ejection driven by hyper-volatiles at large heliocentric distances and then water within approximately 3 AU from the Sun, as such a model would require some mechanisms for the depletion of hyper-volatiles at shorter heliocentric distance, or would otherwise produce unrealistically large ejection velocities. The ejection velocity is:"
410,1404.7168.training.tei.xml,6,"where we choose the reference grain to have radius r ref =1 mm and density ρ ref = 1 g cm −3 , which correspond to β ref ≃ 5.76 × 10 −4 , and D ref = 5 AU for the heliocentric distance. The value of D ref is chosen to be approximately halfway between the comet activation and the comet close encounter with Mars. Since these reference values are fixed for all simulations, we have v ref as the only free parameter characterizing the ejection velocity of all grains within each simulation. Because of the lack of an estimate of the nucleus size, we consider the ejection velocities defined outside of the gravitational influence region of the nucleus. InFigure 1 we show how the velocities from our modeling compare to the observed HST velocities. The three HST observations (Li et al. 2014) indicate velocities of micron-size grains roughly compatible with v ref between 0.8 and 1.0 m/s. Because of the significant dispersion in the velocity range, and the limited number of HST observations, we choose not to fit the exponent of the heliocentric distance D in Eq. (1)"
411,1404.7168.training.tei.xml,7,"The density of dust grains ejected from comets depends strongly on what they are made up of and the bulk porosity of such dust grains. In the literature we find estimates as low as approximately0.1g/cm 3 (Greenberg & Li 1999 ), and as high as approximately 2g/cm 3 for interplanetary dust particles (Love et al. 1994), although that may not necessarily be representative of cometary dust. In our simulations we use a grain density of 1.0g/cm 3 and provide scaling laws to different densities for all the main results."
412,1404.7168.training.tei.xml,8,"The size-frequency distribution (SFD) and production rate Q of grains are assigned during post-processing of the simulations. First, we used a uniform distribution (in log scale) in grain radius between1 µm to 10 cm during the simulations, and a constant production rate. Then in post-processing each simulated grain is scaled up to represent a far larger number of real grains, according to the specific SFD and Q distributions selected. The SFD is parameterized using the differential distribution dQ/dr ∝ r −a and Q depends on the heliocentric distance as Q ∝ D −2 . The index a for cometary dust grains depends on many factors including the heliocentric distance, the specific comet, and the specific grain sizes under consideration (e.g., Hanner & Bradley 2004; Fulle 2004, and references therein). However, many authors find that adopting a single power law index is convenient for modeling purposes although there are a number of counter examples against this approach (e.g., comet 1P/Halley, comet 2P/Encke). Therefore, as a compromise, we adopt the values of 3.5 and 4 for a, which are representative of many comets based on dust tail modeling (Fulle 2004, and references therein)."
413,1404.7168.training.tei.xml,9,"The flux of grains at Mars is determined from simulations after accounting for SFD and Q, by tracking the grains which come within3,400 km from the center of Mars. For scaling purposes, the grains are binned logarithmically into 3 intervals per decade in grain radius, and in linear intervals of size 0.2 AU in heliocentric distance. The properties of the grains are assumed to be uniform within each bin."
414,1404.7168.training.tei.xml,10,"We performed several simulations, each one with a different v ref value. In each simulation, we generatedfive million grains and tracked their trajectory as described in Section 2. Typically only a few hundred to a thousand simulated grains cross the Mars volume, which we approximate with a sphere of radius 3,400 km. In preliminary simulations we used a grain radius between 10 −6 and 10−1meter , and then reduced this range to 10−4 to 10 −2 meter, as this is the relevant range for Mars collisions, see Figure 2. Note how the increased velocity of grains causes a wider range in grain radius to reach Mars, and also how the arrival time window grows accordingly. At the highest velocity tested of v ref = 3.0 m/s we start to see smaller grains arriving at early times, and this effect becomes dominant at higher velocities. The grains have to be ejected at large heliocentric distances to reach Mars: beyond 9.1 AU for v ref = 1.5 m/s, beyond 6.7 AU for v ref = 2.0 m/s, beyond 4.6 AU for v ref = 2.5 m/s, and beyond 2.4 AU for v ref = 3.0 m/s. Therefore, if the comet was not active at heliocentric distance beyond 8 AU and the maximum v ref was 1.5 m/s, no grains could reach Mars."
415,1404.7168.training.tei.xml,11,"The arrival time of grains is well concentrated for lower ejection velocities, with the peak of the flux between90 and 100 minutes after the close approach time of the nucleus, see Figure 3. As we increase the grain ejection velocity, the arrival time interval tends to widen, while the peak tends to move to earlier times. At v ref = 3.0 m/s the peak is well below one hour after c/a, and the flux tends to have a fast raise and then a slow decay, over one hour long."
416,1404.7168.training.tei.xml,12,"In Figure 4 we have the integrated fluence (count and mass) versus the grain ejection velocity parameter v ref and at different SFD index values. The figure provides the fluence values for all ejected grains at the given v ref value. However, as we have discussed above, HST observations indicate lower velocities at which no grains intercept Mars. The way to interpret these plots is then to assume that while the v ref parameter from HST indicates the velocity of the bulk of the grains, it is possible that the velocity distribution of grains contains an high-velocity tail, so that only afew-percent of the grains will have these higher velocities. Processes responsible for this could be the presence of jets, or the shape of the nucleus causing non-uniformities in the gas flow and in the acceleration of grains from the surface. In view of this interpretation, the fluence values in Figure 4 should be decreased by several orders of magnitude as we move to higher values of v ref , to reflect the smaller and smaller fraction of grains which could be in the high-velocity tail. As a result, the fluence decreases by approximately two orders of magnitude at v ref ≃ 1.5 m/s and by several orders of magnitude at higher v ref values, to obtain a maximum fluence of the order of 10 −7 grains/m 2."
417,1404.7168.training.tei.xml,13,"The radiant of the dust grains is at R.A.41.1 ° and Dec. −15.5 ° in the constellation Cetus. In Figure 5 we show the hemisphere of Mars that is exposed to dust grains. Depending on the velocity of the grains, one side of Mars is accessible at lower velocities than the other one. Thus, it is reasonable to expect that the flux of grains in the low-v ref side will be somewhat higher than on the opposite side."
418,1404.7168.training.tei.xml,14,"The results presented here can be compared with previous works already in the literature. In Moorhead et al. (2014) the dust grains fluence is determined using both analytical and numerical approaches, obtaining a count fluence of0.15 m −2. In their model, the comet orbit solution was not yet as constrained as ours, the grain densities used (0.1 g/cm 3) are relatively low, and the ejection velocities appear significantly higher than what we used. We find that the modeling details and parameters in Moorhead et al. (2014) are not supported by observations such as we used in our work, and the large difference in fluence is a direct consequence of very different models."
419,1404.7168.training.tei.xml,15,"In Vaubaillon et al. (2014),two different numerical approaches are used to obtain a count fluence of 3.5 m −2, with grains reaching Mars over a five hours period centered at 2014-10-19T20:00Z UT. The model uses very high ejection velocities, assumes that water-driven production will be principally responsible for grains delivery to Mars, and uses very large volumes to compute the fluence, averaging low flux region and high density regions near the comet nucleus, strongly biasing the results towards high fluence values. In Ye & Hui (2014) a dynamical model very similar to ours is employed, including a similar velocity scaling and similar constraints from observations, and an orbit solution just a few weeks older than ours. They obtain no collisions with Mars at the nominal grain velocities, and fluence similar to ours if the grain velocity is increased, after accounting for different parameters (density and SFD index). Although the peak timing tends to shorter delays after c/a at high v ref values (see Figure 3), the strong attenuation from the velocity distribution causes the v ref ≃ 1.5 m/s timing to dominate, with a peak between 90 and 100 minutes after c/a. The timings provided in this paper refer to the current estimate of the c/a time as indicated in the Introduction, and will not change even if the nucleus c/a will change due to possible strong non-gravitational forces which may become important in the last few months before c/a (Farnocchia et al. 2014). As a matter of fact, Farnocchia et al. (2014) show that non-gravitational perturbations may become relevant only when approaching the perihelion, while the dust grains that may reach Mars are ejected at larger heliocentric distances."
420,1404.7168.training.tei.xml,16,"We did not include fragmentation of grains in this study. The most likely consequence of fragmentation would be to decrease the probability to reach Mars, because of the increased radiation pressure effects."
421,1404.7168.training.tei.xml,17,"We have presented the results of our dynamical model for the delivery of dust grains ejected from the nucleus of comet C/2013 A1 (Siding Spring) to Mars. Direct extrapolation of the grain ejection velocity from HST observations indicates that the bulk of the grains will likely miss Mars. If we include the possibility of afew-percent of the grains to have higher velocities, we find that millimeter radius grains ejected at 9 AU or more from the Sun will collide with Mars. At higher velocities, younger grains from sub-millimeter to several millimeter can reach Mars too. The maximum fluence will be of the order of 10 −7 grains/m 2 . The timing of the peak flux is expected to be 90–100 minutes after c/a, which is between 19:59 and 20:09 UT of October 19, 2014."
422,hal-01321536.training.tei.xml,0,"To cite this version: Jean Clairambault, Olivier Fercoq. Physiologically structured cell population dynamic models with applications to combined drug delivery optimisation in oncology."
423,hal-01321536.training.tei.xml,1,"Optimising drug delivery in the general circulation targeted towards cancer cell populations, but inevitably reaching also proliferating healthy cell populations imposes to design optimised drug infusion algorithms in a dynamic way, i.e., controlling the growth of both populations simultaneously by the action of the drugs in use, wanted for cancer cells, and unwanted for toxic side effects on healthy cells."
424,hal-01321536.training.tei.xml,2,"Towards this goal, we design models and methods, with optional representation of circadian clock control on proliferation in both populations, according tothree axes : a) representing the oncologist's main weapons, drugs, and their fates in the organism by molecular-based pharmacokinetic-pharmacodynamic equations; b) representing the cell populations under attack by drugs, and their proliferation dynamics, including in the models molecular and functional targets for the drugs at stake, by physiologically structured equations; c) using numerical algorithms, optimising drug delivery under different constraints at the whole organism level, representing impacts of multiple drugs with different targets on cell populations."
425,hal-01321536.training.tei.xml,3,"In the present study,two molecular pharmacological ODE models, one for oxaliplatin, and one for 5Fluorouracil, have been designed, using law of mass action and enzyme kinetics, to describe the fate of these two cytotoxic drugs in the organism. An age-structured PDE cell population model has been designed with drug control targets to represent the effects of oxaliplatin and 5-Fluorouracil on the cell cycle in proliferating cell populations. The models for proliferating cell population dynamics involve possible physiological fixed (i.e., out of reach of therapeutic influence) circadian clock control, and varying drug control to be optimised, connected with pharmacological models."
426,hal-01321536.training.tei.xml,4,"Concentrations of drugs, represented by outputs of ODEs, are assumed to be homogeneous in the cell populations under attack by cytotoxic drugs. The possibility to describe the effects of other drugs, cytostatic (including in this category anti-angiogenic drugs, considered as acting on the G 1phase, choking its entries and slowing it down), is also presented, but not put in pharmacokinetic equations and actual simulations in this study, that is focused on the combination of 5-FU and oxaliplatin, a classic therapeutic association in the treatment of colorectal cancer."
427,hal-01321536.training.tei.xml,5,"We then set conditions to numerically solve drug delivery optimisation problems (maximisation of cancer cell kill under the constraint of preserving healthy cells over a tolerability threshold) by considering a trade-off between therapeutic and toxic effects. The observed effects on proliferation are growth exponents, i.e., first eigenvalues of the linear PDE systems, in thetwo populations, healthy and cancer. The solutions to an optimisation problem taking into account circadian clock control are presented as best delivery time schedules for the two drugs used in combined treatments, to be implemented in programmable delivery pumps in the clinic."
428,hal-01321536.training.tei.xml,6,"We represent drug fate in the organism by physiologically based pharmacokinetic-pharmacodynamic (PK-PD) ODEs for drug concentrations. Drug effects, outputs of the PK-PD system, will later be considered as control inputs in a physiologically structured cell population dynamic model representing cancer or healthy tissue proliferation with prescribed targets (see Section 3). In this Section, we considertwo cytotoxic drugs, oxaliplatin and 5-fluorouracil, that act on cells by killing them unless they undergo repair, and a cytostatic drug, assumed to exert a blocking action at phase transitions of the cell cycle without inducing cell death."
429,hal-01321536.training.tei.xml,7,"Drugs are described by concentrations, from their infusion in the circulation, which is represented by infusion flows assumed to be externally controlled by programmable pumps, until their action at the cell level. Whenever relevant, physiological control of biological mechanisms by circadian clocks is represented here by cosine-like curves-in the absence of better identified physiological gating functions-describing gate opening at transitions between cell cycle phases (0: gate closed; between 0 and 1: gate opening)."
430,hal-01321536.training.tei.xml,8,"Oxaliplatin is an anticancer drug that has been in use for more than12 years now in the treatment of colorectal cancer. It exerts its action on cells by creating irreversible oxaliplatin-DNA adducts that subsequently yield doublestranded breaks in the DNA. Although it has been shown to induce apoptosis  in a manner that does not show major dependency on cell cycle phases (with the restriction that of course the DNA is always more exposed, and thus more sensitive to oxaliplatin insults, in S-phase), other studies have shown that its main effect on cancer cells was likely to be mostly active by arresting the cell division cycle at the G 2 /M, and to a lesser extent, at the G 1 /S transitions , effects on apoptosis occurring only after a prolonged exposure time. In the sequel, we shall assess these two main actions, both induced by DNA damage. The effect on death rates will be assumed to be directly proportional to the damage induced to the DNA, whereas the effect on phase transitions, physiologically due mainly to p53 (not represented as such in this study), will be represented by a variable delay due to cells under repair, that will be represented by a specific subpopulation appended to each one of the main phases G 1 and S − G 2 ."
431,hal-01321536.training.tei.xml,9,The PK-PD model for oxaliplatin runs as follows
432,hal-01321536.training.tei.xml,10,", accounting for circadian tuning (if δ =0, ε = 0) of equilibrium values for tissue reduced glutathione (GSH) G 0 and plasma protein (PLP) K 0 concentrations. The variables represented are concentrations, either in blood, or in tissues for: R : Plasma oxaliplatin, with source term a continuous infusion flow i(t) K : Plasma proteins, with equilibrium value K 0 (circadian-tuned if ε = 0) C : Active tissue oxaliplatin (Pt 4+ ion) F : Free DNA, with equilibrium value F 0 : the target of the drug, that establishes DNA adducts by disulphur bridges with it, later creating double strand breaks G : Reduced glutathione (GSH), tissue shield against DNA damage induced by oxaliplatin, with equilibrium value G 0 (circadian-tuned if δ = 0)."
433,hal-01321536.training.tei.xml,11,"A simulation of the model behaviour under square wave drug infusion flow of oxaliplatin following a circadian rhythm is shown on Fig. 1. oscillations of plasma proteins in the presence of infused plasma oxaliplatin (4 consecutive days of chemotherapy followed by 10 days of recovery); lower track: oscillations of the concentrations of cellular reduced glutathione in presence of Pt 4+ ion, active intracellular metabolite of oxaliplatin. Decaying track: modelled apoptosis effect of oxaliplatin; in the absence of repair, the density of free DNA decreases in the mean, leading cells to death. The parameters used in these simulations (see Section 4.4 for details) have been chosen to produce expected effects, and have not been experimentally identified."
434,hal-01321536.training.tei.xml,12,"The features of the model are: irreversible binding of oxaliplatin to plasma proteins, synthesized in the liver; simultaneous attack in the intracellular medium of free (unbound) DNA, the drug target, and reduced glutathione, that thus acts as a competitive target of oxaliplatin, shielding the DNA from oxaliplatin; continuous synthesis of reduced glutathione; possible return to equilibrium value of DNA obtained by excision repair enzymes. Apart from blood pharmacokinetic constants that are easily accessible, other parameters have been evaluated so as to produce likely behaviour for the drug in tissues."
435,hal-01321536.training.tei.xml,13,"When circadian rhythms are considered relevant (i.e., among teams who practice anticancer circadian chronotherapy, cf.), peak phases ϕ PLP and ϕ GSH for tuned values K 0 and G 0 are fixed according to laboratory or clinical published data  adapted to human organisms (when observations were performed on mice, it was assumed that 0 Hour After Light Onset (HALO) = 8 h, local time, and that Humans, contrary to mice, are diurnal animals). In the simulations presented, care was not taken to optimise drug delivery according to a clinically fixed circadian schedule, since optimisation will be performed freely later, using numerical algorithms and with no a priori knowledge on optimal delivery time schedules. Only the parameters of circadian physiological control functions for the cell division cycle and for the metabolism of the considered drugs will be fixed based on such laboratory or clinical data. Note that in this physiologically based model, circadian rhythms are not mandatory and can be neglected by setting δ = ε = 0 if they are considered as irrelevant."
436,hal-01321536.training.tei.xml,14,"The pharmacodynamics of 5-FU is complex and multiply-targeted, as shown, e.g., in, resulting in non viable modifications of the RNA and of the DNA, but most of all in thymidylate synthase inhibition. We will consider here only its effects on its main target, thymidylate synthase, an enzyme that is essential in DNA duplication and S-phase specific. In the same way as direct damage to the DNA was considered as proportional to added cell death, we will assume here that thymidylate synthase degradation is proportional to 5-FU-induced cell death."
437,hal-01321536.training.tei.xml,15,"As in the previous section (2.1) for oxaliplatin, the PK-PD of the cytotoxic drug 5-fluorouracil (5-FU) is represented by a system of ODEs, controlled by a continuous infusion flow j(t), with the help of a continuous flow k(t) of folinic acid, a.k.a. leucovorin, a natural compound of the folate family, that stabilises the complex formed by FdUMP with its target thymidylate synthase (TS). Also for 5-FU have been experimentally evidenced circadian variations of the target TS and of the main enzyme responsible for drug catabolism, namely dihydropyrimydine dehydrogenase (DPD), such catabolism being processed mainly (80%) in the liver , which has been considered here as a filter on the general circulation. On entering into a cell, the active form of 5-FU, FdUMP, may be effluxed out of the cell by an ABC transporter , before reaching its target, a known mechanism of resistance to 5-FU ."
438,hal-01321536.training.tei.xml,16,"and l DPD (t) = l DPD BASE 1 + η cos 2π(t − ϕ DPD ) 24 , accounting for circadian tuning (if ζ =0, η = 0) of equilibrium values for tissue thymidylate synthase (TS) activity S 0 and liver dihydropyrimidine dehydrogenase (DPD) maximal rate l DPD. The variables represented are concentrations, either in blood, or in tissues, for: P: plasma 5-FU, with source term a continuous infusion flow j(t), detoxicated at 80% in the liver by DPD U: intracellular FdUMP (main intracellular active metabolite of 5-FU) Q: plasma leucovorin, also known as folinic acid, with source term a continuous infusion flow k(t) L: intracellular methylene tetrahydrofolate, active metabolite of leucovorin N: nuclear activation factor of the ABC transporter MRP8 (ABCC11), assumed to be sensitive to damage to the FdUMP target thymidilate synthase (TS)"
439,hal-01321536.training.tei.xml,17,"A: ABC transporter activity (MRP8, or ABCC11, for 5-FU) S: free thymidylate synthase (TS), with equilibrium value S 0 (including circadian modulation if ζ =0) B: reversibly FdUMP-bound TS (binary complex U∼T S) T : irreversibly FdUMP-bound TS (stable ternary complex U∼L∼T S) (B + T )/S: ratio of bound to unbound TS FIGURE 2. Behaviour of the variables (concentrations in molecules) under a joint infusion in the general circulation of 5-FU (P) and folinic acid (a. k. a. leucovorin, Q), mimicking a repeated sequence of 4 consecutive days of chemotherapy followed by 10 days of recovery. The respective intracellular metabolites, U for FdUMP and L for methylene tetrahydrofolate (MTHF), act synergistically, binding free thymidylate synthase, abbreviated in the sequel as TS (S), to form firstly a reversible binary complex FdUMP∼T S (B) and secondly an irreversible ternary complex FdUMP∼MT HF ∼T S (T ). The ABC transporter MRP8 activity (A) is stimulated by a nuclear factor (N), itself triggered by bound TS. The last track shows the ratio of bound-to-unbound TS, a measure of damages to TS. The parameters used in these simulations (see Section 4.4 for details) have been chosen to produce expected effects, and have not been experimentally identified."
440,hal-01321536.training.tei.xml,18,"The main features of this molecular PK-PD model are: active degradation of plasma 5-FU by dihydropyrimydine dehydrogenase (DPD) in the liver, considered as a filter placed in the general circulation; active cellular intake in the target tissue, together with active efflux by ABC transporter, of the active metabolite FdUMP of 5-FU; stimulation of ABC transporter activity by nuclear factor, sensor of damage to drug target TS; reversible binding of intracellular metabolite FdUMP to its target, forming a binary complex; irreversible binding of the same by fixation of methylene tetrahydrofolate to the binary complex, yielding a stable ternary complex. A variant of this model, where the nuclear factor inducing cell efflux was assumed to be directly triggered by the drug, which we think physiologically less likely than by its intracellular effects on its target TS, has been published in [52]. When circadian rhythms are considered relevant, peak phases ϕ DPD and ϕ T S for tuned values l DPD and S 0 are fixed according to laboratory or clinical published data, with adaptation from mice to Humans, as mentioned in the previous Section."
441,hal-01321536.training.tei.xml,19,A simulation of the model behaviour under square wave drug infusion flows of 5-FU and folinic acid following a circadian rhythm is shown on Fig. 2.
442,hal-01321536.training.tei.xml,20,"Due to the chemical forms, always prepared for intravenous infusions (oxaliplatin is prepared as an acid solution whereas 5-FU is usually presented in an alkaline form, hence a risk of precipitate in the infusion line), in which thetwo drugs are available in the clinic, it is not advisable to deliver them simultaneously. This has led us to add mutual exclusion terms for their infusion flows in the combined delivery to be optimised, as will be mentioned in Section 4. Rather than adding constraints on drug delivery schedules that are actually in use in the clic, we preferred, for computing convenience, to represent the risk of precipitate formation by an added artificial ""precipitate"" variable in the plasma, with supposed catastrophic consequences for toxicity to a healthy cell population:"
443,hal-01321536.training.tei.xml,21,"resulting in a term d prec .z(t) to be added to the irrecoverable death rates d i (t) and d κτi (t) in the cell population dynamic model (3.3) in Section 3.2, see below. This precipitation risk is systematically taken into account and avoided at the infusion line level in chronotherapy delivery schedules and in the FOLFOX protocol ."
444,hal-01321536.training.tei.xml,22,"As regards cytostatic drugs, that have been named so because they slow down tissue proliferation but, contrasting with cytotoxic drugs, do not-at least at low or medium doses-kill cells by damaging them and thus do not need trigger repair mechanisms, we can represent their action as firstly proposed in [11] by inhibiting cell cycle phase transition rates at G 1 /S and G 2 /M checkpoints. This representation is in particular adapted to cyclin-dependent kinase inhibitors (CDKIs) that control these checkpoints. It may also be used to represent the action of growth factor receptor antagonists, that may be thought of as decreasing the boundary terms at the beginning of each cell cycle phase, and also of anti-angiogenic drugs, that choke a solid tumour population, resulting in proliferating cell population models for the division cycle in decreased boundary terms at the beginning of phases G 1 and S. This point will be made more precise in Section 3. We have already mentioned that oxaliplatin, a certainly cytotoxic drug, can also be considered as having effects on phase transitions, as though it were additionally also a cytostatic drug. A way to represent control on the cell cycle by cytostatic drugs, for instance tyrosine kinase inhibitors (TKIs), with introduction of a quiescent cell compartment, has been proposed in [37], using proliferative and quiescent cell compartments. The equations are written as"
445,hal-01321536.training.tei.xml,23,"where p(t, x) is the density of proliferating cells of age x at time t, and Q(t) is the density of quiescent (i.e., non-proliferating, out of the division cycle) cells at time t, and the drug target here is f , rate of escape at mitosis towards the siding phase G 0 (quiescent cell compartment), this f to be enhanced by a cytostatic drug. The model was identified on the human Non Small Cell Lung Cancer (NSCLC) cell line PC-9 submitted to the TKI drug erlotinib."
446,hal-01321536.training.tei.xml,24,"In fact, the equation for the quiescent phase is just a linear equation and, rewriting the system for the proliferating phase as"
447,hal-01321536.training.tei.xml,25,"we can see that it is nothing but the classical McKendrick transport equation (, see also [11], recalled in Section (3.1)) for one proliferating phase with a balanced control between mitosis ((1 − f )β ) and enhanced ""cell disappearance"" ( f β + µ), i.e., way out of the proliferating phase towards either quiescence or death. In other cell cycle models with phases G 1 , S, G 2 and M , this representation of a target for cytostatic drugs should be placed between phases G 1 and S, resulting in a modification of the McKendrick equation :"
448,hal-01321536.training.tei.xml,26,with the adjunction of a quiescent phase G 0 represented by
449,hal-01321536.training.tei.xml,27,which is thus fed only by cells escaping from G 1 instead of processing into S-phase.
450,hal-01321536.training.tei.xml,28,"In the sequel, we will focus only on the action of cytotoxic drugs oxaliplatin and 5-fluorouracil; nevertheless it is also obviously possible to use this local representation of the dynamics of cytostatic drugs to study and optimise combinations of both types of anticancer drugs, for instance oxaliplatin, 5-fluorouracil and cetuximab, as it is now frequently the case in the clinic of metastatic colorectal cancer. 3. Cell cycle model with drug damage and repair phases 3.1. Age-structured models of the cell cycle in cell populations"
451,hal-01321536.training.tei.xml,29,"Various models of the cell division cycle at the single cell level exist, some of them with remarkable descriptions of their molecular mechanisms, allowing to study entrainment of the cell cycle by the circadian clock based on upto-date physiological knowledge, but they are not adapted to describe proliferation in cell populations, except by considering cellular automata, as in [1][2][3][4]. To the aim of describing proliferation in tissues, we rather advocate age-structured partial differential equations (PDEs), that take into account the main source of variability in such proliferating cell populations, i.e., age in the division cycle. We recall here the McKendrick  (or Von Foerster-McKendrick) model for the cell division cycle in proliferating cell populations , where n i (x,t) is the density of cells in cell cycle phase i of age x at time t:"
452,hal-01321536.training.tei.xml,30,"together with an initial condition (n i (t =0, .)) 1≤i≤I. It may be shown that for constant or time-periodic control on the d i and K i→i+1 , solutions to (3.1) satisfy n i (t, x) ∼ C 0 N i (t, x)e λt , where C 0 is a real positive number and N i are eigenvectors of the eigenvalue problem"
453,hal-01321536.training.tei.xml,31,"The importance of the first eigenvalue λ , which, provided that the system is growing, is positive and simple, and obviously governs (since the positive eigenvectors N i are bounded) the time-asymptotic behaviour of the cell populations n i (t, x) in each phase i, is thus fundamental, and it may be studied as a resulting observable, a function of the theoretical drug control conditions exerted onto the system. This has been done in ."
454,hal-01321536.training.tei.xml,32,"In this system of transport equations,one for each phase i of the cell division cycle, the control on phase transition rates K i→i+1 (t, x)(i = 1, 2) may be decomposed in κ i (x).ψ i (t), using an experimentally identified steplike function κ i of age x  and, in the circadian setting, a theoretical 24-hour periodic function ψ i of time t.The phase difference between ψ 1 and ψ 2 is set to a half-period (12 hours) because of the known phase opposition between the clock-controlled proteins Wee1 and p21 , which themselves control Cdk1 (that determines the G 2 /M transition) and Cdk2 (that determines the G 1 /S transition), respectively. Another (numerical) reason is that, as mentioned in [11], this phase difference has been found to maximise the growth coefficient, first eigenvalue of the periodically controlled McKendrick system; otherwise said, this value of a half-period for the phase difference between ψ 1 and ψ 2 yields the lowest population doubling time."
455,hal-01321536.training.tei.xml,33,"In the sequel, we propose a more elaborate model, starting from the previous ones, for the cell population model under insult by drugs, taking into account various modes of action of cytotoxic drugs on the cell cycle, and knowing that the parameters of this model are many and will take many efforts to be experimentally identified. Nevertheless, we present it as a possible proof of concept for therapeutic optimisation procedures, based on representations that are partly molecular (for intracellular drug effects) and partly phenomenological (for the cell cycle), but always physiologically based, after our knowledge of the mechanisms that govern progression in the cell division cycle."
456,hal-01321536.training.tei.xml,34,"The proposed model, a variation of the previous McKendrick-like models for the division cycle in cell populations mentioned above, runs as follows"
457,hal-01321536.training.tei.xml,35,and
458,hal-01321536.training.tei.xml,36,with
459,hal-01321536.training.tei.xml,37,"In this cell cycle model,5 cell subpopulations are represented by their density variables: n i (i = 1, 2, 3) for agestructured phases G 1 , S − G 2 and M, respectively, plus r i (i = 1, 2) for supplementary repair phases R 1 and R 2 (in which age is unchanged) corresponding to cells subject to DNA damage occurring in phases G 1 and S − G 2 respectively. The durations of phases G 1 , S − G 2 , R 1 and R 2 are not fixed (no upper bound in age, although in fact, they will never last infinitely long), whereas M-phase is assumed to have fixed duration (e.g., x M =1 hour) and to always (just by setting high the value of the constant M in the last equation, sending all cells to division at age x M in M-phase) lead in such fixed time each cell in mitosis into 2 intact daughter cells in G 1. We assume that cells in M-phase are endowed with a supercoiled DNA, which make them unreachable by DNA-targeted drug insults, and we have not considered here the effects of M-phase-specific drugs (vinca alkaloids or taxanes); it is certainly possible to represent them by targets at the M/G 1 transition (i.e., by blocking mitosis), but thus far, the M-phase is free from pharmacodynamic effects in this model."
460,hal-01321536.training.tei.xml,38,"As regards thetwo cytotoxic drugs at stake in this study, apart from this last point about M-phase, oxaliplatin is a non phase-specific cytotoxic drug, whereas 5-FU is S-phase specific. Functions L i (t) quantify the effect of oxaliplatin (and of 5-FU for S − G 2 ) on the cell subpopulation under attack in G 1 (i = 1) and S − G 2 (i = 2) that can be coped with by repair, whereas d κτi (t) functions stand for the cytotoxic effects of oxaliplatin that cannot be repaired (one can think of them as double-stranded breaks in the DNA ), be they occurring in G 1 , R 1 , S − G 2 or R 2. Note that we have not considered in our simulations (see Sections 3.2 and 4.4, where D 2 = 0) irrecoverable effects of 5-FU in our cell population model for the division cycle, assuming that its main target is apoptosis induced in S-phase, probably due to lack of thymine , but that these effects are amenable to repair. We do not consider in this model the intra-S checkpoint. Thus functions L i (t) and d κτi (t) together represent the ultimate pharmacodynamics of cytotoxic drugs on their actual targets: the cell division cycle."
461,hal-01321536.training.tei.xml,39,"In each repair phase R i , fed from either G 1 or S − G 2 by the corresponding ""leak"" function L i , damaged cells with density r i (t, x), keeping their age x unchanged, undergo a repair mechanism represented by a time constant ε −1 i. Those cells that have been successfully repaired move back to the corresponding normal cycle phase, G 1 or S − G 2 , and having become normal cells (n i ) again, are candidates to proceed to next phase at the corresponding Variables n i (t, x) are densities of cells at time t and age in phase x in subpopulations, i.e., cell cycle phases G 1 (i =1), S − G 2 (i = 2) and M (i = 3). Moreover, two additional subpopulations have been added, R 1 and R 2 , with age-independent variables r 1 (t) and r 2 (t), respectively, to describe the fate of those cells that have been hit by drug-induced DNA damage and are waiting to be repaired-or sent to apoptosis-before being able to join next phase at G 1 /S and G 2 /M checkpoints, respectively."
462,hal-01321536.training.tei.xml,40,"checkpoint, which is controlled by functions κ(x).ψ i (t) (multiplied or not by cytostatic drug pharmacodynamics (1 − g κσ i (t)), see below). At the same time, a more destructive cytotoxic death process represented by functions d κτi (t) goes on, and is enhanced by more extended DNA damage, which is represented by a nonlinear effect as a function of DNA damage for functions d κτi (t), that are tuned according to the cytotoxic drug at stake. We consider here that repair in phase R i consists only in a loss of time ε −1 i , that does not depend on the drug dose ; however, the drug dose does influence the instantaneous rate L i (t) of cells sent to repair. A simplifying assumption made is that repair is always performed ad integrum, i.e., we have not considered here the possibility to represent errors in repair that may contribute, together with inefficient control of DNA at cell cycle checkpoints, to the known genomic diversity and instability, let alone aneuploidy, of cancer cell populations. Moreover, the checkpoints in this model represent cell cycle arrest by cumulative effects of oxaliplatin seen as sending cells to the repair phase, this drug having been reported to act more by arrest at transitions than by induction of apoptosis."
463,hal-01321536.training.tei.xml,41,"Normal transition rates (in the absence of damage to the cell), K 1 (t, x) at G 1 /S and K 2 (t, x) at G 2 /M are described in a multiplicative way by κ i (x), the physiological transition rate in the absence of circadian control (determined by the distribution of cell cycle phase durations in the cell population, as shown in [11]), multiplied by ψ i (t), which represents circadian gating by p21 at G 1 /S and by Wee1 at G 2 /M. Thus physiologically, as already mentioned in Section (3.1),"
464,hal-01321536.training.tei.xml,42,"Additional cytostatic drugs, e.g. growth factor inhibitors, cyclin-dependent kinase inhibitors (CDKIs), or even antiangiogenic drugs, may be represented in this model for a cancer cell population by an inhibitory effect on G 1phase by choking boundary terms at cell cycle transitions M/G 1 or G 1 /S. Indeed, since these drugs are assumed to be non-cytotoxic, taking into account their effect should be done not on death terms, but only on the boundary terms, either n 1 (t, x =0) or n 2 (t, x = 0), i.e., in the latter case on κ 1 (x).ψ 1 (t) by a multiplication by (1 − g κσ 1 (t)), where g κσ 1 (t) stands for the pharmacodynamics of such cytostatic drugs (of maximum value 1). This may be achieved by the adjunction of a quiescent phase, as in equation (2.3), that has been studied in detail with parameter identification on cell cultures in [37], using in our case the suggested modifications (2.5, 2.6) of the system. Note that another possible way of representing the action of cytostatics is by slowing down a non constant velocity factor"
465,hal-01321536.training.tei.xml,43,"To perform numerical simulations, we discretised the PDE model (3.3). We adapted the discretisation scheme presented in [11], Section 3.3 by adding repair phases, transitions from the physiological phases towards these new phases and permanent transitions from these new phases towards the physiological phases. The number of cells in the discretised model is represented by n k, j i = n i (k∆t, j∆ x) and the dynamics of this discretized variable is given for the time step k by n k+1 = M k n k , where n k is the concatenation according to ages j and phases i of the n k, j i s. The matrix M k is a block matrix, where each block represents transitions from one phase to the other. The size of each block is related to the duration of the corresponding phase. For instance, we fixed the duration of mitosis toone hour, so that we do not need to consider ages larger than one hour in M-phase. We give in Figure 4 the position of the nonzero elements for a given matrix M k ."
466,hal-01321536.training.tei.xml,44,The coefficients of the PDE model depend on the drug infusion schemes. The coefficients of the transition matrices M k will also depend on them accordingly.
467,hal-01321536.training.tei.xml,45,"Focusing on cancer chemotherapy, we propose to minimise the growth rate of the cancer cell population while maintaining the growth rate of the healthy cell population above a given toxicity threshold Λ (we chose in the Jean Clairambault, Olivier Fercoq Combined anticancer drug delivery optimisation FIGURE 5. Gating at cell cycle phase transitions due to circadian clock control. Functions ψ H i (left, healthy) and ψ C i (right, cancerous) define the hours at which the cells of each population (healthy and cancerous) can change phase: when ψ(t) =0, transition gates are closed. The dashdotted lines correspond to the transition from G 1 to S − G 2 and the solid lines correspond to the transition from S − G 2 to M. In the model presented here, healthy and cancerous cells differ only by their responses to circadian clock control, represented by functions ψ H i and ψ C i ."
468,hal-01321536.training.tei.xml,46,"application presented below Λ =0.021). Infusions here should be thought of as referring to the drugs Leucovorin, 5-FluoroUracil (5-FU) and oxaliplatin (lOHP) as explained in Section 2."
469,hal-01321536.training.tei.xml,47,"We considertwo cell populations with their respective dynamics. We model both of them using the same agestructured cell population dynamics model (3.3) but with different parameters. In fact, we assume that the transition and death rates are the same when there is no circadian control, but that cancer cell populations show a looser response to physiological circadian control. More precisely, we model this phenomenon by different timedependencies for gating at transitions in (3.4). For healthy cells, we set K H i (t, x) = κ i (x).ψ H i (t) and for cancer cells, we set K C i (t, x) = κ i (x).ψ C i (t) where ψ H i (t) and ψ C i (t) are given, similarly to the ones in [11], by ψ H 1 (t) = cos 2 (2π(t − 10)/12)1l [7;13]"
470,hal-01321536.training.tei.xml,48,"8 cos 2 (2π(t − 10)/12)1l (t) + 5 8 cos 2 (2π(t − 10)/3)1l  (t) + ε, ψ C 2 (t) = 5 8 cos 2 (2π(t − 22)/12)1l "
471,hal-01321536.training.tei.xml,49,"recalling from [11] that ε is here a very small positive number (typically10 −10 ) put in the equations only to ensure ψ H i > 0 and ψ C i > 0, which may be shown as sufficient to imply irreducibility of the matrix M in numerical simulations (see below) and thus applicability of the Perron-Frobenius theory."
472,hal-01321536.training.tei.xml,50,"We assume that the drug has the same effect on both populations, i.e. L 1 (t), L 2 (t) and that death rates are the same. This modelling choice couples the behaviours of the populations through the drug infusions."
473,hal-01321536.training.tei.xml,51,Then we represent the growth rates of both populations by the Floquet eigenvalue of the corresponding model (λ C for cancer cells and λ H for healthy cells). We shall search for an optimal drug infusion schedule among the set of measurable and bounded (classical assumptions in control theoryfor instance) and T-periodic functions. We obtain the following Floquet eigenvalue optimisation problem with constraints:
474,hal-01321536.training.tei.xml,52,"g T-periodic, mesurable and bounded"
475,hal-01321536.training.tei.xml,53,"The McKendrick population dynamics yields a cell population described by the number of cells n i (t, x), where i is the phase, t is the time and x is the age in the phase. As seen in Section 3.1, if the death and transition rates d i and K i→i+1 are T-periodic, then the solutions n i (t, x) to (3.1) are asymptotically-i.e., for large time t-equivalent (in a L 1 sense that can be made very precise) to C 0 N i (t, x)e λt , where the eigenvector N i is T-periodic in the time variable."
476,hal-01321536.training.tei.xml,54,"Hence, searching for infusion strategies that optimise the Floquet eigenvalue of a biological system is a way to study its long term behaviour under drug infusion. Indeed, most of chemotherapy schedules are repeated until the patient responds to the treatment, and conversely stopped if the patient does not respond after a given number of chemotherapy courses. In this model, we take into account this periodic behaviour directly in the model and in our objective function."
477,hal-01321536.training.tei.xml,55,"Classically, chemotherapy schedules consist ofseveral days of severe treatment followed by a longer duration of rest (2 days of treatment every other week for FOLFOX therapy , 4 days of treatment every other week or even 5 days of treatment every third week for chronotherapy ). We chose, instead of imposing a rest period, to add a toxicity constraint that imposes in a dynamic way that the growth rate of the healthy cell population always remains high enough for the patient to live with a reasonable quality of life, even though he may experience drug-induced diminished physiological functions. In order to be compatible with the periodicity of physiological circadian control, the periodicity of the treatment should be a multiple of 24h. Numerical tests that we have performed on our model show that a period of one day seems to be a good choice, because other locally optimal solutions that we found assuming longer periods finally proved to be all 24h-periodic."
478,hal-01321536.training.tei.xml,56,"But since we have coupled the population dynamics with the PK-PD model described in Section 2, we have to take care that periodic infusion of drugs in the same way results (asymptotically) in periodic transition and death rates in the population dynamics model. In the case of linear systems, the Floquet-Lyapunov theory (see [62] for instance) gives the following condition: if the eigenvalues of the so called monodromy matrix (also called Floquet multipliers) all have modulus smaller than1, then the system is asymptotically stable, and a periodic input asymptotically results in a periodic output."
479,hal-01321536.training.tei.xml,57,"One can generalise this result to a T-periodic nonlinear system. For a given T-periodic control u let Φ be the map that to an initial state y 0 associates the state y T of the system at time T. If Φ is a contraction in the sense that there exists c <1 such that for all initial states y 0 and z 0 , Φ(y 0 ) − Φ(z 0 ) = y T − z T ≤ cy 0 − z 0 , then a periodic input asymptotically results in a periodic output. We numerically checked this fact during computations."
480,hal-01321536.training.tei.xml,58,"To obtain the numerical resolution of problem (4.1), we firstly discretise the PK-PD models (2.2) and (2.1) and the population dynamics model (3.3). This is the so-called direct method, which consists of a total discretisation of the control problem and then of solving the finite dimensional optimisation problem obtained."
481,hal-01321536.training.tei.xml,59,"In our setting, at each time, the control g l (t) is the flow of drug l (1 ≤ l ≤ n drugs ) infused at time t. Here, n drugs =3 for 5-FluoroUracil, leucovorin and oxaliplatin. The discretised control (g k l ) l,k will be the array of the infusion time step by time step and drug by drug. As we search for 24h-periodic controls with one drug, we only need to define g on one day, i.e. g ∈ R n drugs ×N T , where N T = T /∆t is the number of time steps."
482,hal-01321536.training.tei.xml,60,"We need to bound the flow of each drug to avoid evident toxicity. We bounded the flow of 5-FU and leucovorin to ¯ g 1 = ¯ g 2 =500 g/m 2 /h mg/m 2 /h and the flow of oxaliplatin to ¯ g 3 = 50 mg/m 2 /h. Hence,"
483,hal-01321536.training.tei.xml,61,"Given a discretised infusion strategy g, we build the matrices M C (g) and M H (g), that are the nonnegative matrices modelling the discretised dynamics of each cell population under drug infusion g when drug concentrations represented by the PK-PD models have reached their day-periodic asymptotic behaviour."
484,hal-01321536.training.tei.xml,62,"Thus, we study the optimisation of the growth rate in the discretised model, represented by the principal eigenvalue ρ of the nonnegative matrices M C for cancer cells and M H for healthy cells where M H = M N T H. .. M 1 H is given in Section 3.3 (see also [11], Section 3.3) and M C is defined accordingly."
485,hal-01321536.training.tei.xml,63,Combined anticancer drug delivery optimisation
486,hal-01321536.training.tei.xml,64,"We chose a discretisation step of10 minutes because it may be considered as a lower limit to the half-life time of 5-FU and oxaliplatin in the plasma , which is most likely even lower than the half-time of its downstream molecular effects at the cell level, our concern here. The oldest ages represented in the discretisation scheme are 10 days for each phase except for mitosis (M-phase) where we choose 2h."
487,hal-01321536.training.tei.xml,65,"By the Perron-Frobenius theorem, we know that if M is nonnegative and irreducible, its principal eigenvalue ρ(M) is positive and is a simple eigenvalue. Moreover, the principal eigenvector is unique up to normalisation and can be chosen such that u(M) ≥ 0. In our setting (irreducibility of the matrix and the Perron-Frobenius theorem), one can naturally define a function ρ from the set of nonnegative and irreducible real matrices in R n into R + , that to a matrix associates its principal eigenvalue."
488,hal-01321536.training.tei.xml,66,"General eigenvalue optimisation of non symmetric matrices is a difficult (non convex, non differentiable) problem: see [54] and [65] fortwo algorithms dealing with this problem. However, for nonnegative irreducible matrices, as the principal eigenvalue is simple, this implies that ρ is differentiable. Indeed, denoting by v and u the left and right eigenvectors of a matrix M associated with a simple eigenvalue ρ, the derivative of ρ at M can be written as :"
489,hal-01321536.training.tei.xml,67,"Thus, as the objective function is differentiable, differentiable optimisation theory applies. We then get the complete gradient of the objective by the chain rule. We have a composite objective of the type"
490,hal-01321536.training.tei.xml,68,"where ρ is the principal eigenvalue, M H is the discretised population dynamics, L is the output of the discretised PK-PD and g is the discrete periodic control. To compute the gradient of the objective function, we thus compute the derivatives of all these functions, either in closed form (for log and M H ) or by an iterative algorithm (for ρ and L(g)) and we multiply them all up. Following [32], we set the stopping criterion of internal iterative algorithms according to the theory of consistent approximation."
491,hal-01321536.training.tei.xml,69,"To solve the non convex problem (4.2), we use the method of multipliers, which solves a sequence of non constrained optimisation problems whose solutions converge to a local optimum of the constrained problem (4.2)."
492,hal-01321536.training.tei.xml,70,"We coded the discretised versions of the PK-PD models (Section 2) and of the population dynamics model (Section 3.2), and the method of multipliers in Scilab ."
493,hal-01321536.training.tei.xml,71,We chose the following constants in our model. Recall that most of them were not experimentally identified.
494,hal-01321536.training.tei.xml,72,"and ψ 2 (t) are given in Subsection 4.1 for healthy and cancerous cell populations. For κ 1 (x) and κ 2 (x), we took the functions identified in [11], i.e."
495,hal-01321536.training.tei.xml,73,where ϕ i is the probability density function of a Gamma law given by
496,hal-01321536.training.tei.xml,74,"Γ is the gamma function, α 1 =8.28, β 1 = 1.052h −1 , γ 1 = 0h, α 2 = 3.42, β 2 = 1.47h −1 , γ 2 = 6.75h (we substracted the fixed duration of 1 hour for the mitosis phase M from the duration of S − G 2 − M identified in [11], thus obtaining for ϕ 2 an estimate of the probability density function of the duration of S − G2, so that the index i = 2 actually stands for the G 2 /M transition). At the end of the optimisation algorithm, we obtained a locally optimal solution for the discretised optimisation problem (4.2) and we reported it in Figure 6."
497,hal-01321536.training.tei.xml,75,"It consists of infusing thethree drugs in the following order: leucovorin between 10 h 30 and 21 h 20 (stopping the infusion at 19 h in oder to avoid mixing medicines has a marginal impact on the performances), 5-FU between 19 and 20 h and finally oxaliplatin between 20 and 22 h at maximal flow. Then, these infusions are repeated every day until the situation justifies to stop the treatment."
498,hal-01321536.training.tei.xml,76,"Without drug infusion, the growth rate of cancer cells (0.0265h −1 ) is larger that the one of healthy cells (0.0234h −1 ), this difference being due to different responses to circadian clock control. This gives an evolution of the respective populations, cancer cells becoming more and more present: see Figure 7. By following the infusion strategy numerically determined by the optimisation algorithm, we obtained that the asymptotic growth rate in the healthy cell population was actually above the chosen toxicity threshold (λ healthy ≥ Λ = h −1 ) and that the asymptotic growth rate of cancer cells was weakened to (0.0229h −1 ). Thus by this optimal infusion strategy, the asymptotic growth rate for cancer cells was reduced by 13.3 % while the growth rate for healthy cells was reduced by only 10.0 %, so that the toxicity constraint is satisfied. This gave us a description of the evolution of the respective populations, which is illustrated on Figure 8."
499,hal-01321536.training.tei.xml,77,"These infusions cause possibly recoverable damage to the cells (be they healthy or cancerous), sending them to repair, at a rate L 1 (t) in G 1-phase and at a rate L 2 (t) in phase S − G 2 (see Figure 9)."
500,hal-01321536.training.tei.xml,78,"If we compare L 2 (t) (Figure 9) and the fraction of the cell populations in phase S − G 2 ( Figure 10), we can see that the best time to cause damage in phase S − G 2 is when the proportion of cancer cells is significantly larger than the proportion of healthy cells in this phase, i.e., between22 and 24 h. The strategy takes into account the delay between the infusion and the effective action of the drug at the cell level due to the pharmacokinetics and the pharmacodynamics (diffusion, metabolism and elimination) of the drugs. We may also remark that although the infusion strategy stops infusions for 13 hours per day, the drugs remain active all day long."
501,hal-01321536.training.tei.xml,79,"In our cell population dynamics model, the fraction of cancer cells in G 1-phase is never significantly higher than the fraction of healthy cells in G 1-phase, so that we cannot tune a significant difference between thetwo populations in G 1 , and L 1 (t) should remain small to limit the toxicity of the treatment. Hence, 5-FU, that causes little damage to cells in G 1-phase, is the main drug in this infusion strategy. . Daily mean growth rates for cancer (solid line) and healthy cells (dashed line) when starting drug infusions at time 0. After a 10-day transitional phase, the biological system stabilises towards the expected asymptotic growth rate."
502,hal-01321536.training.tei.xml,80,"Simulating the transition from the stationary state without drug to the stationary state with periodic drug infusion (Figure 11), we see that after a transition of around10 days, the treatment performs as expected (λ cancer = 0.0229 and λ healthy = 0.021). As mentioned above, by this optimal infusion strategy, the asymptotic growth rate for cancer cells was reduced by 13.3 %, whereas the asymptotic growth rate for healthy cells was reduced by only 10.0 %, and the toxicity constraint λ healthy ≥ Λ = 0.021 was satisfied. Note also that the growth rate in the healthy cell population always remains positive, which means that the healthy cell population production never decreases. This does not mean that there is no negative drug effect on this healthy cell population: one should have in mind that we have represented here only the rate of production of healthy cells, e.g., in the gut or in the bone marrow, while for obvious homeostatic reasons, constant consumption (elimination in the intestinal lumen, or release in the general blood circulation) must be considered and should result here, together with this decrease of production, in a decay in physiologically functional healthy cell population (in our example, mature villi enterocytes or mature blood cells). However, since the growth rate in the healthy cell population never becomes negative, the source of healthy cell production is preserved by the optimisation procedure."
503,hal-01321536.training.tei.xml,81,The proposed infusion strategy combines thethree drugs. This means that we obtain better results in terms of the trade-off between damage to cancer cells and toxicity to healthy cells by infusing smaller doses of each drug than a larger dose of one single dose. This confirms (once more) that the use of this combination is efficient for the fight against cancer. We shall remark however that the proposed strategy is different from the ones used today in the clinic.
504,hal-01321536.training.tei.xml,82,"Firstly, the optimisation algorithm yields a daily schedule consisting of infusions during11 hours and rest periods of 13 hours whereas one usually infuses larger doses and then gives longer rest periods to the patient. As discussed in Subsection 4.2, the schedules that seem to take the best advantage from the influence of circadian clocks on the proliferation of cells have a period of one day. However in this work, we did not study the emergence of drug resistances among the cancer cell population, although resistance is one of the main causes of the failure of chemotherapies in the long term. One may wonder here whether resistances are more or less likely to appear when performing short, rather than long, rest periods."
505,hal-01321536.training.tei.xml,83,"Secondly, the locally optimal strategy returned by the optimisation algorithm suggests infusing oxaliplatin after 5-FU. In the FOLFOX schedule for instance, it is advised to infuse oxaliplatin before 5-FU. However, in [35], the authors show that infusing oxaliplatin before, during or after the exposure to 5-FU does not produce significant Jean Clairambault, Olivier Fercoq Combined anticancer drug delivery optimisation differences. The results of the optimisation procedure performed on our preliminary model (in which parameters have not been experimentally identified) suggest that infusing oxaliplatin after 5-FU may have some advantages."
506,hal-01321536.training.tei.xml,84,"We have presented in this study what is to our best knowledge the first physiologically based model of action of a combination of cytotoxic drugs, commonly used in the clinic of colorectal cancer, on the division cycle-with consideration of thetwo main phase transition checkpoints G 1 /S and G 2 /M-in populations of cells, together with the use of numerical optimisation algorithms to meet the question of maximising tumour cell kill under the constraint of preserving healthy cell population from unwanted toxic side effects."
507,hal-01321536.training.tei.xml,85,"The molecular PK-PD models are based on physiological knowledge, can evolve with new findings in this area and their parameters-that are thus far only theoretical, tuned to produce observable effects on their molecular targets-should be identified, at least partly, by biological experiments led in collaboration with teams of pharmacologists. Obviously enough, these models are quite complex, and some of their features (e.g., ABC transporters for 5-FU) may be forsaken if they are not proven to be important, in order to simplify experimental identification of model parameters. So far, these PK-PD models should be only considered as providing biological bases for a proof of concept of therapeutic control optimisation on the cell population dynamic model for proliferation."
508,hal-01321536.training.tei.xml,86,"This cell population dynamic model is a new version of a system of McKendrick-like equations, the characteristics of which must be adapted to represent either cancer or healthy cell populations. In this setting, we have chosen, as we had done previously, to use the chronotherapeutic paradigm, in which healthy and cancer cell populations differ only by good or bad circadian clock control on cycle phase transitions. This should indicate new tracks to optimise cancer chronotherapeutic schedules, when the classic drugs oxaliplatin and 5-FU are combined with other drugs, in particular with irinotecan and cetuximab ."
509,hal-01321536.training.tei.xml,87,"Also of note, since the model relies on physiological principles, it is possible, when chronotherapeutics has not proved useful (which may be due to persistence of good circadian clock control in tumours, or to bad and unrecoverable control by the central circadian pacemaker of the suprachiasmatic nuclei on the whole organism, impinging also healthy cell populations) to use other pathological differences in cancer cell populations, compared with healthy tissues. For instance, adding to this model a physiological representation of the control by the p53 protein of phase transitions, DNA repair or apoptosis launching, based on triggering of the ATM protein by DNA damage and subsequent launching of p53 oscillations (as presented in) should prove useful. Indeed, it is known that p53 is mutated and inefficient in about 50% of solid tumours , and thus instead of circadian control, it is possible to use impaired-or not-p53 control to account for differences between the two cell populations."
510,hal-01321536.training.tei.xml,88,"Another expected benefit of the adjunction of a DNA damage-ATM-p53 model should be to assess the respective roles in cell death and in cell cycle phase transition blockade of the drug oxaliplatin, assumed to act by enhancing cell death but also, and primarily, according to [76,77], by blocking G 1 /S and G 2 /M phase transitions."
511,hal-01321536.training.tei.xml,89,"Obviously also, to theoretically investigate optimisation of therapeutic procedures using new drugs that are known by their action on physiological targets to hit cancer cell, and not, or less, healthy cell populations, is another possible application of this way of representing pharmacodynamic effects of anticancer drugs. It requires a PK-PD model for each drug, but the target cell population dynamic model of proliferation can be based on the same principles, and the optimisation procedure can be strictly the same. Note also that we have chosen to optimise eigenvalues, since the population model is linear. If a linear model proves inadequate, it is also possible to dynamically optimise therapeutics using cell population numbers, as done in [5,15], using the same optimisation principles, but not Perron-Frobenius-Floquet theory."
512,hal-01321536.training.tei.xml,90,"Finally, one must note that in the present representation, healthy and cancer cell populations are considered as evolving separately, without any population coupling effect between them, except by drugs. The question of evolution toward drug resistance, in particular, and optimisation of therapeutics to circumvent it (see a review in [18]), has led to rather different models of cell populations evolving according to a resistance phenotype, in the framework of cell Darwinism, advocated, e.g., in [38,39] and recently studied from a deterministic point of view by integro-differential equations in [14,,5758,71]. In these models, coupling between cell populations is present, representing competition between cells for space and nutrients by a nonlocal logistic term (in a LotkaVolterra-like setting) in the global-cancer and healthy-cell population. These models are amenable to describe evolution towards drug resistance of a phenotype representing between-cell variability in the cancer cell population. Whether it is possible and relevant to address in the same model such phenotype evolution towards drug resistance and circadian modulation on proliferation-two phenomena that do not occur a the same time scale, and not necessarily independently, as the same disorganising mechanisms, present in cancer, that trigger heterogeneity and"
513,1001.4731.training.tei.xml,0,"We present WMAPseven-year observations of bright sources which are often used as calibrators at microwave frequencies. Ten objects are studied in five frequency bands (23-94GHz): the outer planets (Mars, Jupiter, Saturn, Uranus and Neptune) and five fixed celestial sources (Cas A, Tau A, Cyg A, 3C274 and 3C58). The seven-year analysis of Jupiter provides temperatures which are within 1σ of the previously published WMAP five-year values, with slightly tighter constraints on variability with orbital phase (0.2% ± 0.4%), and limits (but no detections) on linear polarization. Observed temperatures for both Mars and Saturn vary significantly with viewing geometry. Scaling factors are provided which, when multiplied by the Wright Mars thermal model predictions at 350µm, reproduce WMAP seasonally averaged observations of Mars within ∼ 2%. An empirical model is described which fits brightness variations of Saturn due to geometrical effects and can be used to predict the WMAP observations to within 3%. Seven-year mean temperatures for Uranus and Neptune are also tabulated. Uncertainties in Uranus temperatures are 3%-4% in the 41, 61 and 94 GHz bands; the smallest uncertainty for Neptune is 8% for the 94 GHz band. Intriguingly, the spectrum of Uranus appears to show a dip at ∼ 30 GHz of unidentified origin, although the feature is not of high statistical significance. Flux densities for the five selected fixed celestial sources are derived from the seven-year WMAP sky maps, and are tabulated for Stokes I, Q and U, along with polarization fraction and position angle. Fractional uncertainties for the Stokes I fluxes are typically 1 % to 3%. Source variability over the seven-year baseline is also estimated. Significant secular decrease is seen for Cas A and Tau A: our results are consistent with a frequency independent decrease of about 0.53% per year for Cas A and 0.22% per year for Tau A. We present WMAP polarization data with uncertainties of a few percent for Tau A. Where appropriate, WMAP results are compared against previous findings in the literature. With an absolute calibration uncertainty of 0.2%, WMAP data are a valuable asset for calibration work."
514,1001.4731.training.tei.xml,1,"The primary goal of the Wilkinson Microwave Anisotropy Probe (WMAP) mission (Bennett et al. 2003b) is to study the cosmic microwave background (CMB) anisotropy, an undertaking that requires detailed knowledge of the instrumental angular response and noise, low systematic errors, and absolute calibration at sub-percent levels. These attributes make WMAP data of natural interest for ancillary studies of sky-based calibration objects. This paper focuses on WMAP observations of selected planets and fixed celestial objects which have been commonly used as calibrators in the microwave, either by ground-or space-based instruments. We provide fundamental measurements for these objects which will be of use not only for calibration purposes, but also other studies, such as planetary radiative transfer. A brief discussion of WMAP planetary radiometric data was made by Hill et al. (2009), with an earlier discussion of Jupiter temperatures by Page et al. (2003b). We extend these presentations to include WMAP observations of Mars, Jupiter, Saturn, Uranus and Neptune over aseven-year interval (2001 - 2008), in five microwave passbands ranging from ∼23 to 94 GHz. We also select five bright celestial sources for more detailed study: Cas A, Cyg A, Tau A, 3C58 and 3C274. The WMAP project routinely compiles and distributes a point source catalog, which tabulates mean flux values in those same five passbands for bright sources off the Galactic plane (Wright et al. 2009; Hinshaw et al. 2007; Bennett et al. 2003a). However, four of the five objects discussed here (Cas A, Tau A, Cyg A, 3C 58) have low Galactic latitudes and are not included in the catalog. An earlier analysis of threeyear WMAP Tau A observations by Page et al. (2007) included both intensity and linear polarization measurements. This paper is one of a suite that details the analysis of WMAP seven-year data. An overview and description of data processing methods is provided by Jarosik et al. (2010). Gold et al. (2010 discuss Galactic foregrounds and techniques for removing them; an updated point source catalog is included. Analysis of CMB power spectra, best fit models and cosmological interpretations are presented by Larson et al. (2010) and Komatsu et al. (2010). Bennett et al. (2010 assess potential CMB "" anomalies "" relative to the best-fit six parameter ΛCDM model."
515,1001.4731.training.tei.xml,2,"We organize this paper as follows: a short discussion of the WMAP instrument, plus data strengths and limitations for calibration source work, is provided in Section 2. The paper then divides intotwo major sections: analysis and results for planets (Section 3), and then Section 4 discusses analysis methods and results for the five bright celestial sources. A closing summary and conclusions follow in Section 5."
516,1001.4731.training.tei.xml,3,"Since its arrival at the Earth-Sun L2 point in2001 August, WMAP has produced a well-calibrated, multi-wavelength microwave survey of the entire sky with high sampling redundancy. Detailed descriptions of the mission profile and hardware can be found in Bennett et al. (2003b), Jarosik et al. (2003a) and Page et al. (2003b . To assist with clarifying the terminology and analysis methods presented in this paper, we describe the key features of the mission. WMAP observes the sky differentially: signal from the sky is simultaneously sampled in two different directions ∼ 140 ° apart and guided by 10 feed horn pairs into 10 receivers, referred to as differencing assemblies (DAs). The receivers cover five frequency bands (K, Ka, Q, V and W) with approximate center frequencies of ∼23, 33, 41, 61 and 94 GHz, respectively. There is  one DA each for K and Ka bands (K1, Ka1),  two DAs each for Q and V bands (Q1, Q2, V1, V2) and  four DAs (W1, W2, W3, W4) for W band. The frequency passbands are broad (∼ 20% fractional bandwidth), so that the exact frequency associated with an observation depends on the source spectrum. An orthomode transducer at the throat of each feed separates the sky signal into  two orthogonal polarizations, allowing for reconstruction of both intensity and linear polarization signals. Angular resolution is moderate, ranging from roughly 0.2 ° FWHM in W band to ∼ 0.9 ° FWHM in K band."
517,1001.4731.training.tei.xml,4,"As a survey mission, WMAP is not commanded to point at specific targets, but executes a pole-to-pole sky-scanning strategy whose position on the ecliptic advances ∼1 ° eachday. To facilitate high redundancy in sky coverage, the spacecraft rotates about a spin axis, which also precesses. These combined motions produce a ∼ 45 ° wide scan swath which looks like a "" belt "" projected onto the sky, passing through the ecliptic poles. The scan swath returns to its original starting position on the ecliptic every year, after completing a full-sky survey with multiple observations per sky pixel. A primary strength of WMAP data is the calibration accuracy, with an absolute calibration error of 0.2% (Jarosik et al. 2010). The absolute calibration is tied to knowledge of the CMB monopole and the spacecraft orbital velocity, and thus is independent of any reliance on previously established measurements of the sources under study. When coupled with WMAP's temporal and spectral coverage, the accurate flux measurements can provide a useful measure of intrinsic or geometrically induced source variability."
518,1001.4731.training.tei.xml,5,"There are some limiting factors, however. Temporal coverage of an object is constrained by the sky-scanning method. For a fixed source on the ecliptic, the WMAP scan strategy provides fortwo ∼ 45-day windows a year in which to observe that source. These "" observing seasons "" are spaced roughly six months apart. Since WMAP does not dwell on an object, but simply scans as it passes through the viewing swath, the number of observations for an individual object is also limited, which affects the signal-to-noise. Finally, WMAP's moderate spatial resolution, which allows us to characterize the planets as calibrators, also places detailed knowledge of structures such as Saturn's rings beyond instrument capabilities."
519,1001.4731.training.tei.xml,6,"There arefive planets with sufficient signal-to-noise for analysis. Listed in order of decreasing signal-to-noise, these are Jupiter, Saturn, Mars, Uranus and Neptune. Since WMAP points away from the Sun, Earth and Moon in its L2 orbit, it cannot observe Mercury or Venus."
520,1001.4731.training.tei.xml,7,"Planets are excluded from the standard map-based products used for WMAP studies of the CMB and Galactic foregrounds. Observations of the planets must be extracted from the calibrated time-ordered data (TOD), and then undergo quality assessment and background removal before they can be analyzed. The extraction process gathers all observations from the TOD taken withinseveral degrees of each planet and associates ancillary information such as position in the focal plane, attitude, planet viewing aspect and data quality flags with each observation. Observations whose sky positions lie within a 7 ° radius of other planets or which are of suspect quality are immediately rejected. In addition, observations are excluded from use if either beam of a DA feed-horn pair samples sky coordinates that fall within the boundaries of a spatial processing mask (Jarosik et al. 2007). The purpose of this "" Galactic masking "" is to mitigate background removal error by excluding observations located near strong-signal, high-gradient regions of the Galaxy. For those observations which survive these quality control steps, Galactic and CMB background signals are subtracted. The background signal subtracted from each observation is computed from the DA-appropriate seven-year mean map (from which planetary data were excluded), using the value from the ∼ 6 ′ wide pixel which is closest in spatial coordinates to those of the time-ordered observation.Table 1 lists the beginning and ending dates of the observing seasons for the five planets, i.e., when they are directly in the WMAP viewing swath. The "" sky proximity "" column in the Table indicate probable reasons for an unusually high rejection of observations within a season."
521,1001.4731.training.tei.xml,8,"As noted in Page et al. (2003b), planets are "" point sources "" for WMAP and thus the brightness temperature of a planet T p can be computed via the relation"
522,1001.4731.training.tei.xml,9,"where T m is the peak response observed by WMAP, Ω B is the main-beam solid angle, and Ω p is the solid angle subtended by the planet. Changing distance between the planet and WMAP, plus changes in viewing aspect of an oblate planet, will cause Ω p (and also T m ) to vary with time. We account for both geometrical effects and compute the brightness temperature relative to a fiducial solid angle Ω ref p via the use of time-dependent geometrical scaling factors f d and f A . The factors f d and f A may be thought of as "" distance correction "" and "" disk oblateness correction "" factors:"
523,1001.4731.training.tei.xml,10,"where A ref and d ref are a fixed fiducial disk area and distance, respectively. A proj disk is the projected area of the planetary disk, and d p the distance of the planet from WMAP, at the time of each observation. The projected area of the oblate disk is computed from A proj disk = πR proj pole R eq ,"
524,1001.4731.training.tei.xml,11,where R eq is the planet's equatorial radius and R proj pole is the projected polar radius given by
525,1001.4731.training.tei.xml,12,"where D W is the planetary latitude of WMAP ( "" sub-WMAP latitude "" ). The fiducial disk area A ref is simply A proj disk evaluated at D W of0 ° . Planetary radii and north pole directions used to compute D W are adopted from Seidelmann et al. (2007); the polar and equatorial radii are listed inTable 2. Because WMAP is at L2, D W is not very dissimilar to that of the sub-Earth latitude, D e . As the brightest "" point source "" visible to WMAP, Jupiter is used to characterize the main beam response of the instrument. This process and analysis are described in detail in Hill et al. (2009), Jarosik et al. (2010), and Page et al. (2003b. The end result is a set of seven-year, azimuthally-symmetrized, one-dimensional radial beam profiles per DA. These radial beam profiles are then used to derive window functions used for CMB power spectrum deconvolution. Unlike the rest of the planetary temperatures in this section, the seven-year mean Jupiter temperatures are derived from the l = 0 value of the unnormalized window functions (which measures T m Ω B ), coupled with the fiducial solid angle for Jupiter (Hill et al. 2009). For all other planetary analysis, brightness temperatures are derived using a template-fitting technique, which takes advantage of the high-quality beam information obtained from sevenyears of Jupiter observations. After the processing steps described above, radial response profiles are produced for each planet, per DA, per season, by assigning observations to 0.25 arcmin bins increasing in angular distance from the planet's center. Each bin is assigned a mean value and an error based on instrument noise appropriate to the number of observations within the bin. The known radial beam response, based on seven-year Jupiter data, is used as a fitting template to the seasonal radial profile for each DA. The fit returns T m for the planet. The brightness temperature is then derived from T m , the main-beam solid angle (Jarosik et al. 2010) and the reference solid angle for the planet as per the above equations. Errors include the formal error in the template coefficient and the propagated error in the beam solid angle.Figure 1 illustrates sample W1 radial profiles for a single observing season for each of the planets. The beam template is overplotted in red. As well as illustrating the technique, the figure is also a useful comparator of signal quality for each planet at W-band. Beam dilution increases with decreasing WMAP frequency: Ω B is roughly 24.6, 14.4, 8.98, 4.20 and 2.10×10 −5 sr for the K -W bands respectively (Jarosik et al. 2010). Tabulated brightness temperatures are appropriate for the Rayleigh-Jeans (RJ) approximation . Brightness temperatures provided in the Tables do not include the CMB contribution blocked by the planet but included in the background. As noted in the Tables, absolute brightness temperature is obtained by adding 2.2, 2.0, 1.9, 1.5 and 1.1 K in bands K, Ka, Q, V and W respectively (Page et al. 2003a ), since in the RJ approximation the CMB temperatures are frequency dependent. WMAP data points shown in comparison to measurements in the literature are converted to absolute brightness."
526,1001.4731.training.tei.xml,13,"Jupiter is a bright, accessible source whose whole-disk observations have long been used for calibration in the infrared through radio. Hill et al. (2009) recommend the use of WMAP radiometry of Jupiter as the preferred method of transferring the WMAP dipole calibration to another microwave instrument. For some instruments, Jupiter is unfortunately too bright, hence the interest in the use of fainter sources, and a set of observations which tie several common sources together on the same calibration scale. We continue to recommend Jupiter as the primary comparison calibrator, however."
527,1001.4731.training.tei.xml,14,"Seven-year mean brightness temperatures for Jupiter, computed as described in Section 3.1, are presented inTable 3. The seven-year values differ from those of the five-year analysis of Hill et al. (2009) by 1σ or less. The largest changes, which hover near 1σ, are for K band and some of the W band DAs. A detailed explanation of these changes is given in Jarosik et al. (2010). Briefly, the change in K band results from improved background subtraction, and the W band changes are driven by small revisions to the beam solid angles , based on modeling. The seven-year temperatures for W band show a decreased scatter between the individual DAs compared to five-year, implying an improved calibration. The Jupiter temperature uncertainties listed in Table 3 are derived from the quadrature sum of the solid angle error and absolute calibration error of 0.2% (Jarosik et al. 2010), as they were in the five-year analysis."
528,1001.4731.training.tei.xml,15,"Thermal emission from Jupiter's atmosphere is the primary signal source at wavelengths spanned by WMAP. As such, the observations are helpful for providing constraints on atmospheric models for Jupiter, which in turn could be used for calibration purposes. However, as noted by e.g., Gibson et al. (2005) , there is a fine point to consider: small contributions to the total observed intensity from a synchrotron component up to about40 GHz are expected. This non-thermal "" decimetric emission "" arises from charged particles trapped close to Jupiter by its strong magnetic field, and dominates the spectrum at frequencies less than about 3 GHz. Estimates in the literature place the synchrotron contribution at 23 GHz near 1% of the total intensity (de Pater & Dunn 2003; Gibson et al. 2005), with a roughly T ∼ ν −2.4 frequency dependence (). However, the synchrotron emission is known to be variable on a wide range of time scales (Santos-Costa et al. 2009; de Pater & Dunn 2003; de Pater et al. 2003; Miyoshi et al. 2000), and accurate removal is difficult. The WMAP seven-year mean Jupiter brightness temperatures are graphically compared against those in the literature in Figure 2. Ground-based observational values are taken from compilations of Joiner & Steffes (1991) and , along with individual observations from de Pater et al. (1982 Pater et al. ( , 2001), Greve et al. (1994) and Goldin et al. (1997). Estimates of the synchrotron emission have been removed from the observations in the figure by many, but not all, of the authors, and not necessarily to the same level. The WMAP observations are plotted "" as observed "" . As an estimate of the stability of Jupiter's emission with orbital phase over the seven year interval, we extend the five-year analysis of Hill et al. (2009), in which temperatures for individual Jupiter observing seasons are computed and compared to the multi-year mean values. Individual seasonal temperatures are derived using the same beam-profile fitting technique which is used for the other planets (Section 3.1). We compute ∆T /T as the mean deviation of all DAs from their seven-year mean values, and include a 1σ standard deviation as a measure of coherency. These results are listed inTable 4. Using the WMAP five-year data, Hill et al. (2009) quote an upper limit of 0.3% ± 0.5% based on the largest deviation and scatter seen in the first 10 observing seasons, which occurred in season four. A careful examination of the entries in Table 4 with the largest deviations and/or scatter (observing seasons 4, 11, 12) shows that these particular instances have more to do with template fitting and background subtraction quality than with intrinsic source variability. The two seasons with high scatter (4, 12) are those in which the number of available observations for fitting has been significantly reduced because of foreground masking ( "" % masked "" in theTable), which increases the statistical error in the temperature determination. Season 11, with the highest ∆T /T , is one in which Jupiter is nearly aligned with the Galactic center, but outside the processing mask. This is a region in which background subtraction is subject to greater error because of higher temperature gradients across the pixels in the map used for background removal. If one ignores these three entries, then an upper limit of 0.2% ± 0.4% is more appropriate. We also searched for indications that K band might be more intrinsically variable than W band, because of the small synchrotron contribution. There is no indication of such a frequency dependence in the data, but the upper envelope here is about 0.5%. The WMAP Jupiter observations were analyzed to determine if any linear polarization signal could be detected. A special version of the beam mapping software was run for Jupiter, in which the angle between the polarization axis of each radiometer and the planet's magnetic dipole was computed for each observation. For each differencing assembly, all observations within a tight radius of beam center were accumulated, corrected to peak beam response, background subtracted, loss imbalance corrected, separated by radiometer, and then used to solve for Stokes parameters I, Q, U and their errors. For a more complete description of the relevant equations, see Hinshaw et al. (2003) and Jarosik et al. (2007). Normalized Stokes parameters Q/I and U/I are reported inTable 5, along with their propagated errors and estimated linear polarization fraction, p lin = (Q/I) 2 + (U/I) 2 . No attempt has been made to correct for noise bias in either p lin or its reported error, which is simply the propagated error. The estimated signal-to-noise is low, with no significant detections in any differencing assembly. Linear polarization of the synchrotron emission at 15 and 22 GHz has been detected by de Pater & Dunn (2003) using the Very Large Array (VLA). They were unable to obtain a synchrotron polarization fraction at 22 GHz, but estimated 20% polarization based on the 15 GHz observations, which implies roughly a 0.2% polarization fraction at K band using a combined thermal and non-thermal intensity. The WMAP results are consistent with this estimate, but do not provide any further constraint."
529,1001.4731.training.tei.xml,16,"Mars' value as a calibrator depends upon the accuracy to which its whole-disk brightness variations (both apparent and intrinsic) can be modeled. Much of the perceived variability from earth-orbit is attributable to geometrical factors: the apparent phase as seen by the observer and changes in solar irradiance as Mars' distance from the Sun varies in its elliptical orbit (Wright 1976). There are a number of additional complexities, however. The day/night cycle imparts a significant rotational modulation to the observed temperature as the surface heats and cools. In the infrared, variable atmospheric effects including molecular line absorption and dust storms can be of importance. Additionally, the Martian surface is itself far from homogenous, with polar ice-caps of changing extent and variations in albedo and thermal inertia on relatively small scales (Putzig & Mellon 2007; Christensen et al. 2001). Planetary science studies notwithstanding, the astronomical community has most consistently referencedtwo basic models when calibrating to Mars: the "" Wright model "" (Wright 1976, Wright & Odenwald 1980, Wright 2007) and the "" Rudy model "" (Rudy et al. 1987, Muhleman & Berge 1991), both developed before the current level of detail available from orbiters such as Mars Global Surveyor (MGS). In the thermal emission regime (λ 10µm), the planet's brightness temperature can be computed to first order as re-emission of absorbed solar energy by a surface layer with a characteristic albedo, emissivity and thermal inertia (de Pater 1990). The Wright model uses this approach to predict thermal emission in the 10-350 µm range, with no positional variations of parameters such as albedo and thermal inertia. The Rudy model originates from VLA observations at 2 and 6cm, which sample further into the Martian subsurface. Concurrent measurements of the polarized flux are used to estimate radio reflectivities and absorption lengths as a function of planetary latitude , and brightness temperatures calculated via radiative transfer as a function of position on a longitude/latitude grid. Typical errors on T b quoted for the Wright model are ±5%, or about ±10K, and those for the Rudy model at 5%-10%. We have chosen in this paper to compare the WMAP observations against predictions from the Wright model, primarily because of its long history of use and readily available model code (Wright 2007), but also because the seasonal brightness temperatures quoted for WMAP are the resultant average over several weeks of data, which means we are only sampling mean properties at best. We have modified the 2007 Wright model code in three ways: (1) JPL ephemeris positions are substituted for the lower precision formulae originally used; (2) an option is added to use WMAP spacecraft positions rather than those of the Earth; and (3) computations are extended out to WMAP frequencies. The first two changes are not critical to the analysis, as the combined modifications introduce at most at 0.6 K difference from the original results. Typically the model predicts brightness temperatures at 3mm which are a few Kelvins lower than those at 350µm, and the temperature spread between the extrema of the 5 WMAP frequencies is quite low, of order 0.5 K. Hill et al. (2009) presented the first five years of WMAP Mars observations at 94 GHz. Here we extend the W-band observations to seven years, and include the observations at the other four WMAP frequencies. Mean brightness temperatures for Mars at five frequencies and seven WMAP observing seasons derived from the seven-year data are presented inTable 6 . After correcting to absolute brightness temperature and converting from Rayleigh- Jeans to Planck brightness temperature 1 , the seven seasonal W-band averages are shown in comparison to the Wright model predictions at 3.2mm inFigure 3. This plot is similar toFigure 15 of Hill et al. (2009), whose preliminary analysis found that the Wright model was a good predictor of the mean temporal variation, but the model T b seemed high by roughly 10%. Our results generally agree, but we find that multiplying the Wright model by a factor of 0.953 fits the seven-year data best. If one uses the Wright model tabulated at 350µm, the plot looks nearly identical, but the scaling factor is 0.941, or about a 6% reduction. A previous comparison by Griffin et al. (1986) indicated that the 3mm wholedisk observations of Ulich (1981) were within 7.5% of Wright model predictions at 350 µm. The first WMAP Mars observing season occurred during the time of the 2001 July-October massive global dust storm which elevated atmospheric temperatures in the mid-IR by nearly 40 K (Smith et al. 2002). The effect of dust storms on microwave data is expected to be less of a concern, but we did fit for the model scaling factor both with and without inclusion of the first observing season, with no significantly different results."
530,1001.4731.training.tei.xml,17,"The uncertainties on the derived WMAP seasonal brightness temperatures increase with decreasing frequency, which follows as a result of decreasing signal-to-noise in the data. However, it is possible to extract a trend in temperature with frequency by finding within each season the ratio of the brightness temperature at each frequency to that at W band, and then computing the mean over allseven observing seasons as a function of frequency. These means are plotted inFigure 4 in black. Errors are standard deviations of the mean. The weak frequency dependence can be characterized roughly as ν 0.03 , indicated by the blue line. The Wright model, processed in the same fashion, predicts almost no variation with frequency (red line). Such a frequency dependence could be attributed to an emissivity change of ∼ 5% ± 2% over the wavelength interval. In a similar vein, a weak 5% ± 3% decline in surface emissivity with increasing wavelength was noted by Burgdorf et al. (2000) in the 50 to 180 µm range, based on analysis of Infrared Space Observatory (ISO) observations of water vapor absorption lines."
531,1001.4731.training.tei.xml,18,"Some consideration was given to placing the WMAP observations in a wider context, with the goal of gaining further insight into the model parameters. A search was made for a dataset that would provide absolutely calibrated, whole-disk observations of Mars which spanned both sides of the thermal energy distribution peak and were taken over a reasonably contiguous interval of time. Mars data taken by the COBE/DIRBE instrument (Boggess et al. 1992; Hauser et al. 1998) for a146day interval (1990 April 28 -September 20) most closely matched these requirements, although no analysis of the publicly available data appears to have been previously published. There are six DIRBE wavebands which sample the thermal emission portion of the Mars spectrum, with effective wavelengths of 12, 25, , 60100, 140 and 240 µm. Of these, the 25 µm observations cannot be used as an independent check of the Wright model, since the model itself was the ultimate source of the DIRBE absolute calibration in this band (S. Moseley & W. Glaccum 2009, private commu- nication, Hauser et al. 1998). In addition, while the 12, 25, 140 and 240µm detectors are well-behaved linear devices, the 60 and 100µm detectors are known to suffer non-linear response , and Mars is on the bright end of that response. DIRBE is a broad-band photometer, and reports monochromatic flux values at the effective wavelength of each band assuming a constant source spectrum in νI ν across the band. One must apply a "" color correction factor "" to the reported fluxes if the source spectrum is different from that assumed. The DIRBE Explanatory Supplement provides such factors for gray-body and power-law spectra. However , in the case of Mars, some of the DIRBE bands intercept known atmospheric absorption lines. A significant CO 2 absorption centered at 15µm lies well within the 12µm passband, and grazes the blue edge of the 25µm band. Numerous minor water vapor absorptions are known to occur at wavelengths sampled by the 60 and 100µm bands (Sidher et al. 2000). We have used spectral data available from the MGS Thermal Emission Spectrograph (TES) public archives and the ISO spectrum published by Sidher et al. (2000) to form approximate correction factors for the DIRBE data at these bands: only the 12µm band required significant correction. For the 140 and 240 µm bands, color corrections were calculated using the spectrum predicted by the Wright model."
532,1001.4731.training.tei.xml,19,"A mean flux at each DIRBE effective wavelength was evaluated over the146day interval, including application of color corrections and scaling to a fiducial solid angle at a distance of 1.5 AU (Table 7). Although the epochs of WMAP and DIRBE observations are separated by more than 10years, the viewing geometry at the time of the DIRBE observations is most closely duplicated during WMAP observing season four. The WMAP mean brightness temperatures for this season were also converted to fluxes using the same fiducial solid angle,"
533,1001.4731.training.tei.xml,20,"and both sets of data plotted as a spectrum in Figure 5. The spectra predicted by the Wright model for the mean time of thetwo epochs are also shown on the plot. For clarity, the quoted model errors of about ±5% are not shown on the plot. On the logarithmic scale needed to show both sets of data together, the  two model spectra overlay each other. Within the quoted data and model errors, the DIRBE measurements at 60 and 100µm do not match well with the model. The DIRBE 12 µm data deviate by a little more than 1σ, accounting for both data errors and model accuracy. In the case of the 60 and 100µm bands, we suspect a non-linear response to a bright object is the root cause. The 12 µm band has a low absolute calibration error, but there is a significant and uncertain color correction due to the CO 2 atmospheric absorption. Ratios of the DIRBE fluxes to Wright model predictions are presented inTable 7; the major source of DIRBE error is that of the absolute calibration, which is also listed. DIRBE observations at 140 and 240 µm agree within the 10% calibration errors, but cannot be used to deduce any trend with frequency, such as seen by WMAP. We decided to forgo adjustments to the model at this point."
534,1001.4731.training.tei.xml,21,"As currently implemented, the Wright model may be used to predict microwave observations at WMAP frequencies by applying a "" post-processing "" multiplicative scaling factor to the published values: T new = T pub × f scl . For both model and data evaluated at3.2mm, we have already noted a scaling factor of 0.953. We use the mean ratios of the remaining  four frequencies to W band (Figure 4) as scaling factors for these frequencies, which are in Column 2 ofTable 8 . The W-band observations can be reproduced to ∼ 0.5%, with confidence decreasing with frequency to ∼ 2% at K band. Since the 3.2mm model values we have computed are not published, we also provide scaling factors to the 350µm temperatures, which are derived from the the scaling factor of 0.941 noted earlier and multiplied through by the 3.2mm scaling ratios."
535,1001.4731.training.tei.xml,22,"Differences between measurements and the Wright model at WMAP wavelengths are not unexpected. The Wright model was designed to predict infrared emission arising from the top layer of the Martian surface, whereas the microwave signal originates from subsurface layers of orderseveral centimeters deep (de Pater 1990; Orton & Burgdorf 2003 ), with potentially different composition and compaction. This was recognized by Griffin et al. (1986 slightly revised version of the model maintained at NRAO 4 . In general, Rudy model predictions using the "" standard dielectric constant "" were slightly high compared to the WMAP observations, by ∼ 1% at 3.2 and 4.9mm and roughly 3% at 7.3, 9.1 and 13.1mm. It should be noted that the WMAP observations provide model constraints only in the disk-averaged sense. The post-processing scaling of the Wright model suggested here simulates a frequency-dependent emissivity, which implies slow changes in mean subsurface properties with depth. In the Rudy model, which already incorporates subsurface sampling, use of model parameters derived from the radio data produces agreement with WMAP within a few percent. It is possible that this agreement could be improved either through the use of a revised dielectric function or updated thermal inertia and albedo maps. The thermophysical parameters used by the Rudy model are based on ∼20 year old maps of thermal inertia and albedo from the Infrared Thermal Mapper (IRTM) aboard Viking (Muhleman & Berge 1991). More recent MGS maps of these quantities obtained using the TES instrument are available with roughly 10 to 40 times better spatial resolution, full planet coverage and approximately twice the signal-to-noise (Christensen 1998; Christensen et al. 2001). The surface albedo has been observed to have some time dependence: a visual comparison between the Viking IRTM and TES albedo maps shows alterations in the spatial pattern have occurred since the Viking mapping (Christensen et al. 2001)."
536,1001.4731.training.tei.xml,23,"As with Mars, Saturn's microwave brightness temperature varies considerably over time for near-Earth observers, primarily as a result of geometrical effects. Here the dominant factors are the observer's changing viewing angle of the rings and projected disk of the oblate planetary spheroid."
537,1001.4731.training.tei.xml,24,"The ring system presents the main difficulty in Saturn's use as a calibrator in the microwave, although substantial progress has been made in understanding and modeling it. As evidenced by images from Hubble Space Telescope (HST) and data from orbiter and flyby missions such as Cassini and Voyager, Saturn's ring system is complex and rich in detail. The brightest rings (A, B, C) are composed of objects typically less than5m in size (Marouf et al. 1983) and are composed primarily of water ices mixed with a small amount of impurities (Poulet et al. 2003, and references therein). Those portions of the rings between the observer's line-of-sight and the planet serve to attenuate emission from Saturn's disk. The rings also contribute to the microwave signal via a mix of scattering and"
538,1001.4731.training.tei.xml,25,"thermal re-emission of planetary radiation. The crossover point between thetwo emission mechanisms occurs near 1cm (Dunn et al. 2002; Schloerb et al. 1980; Epstein et al. 1980), with thermal emission dominating shortwards and scattering longwards. Optical depths have been measured at a variety of wavelengths: Dowling et al. (1987) found that the optical depth of the combined ABC rings is roughly gray over a wide frequency range. Voyager I radio occultation observations (Tyler et al. 1983) at 3.6 and 13cm produced radial profiles of optical depth for the A and C rings and Cassini division, but only approximate results for portions of the B ring with τ > 1. The most complex and recent modeling of microwave data is that of Dunn et al. (2002 Dunn et al. ( , 2005 Dunn et al. ( , 2007) who use a Monte Carlo radiative transfer code to simulate the scattered and thermal emission of the rings based on VLA and other high-resolution observations. WMAP does not spatially resolve the individual components of the Saturn system, nor is the spectral coverage sufficient to definitively isolate ring scattering and thermal spectral regimes. However, the changes in viewing geometry with orbital phase can be used to break degeneracy between ring and disk components, especially as the viewing aspect of the rings becomes more "" edge-on "" . Over the currently available seven-year observing baseline, Saturn's rings are seen at inclinations between  −28° and  −6 ° : the sign convention indicates that the planet's south pole is tilting slowly away from the observer as the 0 ° edge-on equatorial configuration is approached.Figure 6 illustrates the  two extrema andTable 9 lists mean times and ring opening angles for each observing season. Hill et al. (2009) briefly discussed WMAP five-year W-band observations of Saturn, which sampled ring opening angles B ≤ −17 ° . They found that the brightness temperature could be fit remarkably well with a simple sin B dependence. This convenient relationship does not effectively characterize the behavior at lower inclinations, however, and so a more complex formulation is needed for the seven-year data. We adopt a simple empirical model of Saturn's microwave emission, variants of which have appeared in the literature for over 30years (e.g., Epstein et al. 1980). The goal is to provide a predictive formula for the unresolved observed brightness of Saturn with as low an error as possible: however, given the WMAP data constraints, such a model will lack physical detail. At a given frequency ν, we assume a single temperature for the planetary disk, T disk (ν), and that all rings are characterized by the same temperature T ring (ν). The model variant we adopt allows for seven radially concentric ring divisions as defined by Dunn et al. (2002), which are based on optical depth variations observed by Voyager (Tyler et al. 1983). Each of the seven ring sectors has its own ring-normal optical depth τ 0,i , with 1 ≤ i ≤ 7, but each τ 0,i is assumed to be both constant within its ring and frequency independent."
539,1001.4731.training.tei.xml,26,"Those portions of the planetary disk which are obscured by ring cusps will have their emission attenuated by a factor e −τ 0,i | csc B| , where B is the ring opening angle seen by the observer. Thus at a given frequency and ring opening angle B:"
540,1001.4731.training.tei.xml,27,"where A ud , A od,i and A r,i are the projected areas of the unobscured disk, the portion of the disk obscured by ring i, and i th ring, respectively. These areas are normalized to the total (obscured+unobscured) disk area. T ring (ν) is an observed brightness temperature and hides such physical details as ring emissivity and the apportionment of the scattering versus thermal emission. The assumption of a single mean ring temperature is a convenience rather than reality (Spilker et al. 2006; Grossman et al. 1989; Dunn et al. 2002). However, the apportionment between individual rings is not critical to the model fit, as it only depends on the summed total ring emission. Potential contributions to modeling error from nonuniform planetary disk emission are discussed later in this section."
541,1001.4731.training.tei.xml,28,"The WMAP brightness temperatures inTable 9 are sorted by frequency band and B and then a single simultaneous fit for the model parameters is made.Two separate fits were tried, differentiated from one another in the handling of the ring optical depth. In one fit, the τ 0,i were held fixed at the values chosen by Dunn et al. (2002) (Table 10), and the ten disk and ring temperatures (for each of 10 DAs) were solved for. In an alternate fit, the relative ratios between the τ 0,i were fixed as per the table, but a single τ 0,max was solved for in addition to the ten free temperatures. Both fits returned similar results, with reduced χ 2 of ∼ 1.2 for ∼ 100 degrees of freedom. We show only the results for the fixed-τ fit."
542,1001.4731.training.tei.xml,29,"The disk and ring brightness temperatures derived from this model fit are presented inTable 11. Formal errors for the ring and disk temperatures at lower frequencies are higher because the ring contribution is smaller and there is a larger covariance between thetwo components. The model fit and residuals are shown inFigure 7. The model reproduces the observations to within ∼ 3%, but observations closer to B = 0 ° would better constrain the disk temperatures and so reduce the covariance between the disk and rings. The parameterization chosen for the ring optical depths, together with the incomplete sampling of the ring system orientation, allows for alternate models which would fit the data equally well but return somewhat different mean disk and ring temperatures. This systematic is not reflected in the formal fitting errors inTable 11. In order to assess the magnitude of this error, we explored a small number of model variants. The model which fit the data equally well but returned temperatures with the largest differences compared toTable 11 consisted of a combined ABC ring represented by a single τ 0 , which was a free parameter in the fit. Disk temperatures derived for this model were ∼ 2.5K higher, and ring temperatures ∼ 1  K  lower. As a conservative approach, we additively combine this estimated systematic error with the formal fitting errors to produce the "" adopted error "" columns inTable 11. The derived disk and ring temperatures match well with those already in the literature.Figure 8 shows the WMAP derived disk temperatures in context with the compendium of plus more recent interferometric observations. The ring temperatures also compare well with those measured directly from high-resolution images of the ring ansae (Dunn et al. 2005; Schloerb et al. 1980;): seeFigure 9.Figure 10 illustrates the percentage contribution of the ring to our total model, which sums the ring emission and attenuation of disk radiation, as a function of frequency and ring inclination B."
543,1001.4731.training.tei.xml,30,"The empirical model described here assumes Saturn's mean whole-disk temperature is time invariant at each frequency. The presence of band-like structures (which can persist foryears) and latitudinal brightness gradients at the ∼ 5% level have been noted by several observers at wavelengths of 2cm and longward (Grossman et al. 1989; de Pater & Dickel 1991; van der Tak et al. 1999 ). There is less discussion in the literature of disk temperature structure and/or variability at the frequencies observed by WMAP: there have been no reports of bands or localized structures, but some indication of north/south latitudinal brightness differences. Within the seven-year epoch under consideration, Dunn et al. (2005) observed a latitudinal disk brightness gradient at 3mm, such that the north pole was brighter than the south by ∼ 5%. They interpreted differences between their 1.3mm and 3mm data, taken roughly 4months apart, as possible evidence for atmospheric changes on week to month timescales. In order to estimate potential model bias caused by the assumption of a uniform disk temperature, Saturn seasonal brightness temperatures were simulated using geometrical models similar to those described in this section, with the addition of a disk latitudinal gradient as described by Dunn et al. (2005) . If one makes the assumption that such a temperature gradient is persistent over seven years and exists at all WMAP frequencies, then such structure would produce a temperature behavior with inclination which is asymmetric about B = 0 ° . This is because the rings would obscure hotter regions in the northern portion of the disk but colder regions in the south, producing a maximum deviation from symmetry of ∼ 1% at the largest ring opening angles. Since the WMAP seven-year observations sample slightly less than half of the full B range, this asymmetry would not be evident, and a model fit which assumes a constant disk temperature would use the ring model component to compensate for the disk temperature gradient. For a simulated dataset matching the WMAP seven-year observing seasons, the model fit returns a mean ring temperature biased by less than 1 K, with an unbiased recovery of the mean whole-disk temperature. This estimate is of uncertain quality, however, because it is based on a snapshot of Saturn at  one wavelength and a narrow time window. For this reason, the adopted errors inTable 11  An extended, low optical depth dust ring associated with Saturn's moon Phoebe was recently discovered using Spitzer imaging at 24 and 70µm (Verbiscer et al. 2009). With a quoted radial extent between at least 128 and 207 Saturn radii, or roughly 1 ° in apparent diameter, it would be possible for WMAP to resolve the Phoebe ring, with W band as the best chance for doing so. However, no signal from the ring was detected by WMAP. Properties of the grains are not well specified, but naive estimates assuming typical dust emission and optical depth frequency dependencies would predict temperatures at 94 GHz of less than 1 µK, well below achievable noise levels of about 300 µK (1σ)."
544,1001.4731.training.tei.xml,31,"With their lower brightness, lack of a dominant ring system, and small solid angle, Uranus and Neptune are used as primary calibrators in infrared through radio wavebands for both ground-and space-based instruments. Spectral coverage in the microwave is somewhat undersampled, however, leaving room for interpretation in atmospheric modeling efforts . The microwave spectra of Uranus and Neptune lack the broad NH 3 absorption centered near24 GHz which is characteristic of Jupiter and Saturn. At millimeter wavelengths, collision-induced absorption by H 2 is considered the dominant continuum opacity source. CO rotational absorptions have been observed for Neptune in the sub-millimeter (Marten et al. 2005 ). At centimeter wavelengths, ammonia (which is depleted relative to solar nitrogen levels ) and hydrogen sulfide are the main opacity contributors (Deboer & Steffes 1996; Spilker 1995;). Of special note is Uranus' unique 98 ° obliquity, which allows for a slowly changing pole-to-pole panorama of the planet as it moves in its 84year orbit, and may also play a role in determining atmospheric conditions. For selected frequencies, observational databases spanning decades have permitted characterization of the variability of these two planets. Uranus in particular has exhibited long-term whole-disk temperature changes since 1966 which correlate with the viewing aspect of the south pole. After accounting for geometrical solid angle changes, Kramer et al. (2008) reported a gradual temperature drop of order 10% over 20years (1985-2005) at 90 GHz. This is interpreted as a true integrated-disk temperature change, as the 1985 face-on contribution from the bright south pole progressively morphs into a 2007 view dominated by colder equatorial regions. Similar findings, with similar phasing albeit different amplitudes, have been reported in the radio (Klein & Hofstadter 2006) and visible (Hammel & Lockwood 2007). There is weaker evidence that some portion of variability in the light curves is attributable to changes deep in the atmosphere (Klein & Hofstadter 2006); Hofstadter & Butler (2003) used VLA 2 and 6cm "" snapshots "" of the disk over several  years to argue for opacity changes in zonal bands. Neptune's microwave variability is less well documented, although there is ample evidence for variability in the visible and near- IR, as summarized by Hammel & Lockwood (2007). Kramer et al. (2008 find the 90 GHz integrated disk temperature to be constant to within ∼ 8% over 20years. Peak WMAP antenna temperatures for these distant "" ice giants "" range from roughly 1 mK at W band to ∼0.2mK at K band. The signal-to-noise for individual observations of these objects is low (∼0.3 at W band, ∼0.14 at K band), resulting in large statistical errors in single-season disk temperature determinations. Tables 12 and 13 present the single-season brightness temperatures computed for Uranus and Neptune. Brightness temperatures are listed per frequency rather than per DA as a means of boosting signal-to-noise for those frequencies with multiple DAs. Temperatures for Uranus observing seasons four and five exhibit somewhat larger error bars than other seasons. For these two seasons, there was a reduction in the number of observations available for analysis because data quality checks excluded observations in close proximity to Mars sky coordinates. Sub-WMAP latitudes for Uranus range between −30 ° and 4 ° over our seven-year baseline . Linear correlations of T b against both sub-WMAP latitude and time produced no statistically significant trend other than a flat line. This is not surprising given that the tightest seasonal temperature errors are of order 10%, which encompasses the entire twentyyear range of variation seen in the 90 GHz light curves of Kramer et al. (2008). We performed similar correlations for Neptune, with again the same null variability result. Note, however, that the sub-WMAP latitude for this planet is relatively unchanged over the epochs of observation . With no discernable variability over the observing baseline, we computed seven-year means of the disk brightness for each planet; these are listed as the last line in Tables 12 and 13 . These seven-year means in turn may be compared to observations in the literature and placed in context with the microwave spectra in general.  only returned those few acquired in the late 1960s, listed inTable 1 of Gulkis et al. (1978). For a variety of reasons, more recent observations would be preferable, but the older epoch does have the advantage of sharing a similar viewing geometry to that of the WMAP data. These older observations are plotted in gray in the Figure, and agree well with the WMAP measurements, but also do not define a "" dip "" at high statistical significance. Although suggestive of an interesting atmospheric opacity constraint for future study, the possibility of such a feature has gone unremarked in the literature."
545,1001.4731.training.tei.xml,32,"Non-variable, spatially isolated fixed celestial calibrators formillimeter wavelengths are not common. At high Galactic latitude, bright sources are predominantly identified as some form of AGN/QSO, which are prone to outbursts and variability on a wide range of timescales and frequencies. Brighter sources in the Galactic plane tend to be HII regions (which may not be "" point-like "" ) or supernova remnants (SNRs) with potential variability. At moderate spatial resolution, confusion with neighboring diffuse and compact sources in the Galactic plane can be an issue for background subtraction. Five sources were chosen for study out of an original list which included some of the brightest sources from Baars et al. (1977) and Ott et al. (1994), in addition to the brightest, least variable objects from the seven-year WMAP source catalog (Gold et al. 2010). The five selected sources are listed inTable 14. Some of the sources which were initially considered  but ultimately rejected, primarily because of low background contrast, included 3C286, NGC 7027, 3C84, 3C218, 3C123, and 3C147. There are unfortunately few suitable calibration sources in the Southern hemisphere with a long-term history of observation."
546,1001.4731.training.tei.xml,33,"Flux densities of the selected sources are measured from theseven-year sky maps at HEALPix 5 resolution 9 (Nside=512). For each frequency band, the azimuthally symmetrized beam profile (see Section 3.1) is convolved with a sky map pixel to produce a map-based beam template. The sum of the pixel-convolved beam template plus a sloping planar base level is fit to the Stokes I, Q, and U sky map data at each source position, using pixels within 3.5 times the beam width σ (1.5 times the FWHM) in each band. The peak source temperature from each fit is converted from thermodynamic temperature to Rayleigh-Jeans brightness temperature and then translated to a source flux density using a conversion factor Γ that is a weak function of the source spectral index (Jarosik et al. 2010). For point sources, this method of flux measurement is more accurate than the aperture photometry method as used for example by Page et al. (2003b) for Tau A. The WMAP beam profiles have extended wings (Hill et al. 2009), so an aperture radius 3 times the beam FWHM should be used and the results are more susceptible to error due to background confusion. The uncertainty in flux density is calculated as the quadrature sum of (1) map measurement uncertainty, (2) the uncertainty in Γ (0.5% to 0.7% depending on the band), and (3) the 0.2% absolute calibration uncertainty. For Stokes I, map measurement uncertainty is estimated from the rms fit residual in the fit region. The Stokes I residuals generally appear to be dominated by beam asymmetry effects, but also include background confusion and noise. (The source profiles in the maps are asymmetric due to asymmetry in the instantaneous beam profiles and the nonuniform distribution of scan angles over a year for sources away from the ecliptic poles.) For Stokes Q and U, map measurement uncertainty is calculated either from the 1σ source peak uncertainty and base level uncertainty from the beam fitting or as the Q or U flux times the fractional map measurement uncertainty for I, whichever is largest. The latter method is the estimated uncertainty in Q or U due to beam asymmetry effects, assuming the fractional uncertainty is the same in Q or U as it is in I. The flux determination was tested on simulated sky maps containing a population of point  sources with no other signals and no noise, generated using WMAP beam window functions as described in Wright et al. (2009). Recovered flux densities were accurate to about 0.1% or better, except in W band where the recovered fluxes tended to be larger than the input fluxes by up to 1%. This is allowed for by including an additional 1% uncertainty term in the quadrature sum for W band. Fractional year-to-year variability for the selected sources has been obtained using sky maps for individual years 1-7. To remove confusion noise from the CMB and Galactic foregrounds, we subtract the seven-year average map from each individual year map for each band. A pixel-convolved beam plus flat base level is fit to each difference map at each source position, giving a flux difference ∆F i for the ith year for each source in each band. Uncertainty in ∆F i is calculated from the quadrature sum of the source peak uncertainty and the base level uncertainty. The flux difference is divided by the mean flux from the seven-year map to get the fractional flux variation ∆F i /F . For K, Ka, and Q bands, there is a small ( 0.2%) but significant year-to-year variation in the WMAP calibration, which we have measured by correlating each yearly map against the seven-year map (seeFigure 1 of Jarosik et al. 2010). The measured fractional flux variations in K-Q bands are corrected for these calibration variations c i using (∆F i /F ) corrected = (∆F i /F ) measured + (1 − c i )."
547,1001.4731.training.tei.xml,34,"Source flux densities from theseven-year maps are presented inTable 15. Fractional uncertainties for the Stokes I fluxes are typically 1 to 3%. For some of the sources, the maximum source extent given inTable 14 is not entirely negligible relative to the WMAP beam width in V or W band (FWHM 19.6 ′ in V, FWHM 12.2 ′ in W, Hinshaw et al. 2009). We have estimated the possible error due to source extent for Tau A. The 1.4GHz map of the Tau A region from the NVSS survey (beam size 45 ′′ FWHM, Condon et al. 1998) was smoothed with the symmetrized WMAP V or W-band beam and converted to a resolution 9 (Nside = 512) HEALPix map. The flux determined by our method was found to underestimate the true flux by 1.5% in V band and 3.7% in W band. Spatial variations of the spectral index over Tau A are very small (Morsi & Reich 1987; Bietenholz et al. 1997; Green et al. 2004) so the source extent at W band is probably similar to that at 1.4 GHz. Our V and W-band fluxes for Cas A and 3C58 may also be underestimated by similar amounts. To allow for this, the uncertainty values listed for these  three sources inTable 15 nificant secular decrease is seen for Cas A and Tau A. The results are consistent with a frequency independent decrease of about 0.53% per year for Cas A and 0.22% per year for Tau A. Our results for Cas A fall between the ∼ 0.6% per year decrease found by O'Sullivan & Green (1999) Significant yearly flux variation is not seen for Cyg A and 3C58; the rms year-to-year variation is consistent with the uncertainties in each band. The lowest rms variation is in K band, and is 0.27% for Cyg A and 0.33% for 3C58. Carilli & Barthel (1996) give an upper limit of 10% on Cyg A core variability from observations at 5, 15, and 90 GHz over timescales from 10months to 15years. The core contributes ≤ 10% of the total flux at WMAP frequencies (e.g., Robson et al. 1998). For 3C274 there is evidence for year-to-year variability of about 2% in K, Ka, and Q bands. The rms variation is 1.8 to 2.6 times the mean uncertainty in these bands, and the variations are correlated from band to band. Previous observations of 3C274 have shown greater variability. At 90 GHz, Steppe et al. (1988)Wagner et al. 2009), which corresponds to 4% of the total flux. Smaller core flux variations, less than 1% of the total flux, have been observed at lower frequencies (Morabito et al. 1988; Junor & Biretta 1995; Harris et al. 2009). Spectra of the seven-year WMAP fluxes together with previous measurements from the literature are shown in Figures 14 -18. For Cas A, we have scaled the WMAP fluxes, the Archeops fluxes of Desert et al. (2008), the SCUBA fluxes of Dunne et al. (2003), and the BLAST fluxes of Sibthorpe et al. (2010) to epoch 2000 using a secular variation of −0.53% per year. These are plotted with previous measurements that were scaled to epoch  2000 by Hafez et al. (2008) using frequency-dependent scaling. For Tau A, we have scaled previous measurements from Perez et al. (2010) to the epoch of the WMAP data using a secular variation of −0.167% per year at all frequencies. Results are not significantly different if −0.22% per year is used.Table 17 presents parameters from fits to the spectra for WMAP data alone and for the combined data. The two fits are generally consistent within their uncertainties over the WMAP frequency range. Some notes on the individual sources follow."
548,1001.4731.training.tei.xml,35,"A slightly curved spectrum gives a better fit to the combined data than a power law (chi-squared per degree of freedom χ 2 ν =1.20 compared to χ 2 ν = 2.13 for a power law). The flattening of the spectrum with increasing frequency was previously noted by Hafez et al. (2008). This may be consistent with observations of spatial variations of the spectrum in Cas A (e.g., Wright et al. 1999, Anderson & Rudnick 1996). Wright et al. (1999) presented spectra of 26 brightness peaks from maps with 7 ′′ resolution at 1.5, 5, 28, and 83 GHz. The data were mostly consistent with power-law spectra, with spectral indices ranging from -0.75 to -0.95. (For comparison, the overall spectral index from a power-law fit to our integrated spectrum from 1.4 to 93 GHz is -0.73). Such a variation will lead to curvature in the integrated spectrum. Wright et al. (1999) also found curvature for some of the brightness peaks with the spectra progressively flattening at the higher frequencies, and noted that such curvature is expected from models of particle acceleration in cosmic ray modified shocks (Reynolds & Ellison 1992). Within the WMAP frequency range, all of the epoch 2000 scaled fluxes are consistent within the uncertainties with the fit to the combined data. This includes the WMAP fluxes, the absolutely calibrated fluxes from Janssen et al. (1974) at 22.29 GHz and Mason et al. (1999) at 32 GHz, the 33 GHz flux from Hafez et al. (2008), which is calibrated using the five-year WMAP Jupiter temperature, and the 86 GHz flux from Liszt & Lucas (1999), which is calibrated using the Ulich (1981) Jupiter temperature. Above 300 GHz, there is excess emission above that expected for synchrotron emission, which has most recently been interpreted as emission from cool dust by Sibthorpe et al. (2010). The 353 and 545 GHz fluxes from Archeops (Desert et al. 2008) are much higher than the 600 GHz flux from BLAST (Sibthorpe et al. 2010) and the 353 and 666 GHz fluxes from SCUBA (Dunne et al. 2003). The Archeops measurements were made with a larger beam (∼ 12 ′ compared to ∼ 20 ′′ or better for SCUBA and BLAST) and appear to be affected by dust emission that is not associated with Cas A. The WMAP 23 GHz (K band) polarization map for Cas A exhibits unexpected structure."
549,1001.4731.training.tei.xml,36,"Figure 19 showsseven-year mean intensity and polarization images centered on each source at each of the five WMAP frequency bands. These images are 4.15 ° on a side, have not been background subtracted, and are scaled such that brighter pixels are black. The polarization (P) image for Cas A shows an irregular ∼ 0.15mK ring at a radial distance roughly 40 ′ − 50 ′ from the source position. The angular extent of Cas A is ∼ 5 ′ (Table 14), leaving the reality of the ring feature in question. We have attempted to simulate this feature under the hypothesis that it is an artifact introduced by a combination of beam and source spectrum characteristics. Cas A is a steep-spectrum source. As a result of effective frequency differences between the  two K-band radiometers (Jarosik et al. 2003b), the K11 radiometer (fed by the axial OMT port) has an FWHM roughly 3% wider than that of K12 (which is fed by the lateral OMT port), and the peak observed signal in K11 is a few percent higher than that of K12. Simulated beam maps for the  two K-band radiometers were generated separately using Jupiter data as a template, and then individually scaled to peak values representative of Cas A. The difference between the  two beams as a function of azimuthal angle and radial distance from beam center can be used to compute a rough estimate of induced Cas A polarization signal, under the assumption of complete scan-angle coverage.Figure 20 shows the results of such a simulation, which produces a feature with an approximately correct peak position (near 50 ′ ), but slightly wider and 30%-40% brighter than that shown in the data image. A more complete simulation would include scan-angle coverage effects, which in this scenario are presumed responsible for the gaps in the ring. This is the only known instance of an apparent artifact in WMAP polarization data."
550,1001.4731.training.tei.xml,37,"The fluxes from Janssen et al. (1974) at22.29 GHz, Wright & Birkinshaw (1984 at 89 GHz, and Wright & Sault (1993) at 94 GHz are consistent with the WMAP results. The flux from Hafez et al. (2008) at 33 GHz is lower than WMAP Ka-band flux by 2.7σ, taking both flux uncertainties into account. We found that excluding the Hafez et al. (2008) flux from the power-law fit to the combined data improved χ 2 ν from 2.89 to 0.79, so it was excluded for the fit plotted inFigure 15 and the fit parameters given inTable 17. Most of the measurements above 100 GHz inFigure 15 are fluxes summed over the core and two hot spots in the radio lobes. These are probably valid measurements of the total flux, since the contribution of extended emission from the steep-spectrum lobes appears to be small or negligible at these frequencies. At 230 GHz, Salter et al. (1989b) found that the integrated flux over the entire source was only about 10% greater than the summed flux of the hot spots and core, which did not amount to a significant detection of emission from the lobes."
551,1001.4731.training.tei.xml,38,"The WMAP flux agrees with that of Janssen et al. (1974) at22.29 GHz and with that  of Hafez et al. (2008) at 33 GHz. Flux measurements from three-year WMAP sky maps by Macias-Perez et al. (2010) are consistent with our results in K-V bands, but their Wband measurement is 2.8σ greater than ours. The difference is probably due to difference in background subtraction error. Tau A has been recommended as a polarization calibrator for CMB experiments by Aumont et al. (2010) This mean rotation measure appears to be consistent with the rotation measure map obtained by Bietenholz & Kronberg (1991) from VLA observations at 1.8 ′′ resolution. They found a large-scale rotation measure of ∼ −21 rad m −2 that they attribute to the interstellar medium, and from the observed depolarization they inferred absolute rotation measure values of a few hundred rad m −2 in unresolved filaments. Lopez-Caniego et al. (2009) report polarized flux densities they measured from 5-year WMAP data that are smaller than the values inTable 15 by 9%, 15%, 28%, and 67% at 23, 33, 41, and 61 GHz, respectively. For Cas A, Cyg A, and 3C274, their polarized fluxes agree with those inTable 15 within the uncertainties."
552,1001.4731.training.tei.xml,39,"The WMAP data show the spectral steepening first suggested by the84 GHz measurement of Salter et al. (1989a) and confirmed by the upper limits of Green & Scheuer (1992) in the infrared. Our adopted form for the spectrum transitions between a low-frequency power law and a high-frequency power law, and gives a good fit to the combined radio to infrared data. Our fit gives a change in spectral index of about 0.9. Additional steepening is needed to explain the observed X-ray spectrum, and Slane et al. (2008) suggested there is a second spectral break somewhere in the infrared. They presented model calculations in which the break at ∼ 100 GHz is due to a break in the electron spectrum injected into the nebula and the higher frequency break is due to synchrotron losses. 3C274 – The WMAP K-band flux agrees with that of Janssen et al. (1974) at 22.29 GHz. The combined data are better fit by a quadratic (χ 2 ν = 0.99) than by a linear power law (χ 2 ν = 2.54). Curvature in the overall spectrum might be expected since 3C274 has a steep spectrum  halo. Above 10 GHz, the contribution of the halo to the total flux is < 1% (Baars et al. 1977). Over the WMAP frequency range, the quadratic fit and the WMAP only power-law fit are not significantly different. The data of Ott et al. (1994) give a flatter 1.4-10.6GHz spectral index than that of Baars et al. (1977), and they suggested that this may be due to core activity that could cause variability at the 5-10% level. As noted above, variability of about 4% and 17% has been observed at 43 GHz and 90 GHz, respectively."
553,1001.4731.training.tei.xml,40,"WMAP data provide well-calibrated radiometry of the outer planets and bright sources. Theseven-year data provide the longest WMAP baseline to date from which to study temporal variability, plus an increase in signal-to-noise which allows us to include fainter objects such as Uranus and Neptune in the analysis. Jupiter temperatures derived from the seven-year data are within 1σ of the previously published five-year values of Hill et al. (2009). The disk-integrated temperatures derived for Jupiter at WMAP frequencies have uncertainties of less than 1%, with an apparent seasonal stability of ∆T /T = 0.2% ± 0.4%. As a planet, Jupiter has additional advantages in that it has a small apparent angular diameter and moves with respect to the fixed sky, allowing for well-characterized background subtraction. Of the ten objects studied, WMAP uncertainties for Jupiter are lowest, and we continue to recommend it as the best means for transferring the WMAP dipole calibration to another microwave instrument."
554,1001.4731.training.tei.xml,41,"With the aid of models to predict geometrical and/or intrinsic variations, the WMAPderived temperatures for Mars and Saturn are predictable to ≤3%. The recommendation for using the Mars values needs to be tempered by the knowledge that these have been derived using mean seasonal properties only. The Saturn model is purely empirical, and can be improved with the addition of the 2009 and 2010 WMAP observations when the rings are nearly edge-on. Uncertainties in the seven-year mean Uranus temperatures range from 3% at W band to about 7% in K and Ka band; only the Q, V and W measurements are suitable for calibration at the 5% level. Some form of simple model including polar brightness effects would be required to extend the seven-year mean data here beyond the observing epoch, since longterm studies in the literature show clear seasonal temperature variations. There is also the surprising hint of a dip in the microwave spectrum near 30 GHz, where none is expected based on discussions in the literature. Seven-year mean temperature uncertainties for Neptune exceed 5% at all WMAP frequencies and so are not recommended for serious calibration usage. Flux densities in Stokes I for the five celestial calibrators have typical uncertainties of 1%-3%, and are in good agreement with previous measurements in the WMAP frequency range. For four of the sources, the WMAP data and other available data are consistent with a simple well-defined spectrum over  two decades in frequency or more. The WMAP observations improve the accuracy of the spectral fit parameters and in some cases they extend the frequency range where the object may be used for calibration. The uncertainties in the spectral fits are typically 1% or less in amplitude and 0.02 or less in spectral index. We provide new estimates for the secular variation of Cas A and Tau A, and provide limits and an estimate for year-to-year variability of the other sources. We present WMAP polarization data with uncertainties of a few percent for Tau A. The seven-year WMAP data products are available to the research community through the Legacy Archive for Microwave Background Data Analysis (LAMBDA) at http://lambda.gsfc.nasa.gov. The WMAP mission is made possible by the support of the Science Mission Directorate Office at NASA Headquarters. This research has made use of NASA's Astrophysics Data System Bibliographic Services. We acknowledge the use of the HEALPix package (Gorski et al. 2005)."
555,halshs-01279855.training.tei.xml,0,"The Tour de France is one the world""s largest sports events. Thethree-week race attracts 10 to 15 million (non-paying) spectators along the roads each year and individual stages are watched by 15 to 20 million TV viewers worldwide (see chapter 5). Although generally cycling races are not lucrative, the Tour de France organiser""s balance sheet reveals that the Tour has always been profitable in the past two decades."
556,halshs-01279855.training.tei.xml,1,"How can this attractiveness and the economic success story be explained? Most sports economists are used to turn to a contest""s outcome uncertainty as a first explanatory variable for success. However, fans of the Tour de France are not often surprised by the name of the final winner of the race, usually not even by thethree riders sharing the podium. Nobody needs more than a few riders"" names to predict with great certainty who the winner will be. Thus, explaining Tour de France success by outcome uncertainty and its usual metrics in terms of competitive balance must be checked carefully."
557,halshs-01279855.training.tei.xml,2,"Tour de France attendance and TV audience do not seem to be seriously threatened and surely not definitely affected by recurring doping scandals either. This was neither the case after big doping issuesmany decades ago (for example when Tom Simpson died during the climb of the Mont Ventoux in 1967), nor in the past twenty years (from the Festina affair in 1998 up to Lance Armstrong being stripped from all his Tour de France wins in 2012). Although doping seems not to be an immediate threat to the popularity of the Tour de France, it is clear that doping detection and doping sanctions have not been efficient and effective enough so far to clean this sport contest from the use of forbidden doping substances and practices. This raises the issue of what could be a better way to combat the use of doping in the Tour de France or in cycling in general."
558,halshs-01279855.training.tei.xml,3,"The chapter goes as follows. In the first section it is demonstrated how the Tour de France is a high quality product. This is a result from its accurate design, its management, its economic model and its finance structure, both in comparison to other mega-sporting events and with reference to tournament theory. It is not easy to assess the competitive balance in the Tour de France since, as was demonstrated in chapter 10, it is at the same time an individual and a team sport contest. After reviewing some results published in literature so far, a new metrics for evaluating competitive balanced in the Tour de France is presented in section 2. Finally, the Tour de France cannot ignore doping as a potential threat to fan attendance and TV viewing. We therefore discuss the issue of doping and a new procedure to deal with doping in section 3."
559,halshs-01279855.training.tei.xml,4,"A top sports event offered for free will automatically attract a significant demand. Beyond this basic and quite obvious cause of Tour de France attractiveness, its design and management are also explanations of its success. As we will show below, the latter may be due, to some extent, to the Tour fitting with the recommendations derived from tournament theory and to basically sticking to a modern model of professional sport finance."
560,halshs-01279855.training.tei.xml,5,"The Tour de France is managed by Amaury Sport Organisation (ASO). ASO also organises other professional cycling races (such as Paris-Roubaix or the Critérium du Dauphiné Libéré) as well as golf tournaments, track and field events, car races and horse-riding contests. Professional road cycling accounts for close to70% of ASO revenue, mainly because of the success of the Tour de France (Desbordes, 2006). Since ASO is a privately-owned company, it aims at profit maximising and designs its strategy accordingly. A first strategic tool consists in multiplying the number of trophies within a race. This process started up in 1919 with introducing the yellow jersey and lasted until 1989 when the number of trophies has nearly stabilised, respectively rewarding the best rider overall (yellow jersey), the best climber (polka-dot jersey), the best sprinter (green jersey), the best young rider (any rider aged below 25, white jersey), and the best-ranked team. From time to time other trophies appeared and vanished again after some years. By creating ""different races within the race"", ASO attempted and succeeded in making the Tour de France a more vivid contest with multiple opportunities for duels between riders or teams competing for a given trophy and changes in the tactics adopted by riders or teams during the course of the race."
561,halshs-01279855.training.tei.xml,6,"Two other factors that attract people to see riders on the Tour de France roads are linked together. The first one is the riders"" performance, the second one lies in the hardness of the race. Based on the number of racing days and the number of rest days per racing day, the Tour de France became much harder after the 1920s with the number of racing days stabilising up to about 20 . the number of stages was usually 15 at most and the riders enjoyed at least one day of rest after each stage. The overall riding distance was 2,428 kilometers in 1903 and rose to a maximum of 5,745 kilometers in 1926. Since then the distance shortened to about 3,500 kilometers on average. Thus, compared with the initial era of so-called ""road""s convicts"" (les forçats de la route in French), riders now spend more days on the roads and have less often time to rest. The race also became harder due to the introduction of mountain stages. The first mountain ever climbed in the Tour de France was the Ballon d""Alsace in the Vosges region in 1905. A handful of years later, high-mountain stages in the Pyrenees (1910) and the Alps (1911) were introduced. However, the number of passes over 1,000 meters in a given Tour de France has not increased significantly on average ."
562,halshs-01279855.training.tei.xml,7,"Apart from the physical hardness of the Tour de France, it is also the increase in riders"" performances that has made the Tour attractive to spectators. The overall average speed of the yellow jersey winner was between25 and 30 kilometers per hour (km/h) . Since World War II it has steadily increased. From 25.7 km/h in 1903 to 41.7 km/h in 2005 (the fastest Tour ever) is an improvement of 62%. Part of this acceleration is due to technical progress affecting riders"" bikes such as the introduction of derailleur gears, the increase in the number of gears, lighter bikes, profiled wheels, etc. (Calvet, 1981;Andreff, 1985). Improvement of the road surface, shorter stages, multiple stakes and high effort intensity in the crucial parts of the race have also triggered both higher rider performance and increased attractiveness to spectators. Improved physical and medical preparation, better nutrition, and sometimes doping may have played a role as well. Another index of riders"" performance is the withdrawal ratio, the percentage of riders who do not finish all stages of the Tour de France. This ratio decreased from over 70% in the 1920s . The highest withdrawal ratio was reached in 1926 (as explained above the longest Tour de France ever held) . In 2009, the lowest withdrawal ratio was recorded with 156 out of 180 riders finishing the Tour de France. Only 24 riders (13.3%) abandoned the race."
563,halshs-01279855.training.tei.xml,8,"Last but not least, the Tour de France is a sports event that is supplied for free tomillions of spectators along the roads. However, the demand for it is not infinite. The demand is rationed by various constraints such as the date and location of a stage or the hosting capacity of a geographical site (like the slopes of the Mont Ventoux or Alpe d""Huez), limiting the number of spectators. Nevertheless, from an economic point of view, a free sport event is likely to draw a huge attendance. Indeed, 10 to 15 million spectators per year attend the race over the course of three weeks. This is significantly more than the biggest mega-sporting event in the world, the FIFA World Cup . Moreover, since all spectators cannot attend the Tour 21 days long, they also demand a story telling which was first supplied by newspapers, then through radio broadcasts and eventually through TV broadcasts."
564,halshs-01279855.training.tei.xml,9,"The Tour de France has not always been a profitable business.The deficits vanished with rising TV rights revenues. The first Tour de France TV broadcast was the finish of the last stage at the Parc des Princes stadium in Paris in 1948 whereas the first TV broadcast of a mountain pass was at the Aubisque in 1958. Nowadays, the Tour is the major sports event in the French broadcasting market with well over 80 hours of broadcast, ahead of the Roland Garros tennis tournament (77 hours), Champions League matches, Formula 1 races and the rugby Six nations tournament. In 2013, 24 million French people watched the Tour de France for at least one hour on French television, a TV market share of slightly over 30%. "
565,halshs-01279855.training.tei.xml,10,"Today it is a global mega-sporting event. Over100 TV channels in 190 countries now broadcast the Tour de France, with live broadcast in 60 countries (www.aso.fr). Consequently, the Tour de France TV rights revenues have increased significantly over the years. In 1960, the French public TV channel started paying TV rights for broadcasting the Tour de France. The French broadcasting rights equalled or the French professional football league broadcasting rights over one season (€ 668 million). No detailed information on the international TV deals is available, but in chapter 3 it was shown that the global broadcasting rights for the Tour de France probably amount to about € 50 million a year. As a result, the budget of the Tour de France has literally skyrocketed . the Tour operational budget grew from € 5 million to € 50 million (Mignot, 2013) primarily due to the growth of TV rights revenues that multiplied by 65 over the same period of time (from € 250,000 to € 16 million). In the past decade, the budget further increased from € 77 million in 2003 up to € 130 million in 2013. Table 1 illustrates the financing sources of the Tour de France. Basically, three types of income can be distinguished: TV broadcasting rights, merchandising and sponsorship revenue, and income from municipalities. The media success story is the basis for the solid economic foundations of the Tour de France because it provided 44% of its overall budget in 2010. Just like in other professional sports, TV rights are now an important source of revenue, resulting from an organiser""s strategy of using its monopoly power over the Tour de France to increase these rights. This kind of revenues emerged in 1925-1929 when the Tour had transformed from a race opposing individual riders into a race between opponent teams sponsored by trademarks and commercial companies. Since 1930, additional sponsorship income was generated with the introduction of a publicity caravan, i.e. dozens of vehicles preceding the riders by a few minutes and distributing product samples to spectators (see chapter 1). The share of advertising and sponsorship revenue has declined in the past two decades to 51% of the budget in 2010. Finally, each year over 200 cities are a candidate for hosting a Tour de France stage arrival and/or departure . However, because of the rising overall budget the share of the contributions from these cities in the total budget of the Tour de France has decreased from 40% in 1952 to 5% in 2010. The Tour de France has thus also evolved towards a contemporary model of professional sports finance and developed from a so-called SSSL (= Spectators, Subsidies, Sponsors, Local) model to a MCMMG (= Media, Corporations, Merchandising, Markets, Global) model with regard to its major sources of finance (Andreff & Staudohar, 2000). In the MCMMG model, media, in particular through TV broadcasting rights, have become a significant source of income for many sports. Furthermore, alongside with sponsors and gate receipts (spectators) whose share in overall finance of sports events has shrunk, new sources of finance have emerged including corporations (investment funds, big companies, Russian oligarchs, Middle Eastern sheiks enriched by oil sales, …), merchandising of non sporting goods (e.g. T-shirts) under a club""s or a player""s label, and markets. With respect to the latter, we see that money is channeled into contemporary professional sports from two markets: a labour market for talent in which a club makes money in selling home-educated and trained talented players and a capital market that enables to trade a club""s shares at the stock exchange and collecting money from the fans as shareholders. Most of these new sources of finance are global."
566,halshs-01279855.training.tei.xml,11,"From table 1 we also see that although the current Tour de France financing model is in line with big professional sports leagues such as the football English Premier League and the French Ligue 1, it remains specific in a way too. For example, its TV-dependence is slightly lower in percentage of overall revenues compared to football and merchandising and sponsorship (51%) is still a more important source of income to the Tour de France organisers than TV rights revenues. This may be interpreted as either the Tour de France being in a transition phase between the SSSL and MCMMG models, or as a consequence of the absence of gate revenue which must be compensated for by sponsorship money. In fact, the lack of gate revenue potentially deprives the Tour de France of up to a quarter (like in English football) of overall revenues. The share of public money received from municipalities is more important in the Tour de France than in European football though it is now reduced to 5% only. At the end of the day, the profitability of the Tour de France is likely to be guaranteed as long as it will attract media, advertisers and sponsors even without any spectatorship income."
567,halshs-01279855.training.tei.xml,12,"A more theoretical reason may explain why the Tour de France is such a successful sport contest. The tournament theory (Tullock, 1980) was first conceived for analysing the efforts dedicated by competing job seekers to get a job and was then adapted to sporting contests, namely tennis tournaments (Rosen, 1986). Each participant is assumed to independently choose the quantity of resources (physical efforts) he is going to invest in view of winning the tournament and receiving the winner""s prize. His winning probability increases with this quantity. Let V stand for the value of the winner""s prize and n the number of competing participants. Each participant i dedicates an effort e i and his probability of winning p i depends on his share in the overall effort devoted by all participants that is:"
568,halshs-01279855.training.tei.xml,13,The financial gain of participant i is:
569,halshs-01279855.training.tei.xml,14,"where c i stands for marginal cost of each participant i""s effort or investment in the tournament and m is a parameter that measures the power of the tournament to discriminate across competing participants,"
570,halshs-01279855.training.tei.xml,15,"i.e. m indicates how muchone unit of competitor i""s effort increases his winning probability."
571,halshs-01279855.training.tei.xml,16,The assumption that the organiser of a sport tournament acts profit maximising implies that the goal is to attract as many spectators as possible by gathering high quality athletes and by securing that these athletes dedicate maximal efforts to win. Therefore the organiser must conceive incentives in such a
572,halshs-01279855.training.tei.xml,17,"way that athletes maximise their efforts and, consequently, produce a spectacular sporting event. From tournament theory it is mathematically derived (Szymanski, 2003;Andreff, 2012) that:"
573,halshs-01279855.training.tei.xml,18,"a/ Whenone competitor has a very high capacity to win, the tournament practically is without interest for other participants who will dedicate only a minimal effort. The organiser must avoid such a competitive imbalance."
574,halshs-01279855.training.tei.xml,19,"b/ Individual effort and aggregated effort of all the participants increase with the value of the winner""s prize V."
575,halshs-01279855.training.tei.xml,20,c/ Individual effort decreases with the number of participants.
576,halshs-01279855.training.tei.xml,21,d/ Aggregated effort increases with the number of competitors.
577,halshs-01279855.training.tei.xml,22,"e/ Participants"" efforts are more intense in a tournament with multiple prizes, where there are several prizes or trophies at stake, as soon as the competitors"" abilities to win are different."
578,halshs-01279855.training.tei.xml,23,"f/ The effort will be more intense the wider the gap between the winner""s prize and the prize rewarding the runner-up, and the wider the gap between the prize for the runner-up and the prize rewarding the third ranked rider, and so on. This is particularly true when the differences between the competitors"" winning abilities are small."
579,halshs-01279855.training.tei.xml,24,"As ASO indeed aims at making a profit from organising the Tour de France, the choice of an appropriate incentive mechanism is crucial. Most of the prerequisites for a successful and attractive between more teams with fewer riders each and fewer teams with more riders each. At the end of the day, the organiser chooses the number of teams and which teams are selected to participate in Tour de"
580,halshs-01279855.training.tei.xml,25,"France each year. As described in chapter 2, this is not without conflicting interests between the organisers of the ""grand tours"" (ASO, RCS and Unipublic) and the UCI claiming that all the Pro Tour teams must be invited in each ""grand tour"" (Rebeggiani & Tondani, 2008). Total prize money in the Tour de France is now over€ 2 million (for a detailed analysis, see chapter 3) which, compared with other cycling races, is rather fulfilling the prerequisite (b). Since the prize money is distributed over multiple trophies, condition (e) is satisfied as well. Only condition (a) is debatable when one witnesses a rider winning the Tour de France five times like Jacques Anquetil, Eddy Merckx, Bernard Hinault and Miguel Indurain, or, in the case of the later disqualified Lance Armstrong, seven times in a row."
581,halshs-01279855.training.tei.xml,26,"Condition (a) is not valid either when the yellow jersey winner is too much ahead of the secondranked rider, like in1952 when Fausto Coppi won the Tour de France with a lead of over 28 minutes."
582,halshs-01279855.training.tei.xml,27,"In fact, condition (a) raises the issue of competitive balance in the Tour de France which is discussed in section 2 below. Finally, prerequisite (f) refers to the distribution of Tour de France prize money across different trophies and best-ranked riders. In the2014 Tour de France, for every lower position in the general classification the prize money was more or less halved between the first seven ranked riders: from € 450,000 for the winner over € 200,000 for the runner-up to € 100,000 for the third ranked rider, and so on to € 11,500 for the rider ranked in seventh place. A similar gap is assessed as a strong effort incentive in most individual sports tournaments such as, for example, tennis. With this incentive prize structure a rider who jumps from the seventh to the fifth rank multiplies his financial gains by four and the same occurs when a rider jumps from the third to the first place. The prize structure for winning a stage is similar with also approximately a doubling of the monetary reward for one rank improvement between the four best ranked riders. The prize structure for the other trophies is less in tune with tournament theory. In the race for the green, the white and the polka-dot jerseys, financial gains are not doubled when a rider improves his rank by one. These trophies thus clearly have a less incentive prize structure. If one ASO objective is to ""multiply the races within the race"" this must not go as far as disturbing the contention for the yellow jersey. That is the reason why incentives are significantly lower (€ 25,000 for the green and polka-dot jersey winners, € 20,000 for the white jersey winner) and less structured according to the tournament theory for trophies that only appeal to specialised riders like sprinters or climbers."
583,halshs-01279855.training.tei.xml,28,"It should be remarked though that this focus on prize money is not always relevant in the context of cycling races. There are significant indirect financial and non-financial gains too from winning, such as a salary increase and fame. Since, as was shown in chapter 3, to an individual cyclist salaries are much more important than prize money, this incentive could be higher than the pure prize money."
584,halshs-01279855.training.tei.xml,29,"When economists attempt to explain a successful attendance and/or TV audience for a sports event, they are used to refer to competitive balance. Applying this concept to professional road cycling is not evident but in chapter 12 this issue is discussed in detail. An introduction to competitive balance in the Tour de France and a new tentative metrics is presented here."
585,halshs-01279855.training.tei.xml,30,"Outcome uncertainty is at the heart of sports economics (Rottenberg, 1956;Neale, 1964). When a big gap between the aggregated players"" talent oftwo teams is witnessed, there is a quite low probability to win for one team while the other is nearly certain of winning. In such a case, outcome uncertainty is negligible and the game exhibits a competitive imbalance or a weak competitive balance. If, by chance and as it happens sometimes, the underdog defeats the favourite team, the outcome will be assessed as a ""surprise"". However, the notion of a surprising sporting outcome is still in the cradle (M. & W."
586,halshs-01279855.training.tei.xml,31,"Andreff, 2014). A similar approach is used to assess the outcome uncertainty of a sport contest or a sports league. Sports economists often assume and then attempt demonstrating that a balanced sport contest or game does attract big audiences while a low attendance signals a weak competitive balance."
587,halshs-01279855.training.tei.xml,32,"This view has been mitigated in recent years. A distinction betweentwo types of sport show consumers has been successfully tested. On the one hand, fans and season ticket holders basically expect the greatest possible number of wins from their favourite team and thus care less about competitive balance. On the other hand, TV viewers and casual spectators often care less about the winning team and are attracted by more balanced games. For example, for NBA basketball it has been shown that fans wish at least two-thirdsof wins for their favourite team (Rascher & Solmes, 2007)."
588,halshs-01279855.training.tei.xml,33,"This differentiation in the product market for sport shows has recently been integrated in a disequilibrium model of a team sports league (Andreff, 2014a)."
589,halshs-01279855.training.tei.xml,34,"Taking the audience interest as a benchmark, the Tour de France would definitely be considered to be a well-balanced competition given its10 to 15 million people attendance. But since this sport show is offered for free, such a conclusion might be misleading. If one wants to really test the concept of competitive balance, a next issue is to develop an accurate metrics. With team sports, the theoretically best competitive balance for a game is when both teams have a 50% probability of winning. The most used proxy for this probability consists of comparing the win percentages of the two teams and, for the league as a whole, of designing some sort of win dispersion or concentration indices (Andreff, 2012;"
590,halshs-01279855.training.tei.xml,35,"Groot, 2008), the most famous being the Noll-Scully index. These indices are static as they pertain to justone game or one season. A dynamic competitive balance index refers to several seasons in a row or, in the case of Tour de France, to several consecutive years. Usually dynamic competitive balance is measured by looking at a rank correlation coefficient across seasons."
591,halshs-01279855.training.tei.xml,36,"The only (albeit serious) problem with aforementioned indices is that just like any other cycling race, the Tour de France is not a bilateral confrontation betweentwo teams or two riders and the competitive balance indexes for team sports league thus cannot be applied as such. One cannot derive and calculate a win percentage between, for example, Team Sky and Movistar from their performances in previous cycling races since their sporting outcomes are not the result of a bilateral confrontation but of an overall series of contests with many other cycling teams. Since it is not possible to define bilateral win percentages, another metrics has to be developed. This is all the more the case because road cycling is at the same time an individual sport and a team sport. There is just one yellow jersey winner but all riders are grouped into teams and no one can win the yellow jersey without the work and effort from his teammates. Moreover, given the multiple trophies that are at stake, two or more teams often divide labour and co-operate (sometimes collude) in view of winning a specific trophy."
592,halshs-01279855.training.tei.xml,37,"Such division of labour usually alleviates competition, influences the competitive balance, and if it happens too frequently, may offset the organizer""s incentives to solicit more effort from the riders."
593,halshs-01279855.training.tei.xml,38,"Although this leads to the impression that the Tour is very imbalanced, the conclusion relies on the performance of only one rider, making it even less robust than above. Nevertheless, it means that one feature often associated with outcome uncertainty, namely an unforeseeable sporting result (Neale, 1964) is missing or really weak in Tour de France."
594,halshs-01279855.training.tei.xml,39,"A common limitation to all previous indexes is that they assess competitive balance only on the basis ofone or two riders or, at maximum, of a small number of riders in contention for the yellow jersey as (2008, 2012 and 2013) out of seven years. Now, if m 1 , m 2 , …, m n stand for all teams"" actual average times (with n the number of teams), then we can determine the actual average team time for the whole peloton (as follows:"
595,halshs-01279855.training.tei.xml,40,"We define  as the standard deviation of the distribution of all actual teams"" average times around the mean . The standard deviation is a potential index of competitive balance. For example, one can compare   for Tour de France 1 to    for Tour 2 and conclude from     that Tour 1 is more balanced than Tour 2 since the teams"" strengths are more closely matched in the former than in the latter."
596,halshs-01279855.training.tei.xml,41,"Remind that the famous Noll-Scully index compares the actual standard deviation of win percentages to a theoretical optimal one, the best possible competitive balance when all teams have à50%"
597,halshs-01279855.training.tei.xml,42,"probability to win. What would be the best Tour de France competitive balance? It is obvious that when  =0 all teams have shown exactly the same strength. However such perfect competitive balance is as much theoretical as the Noll-Scully benchmark of all teams having an equal 50% probability to win. Its practical interest is limited in the real world. Another index is required to qualify how much one specific Tour de France is balanced or imbalanced. Here two options exist. One is to refer to the coefficient of variation  = . A statistical distribution is considered as extremely scattered (i.e. the Tour is extremely imbalanced) when ≥1, and the closer to zero the less it is scattered. As a benchmark  may be accepted for concluding that a Tour is balanced. A second option is to compare the observed distribution of team""s actual average times with a Gaussian (or normal) distribution. In the latter, 68.3% of the population is contained between and . If for a given Tour de France the number of teams comprised within this interval is smaller than 68.3%, not only the Tour is imbalanced but it must be assessed as abnormally imbalanced since teams"" strengths are less concentrated in the central part of the distribution than in a Gaussian population. Such an imbalanced situation could result from having too many weak teams participating in the Tour de"
598,halshs-01279855.training.tei.xml,43,"France. Conversely, if the number of teams in the above interval is bigger than68.3%, then the Tour is less imbalanced than it would have been under the conditions of a Gaussian distribution. The 2010 Tour de France with a 68.2% of teams within the (; ) interval has a nearly"
599,halshs-01279855.training.tei.xml,44,"Gaussian distribution of teams"" strengths and can be taken as a benchmark. Several Tours (2007,2011,2012 "
600,halshs-01279855.training.tei.xml,45,"This leads to a major conclusion. Whenmultiple years are considered, the Tour de France generally shows a dynamic competitive balance, with uncertainty and unpredictability. However, when just one Tour is considered, the Tour de France rather exhibits a static imbalance. This implies that in the course of the race it is rather predictable what teams will perform the best, but from one Tour to the other the outcome is much more unpredictable due to team changes, team recruitment, the selection of riders for the next Tour, and the resulting team performance. One could therefore conclude that although each Tour is a new story, for each story the scenario is rather well known to the spectators."
601,halshs-01279855.training.tei.xml,46,"Nevertheless, the Tour still attractstens of millions of people along the roads and in front of television sets. This confirms earlier observations in sports economics research that questioned the importance of competitive balance as the main determinant of viewership interest for a sports event."
602,halshs-01279855.training.tei.xml,47,"3. Doping did not hinder Tour de France success, but could it be fought better?"
603,halshs-01279855.training.tei.xml,48,"The Tour de France success story has not been disturbed too much so far by its long-lasting association with doping, not even by the doping troubles of the pastdecade. Does this mean that doping has no impact on the Tour outcome? Or that fans and spectators are unaware of doping? Or that the anti-doping combat has become so effective that doping use is actually on its way down within the peloton? Doping in cycling is discussed in detail in chapter 13. An introduction to doping in the Tour de France and an innovative procedure to better deal with the use of doping is presented below."
604,halshs-01279855.training.tei.xml,49,"It is very difficult to empirically test the impact of doping on the outcome of a sports contest since there is no available database about who is doped or not in a team, in a sport, or in the Tour de France."
605,halshs-01279855.training.tei.xml,50,"As a consequence, the above used competitive balance indexes as well as the analyses of sporting success determinants (Torgler, 2007) or team efficiency ( Rogge et al., 2013) in the Tour de France, are for certain distorted to an unknown extent. In fact, they do not include an explanatory or dummy variable supposed to capture the doping effect on rider and team performances. For example, the estimated probability to be ranked among the first25 riders for the yellow jersey trophy calculated by"
606,halshs-01279855.training.tei.xml,51,"Torgler (2007) Despite the overwhelming evidence on doping, the Tour de France audience has not dropped over the years. A recent inquiry in Flanders (Van Reeth & Lagae, 2014) however, did show that most spectators do prefer ""clean"" performances in the Tour over doping-prone exceptional results. The study shows they are well aware of doping that plagues the Tour de France. But although they think that it is a problem that will never disappear completely from cycling and thus will continue to affect its credibility, cycling fans do not refrain from attending or watching it on TV, as another study ) has demonstrated. A new doping case during the Tour de France appears to have no significant immediate impact on average TV audiences of the Tour de France and triggers a small7.6% decrease in peak TV audiences. However, the impact of doping seems to have a stronger impact in the year following the unveiling of a major doping case, like the Festina case in 1998 or the Contador case in 2012. A significant 17.5% decline in average TV audience and a 7.7% fall in peak TV audience is recorded for the following Tour de France. This impact seems to temporary though since the effect only lasts for one year. Although apparently it has not strongly affected Tour attendance and TV audiences so far, doping may still be damaging the Tour success story in the long run. This requires new ideas on how to combat doping in cycling more efficiently."
607,halshs-01279855.training.tei.xml,52,"Anti-doping policy is still very much inspired by a traditional view on doping. In this vision, doping is considered to be fraudulent behaviour and therefore must be analysed based on the theory of the ""economics of crime"" (Becker, 1968), as it has been applied to sports (Bourg, 2000;Maennig, 2002)."
608,halshs-01279855.training.tei.xml,53,"Detection and sanctioning is then validated by an argument that runs as follows: a/ since it is forbidden by law, doping is fraudulent, and must be controlled; b/ since doping is profitable for an athlete in terms of wins and revenues, even it is forbidden athletes are prone to use it; c/ in order to stop doping, its costs to the athlete must be raised through stronger sanctions (fine, suspension, …) up to a level where it is no longer profitable to dope. In the end, an athlete makes the decision to dope if the expected net gain is positive, i.e. if the extra benefits (revenue, fame, …) that can be expected from the use of doping exceed the costs of using doping (actual costs and expected costs of sanctions if caught)."
609,halshs-01279855.training.tei.xml,54,"A new analytical approach to doping in sports economics relies on game theory. The starting point is to basically assume that riders dope themselves because it corresponds to their deeply rooted economic and financial interests, and not because they are bad by nature, professional cheaters or criminals.Two papers written in the wake of the Festina scandal (Eber & Thépot, 1999 andBerentsen, 2002) analyse doping as a result of a prisoner""s dilemma, one of the most famous problems in game theory. In a nutshell, the doping game is a prisoner""s dilemma adapted here to cycling where: a/ there is a (financial and health) cost borne by riders when they use doping; b/ the use of doping substances and techniques enables a rider to enhance his performances; c/ if, in an assumed two-rider race, both riders dope the final ranking remains unchanged compared to a situation without doping; and d/ if one of the two riders is caught, he will never denounce the other rider for being doped as well. Cycling the latter behaviour is known as omerta or the ""law of silence"": it is forbidden to openly talk about doping. In such a prisoner""s dilemma, it can be shown that doping is a dominant strategy which means that it is always much more profitable for both riders to dope. The game theoretic literature about doping has also proposed several new anti-doping policy recommendations, all focused on providing riders reduced incentives to use doping. Eber and Thépot (1999) advocate lower spreads between the prizes (which is in conflict with tournament theory), fewer races, improved testing and better prevention."
610,halshs-01279855.training.tei.xml,55,Berentsen (2002) suggests a new rank-based sanctioning mechanism.
611,halshs-01279855.training.tei.xml,56,"However, as long as the wrong incentives are given to riders also these solutions will be of little help in the fight against doping. We therefore suggest a revolutionary anti-doping regulation in cycling where the riders themselves lose interest in the use of doping, or even have an interest not to dope even when other competitors do. This can be realized by the use of appropriate incentives. The suggested scheme below is to some extent in line with Breivik""s (1992) views, and elaborates on the ""drug diary system"" put forward in Bird and Wagner (1997), and on Andreff (2012). The device would work as follows:"
612,halshs-01279855.training.tei.xml,57," At the start of the cycling season, all riders (in accordance with their doctors) must register compulsory the list of all doping substances and techniques they plan or, when necessary, want to use over the whole season. Such a doping diary is secret to the other riders. It could, for example, be deposited in the hands of each rider""s lawyer or with the UCI. Riders fully commit themselves to stick to the self-declared list of doping products.  If,during the season, a rider is tested positive for having taken products listed in his diary, no sanction applies.  However, if a rider is tested positive for having taken products not on this list, the sanction is extremely severe, for example a lifetime ban from professional cycling. A severe sanction is needed because not only did the rider use doping, he also cheated and blurred the transparency required for this scheme to be efficient. Because the rider does not want to comply with the rules of the game, he is excluded from professional cycling forever.  If a rider is performing extremely well, other riders who are suspicious (anyone of them, under the precondition that they are at leasttwo) are allowed to ask the rider (or his lawyer or the UCI) to unveil his doping diary in order to check whether the rider complies with it.  If it appears from (4) that the suspected rider does not comply with his doping diary, just like under (3) he is banned for life from professional cycling.  If it appears from (4) that the suspected rider does comply with his doping diary, the two or more claiming riders have to admit that the suspected rider either is definitely stronger than they are during this season or that his doping diary is more efficient or better programmed than their own."
613,halshs-01279855.training.tei.xml,58," As a result of this scheme, in particular at the beginning of such a regulation when there may be many claims against successful riders, other riders would have an incentive to copy the unveiled doping diary of the winners. This would triggerthree beneficial incentives. First, step by step a doping norm (diary) will prevail in the whole peloton, a typical story of social norm formation. All riders will start to use the same most efficient or best conceived doping programme. Second, because of this social norm formation, riders will soon realize that the differential and artificial competitive advantage of hidden doping is limited and short-lived. Third, the omerta system will vanish because the best doping programme will be publicly known to and consequently openly used by all riders once the most performing doping diaries of suspected winners are unveiled.  Once riders are convinced that doping becomes useless as soon as doping substances and techniques are no longer differential between riders, it will not be very difficult to convince them that a same optimal doping recipe does not imply ""the more the better""."
614,halshs-01279855.training.tei.xml,59,"Step by step, the optimal doping programme could be downgraded and freed from its most health threatening substances. In the long run, with the riders"" consent only the safest stimulating products, necessary in a hard and demanding sport such as cycling, would be maintained in the programme."
615,halshs-01279855.training.tei.xml,60,"Moreover, such anti-doping regulation features an incentive scheme where each rider is not interested in more and more doping for himself, but instead is interested in benefiting from a competitor""s decision to over-dope beyond his declared programme. To illustrate this point, imagine afour-rider racing circuit that operates under the above regulation. It will produce the following incentive scheme: He can expect a share increase though when competitors over-dope beyond their diary. The outcome is that each rider""s interest is to let the others over-dope and yield a higher share in wins from their exclusion. The incentive scheme is efficient and works in the right direction. Each rider becomes unwilling to over-dope or cheat whatever the others do and benefits in terms of wins from the wrong over-doping strategy possibly adopted by competitors. Doping thus shows a self-defeating strategy and will ultimately be rejected by most riders."
616,halshs-01279855.training.tei.xml,61,"Should one be optimistic about the above-suggested anti-doping regulation? For certain, it must not be considered to be a miracle solution. But the proposal at least focuses on a crucial point that could possibly be elaborated a bit further in forthcoming studies: the incentive scheme must convince riders that as soon as doping diaries are not differential between riders, doping is no longer a strategy that enables them to win more than their natural ability warrants. It may happen that there will be significant transition costs between the current anti-doping system and the one proposed here. In the first periods of the new regulation enforcement, some riders may feel or assume that the best performing doping programme is the most overloaded one. These riders might use substances dangerous to their health or life. If this assumption reveals to be true, they will soon be copied by other riders and lose their temporary comparative advantage. If on the contrary the assumption were to be wrong, or after a temporary increase in the number of sicknesses and deaths in the peloton (a sort of transition crisis between thetwo anti-doping systems, the hard price to pay for adopting the second one), all riders would be convinced that the best or optimal doping programme is in no way the most comprehensive or dangerous one in terms. It might be difficult though to convince stakeholders in cycling to enforce such a new regulation. Although riders would probably soon understand the mechanics of the embedded incentive scheme, they might be more concerned about the short-term effects of such a scheme on their health, i.e. when dangerous substances are put on the doping diary list (and subsequently used) by a large number of riders. But significant obstacles might also come from cycling governing bodies such as the UCI, Tour de France organisers ASO, and so on. They might especially fear the transparency about doping practices in the peloton that would result under such a regulation and its impact on the image (and thus the economic value) of cycling. Why not try to convince them as well that the good incentives that the proposed scheme creates are always preferable to the current bad incentives?"
617,halshs-01279855.training.tei.xml,62,"The Tour de France is one of the most popular and attractive sport events in the world. It attracts millions of people along the roads and reaches a global TV audience. This results from its nice design and appropriate management, the fact it is a product that is supplied for free, an acceptable fitness with the prerequisites of tournament theory, and a modern model of finance linking its sporting success with TV rights revenues. However, the Tour success story is not decisively associated with an excellent competitive balance. A new metrics of the latter, adapted to the team sport dimension of cycling, exhibits that each Tour de France is basically imbalanced while its dynamic competitive balance over the years is much better. Already for a long time but especially since1998 does the Tour de France has to deal with doping scandals and rider disqualifications, although so far this did not harm spectator and TV viewership interest for the Tour. However, the long term credibility of the Tour is still at stake. From this point of view, an innovative, more efficient and perhaps controversial antidoping regulation including incentives not to dope is developed here."
618,hal-01223150.training.tei.xml,0,"Cancer is a large group of diseases that could affect any part of the body, characterized by abnormal cell proliferation, and an increasing migration rate that could derive in invasion and organ spreading, becoming the leading cause of death all over the world. Thus, cancer is a critical societal and scientific problem. Great amounts of human and material resources are yearly spent in developed countries in attempts to understand its root causes and to develop successful prevention and treatment strategies."
619,hal-01223150.training.tei.xml,1,"Most of human cancers have acquiredsix basic capabilities : self-sufficiency in growth signals, insensitivity to growth-inhibitory signals, programmed cell death evasion, limitless replication potential, sustained angiogenesis, and tissue invasion, which could cause metastasis. In other words, the defense mechanism preventing each of these acquired capabilities must be thwarted before cells become a malignant and invasive tumor . Hence, it is necessary address new strategies for cancer understanding and treatment. For doing so, establishing correlation between observable phenomena, as well as, observing if a particular intervention produces a significant response could be very useful to state hypotheses postulating which physical processes are involved and how they interact. For example, tumor cell proliferation (observable phenomenon) can be correlated to oxygen concentration (another observable phenomenon). Experimentally speaking, if we would modify the oxygen availability to observe whether cell proliferation is significantly modified, then we could estimate the oxygen threshold over which cancer cells begin to proliferate. Biological experiments needed to test such hypotheses can be time-consuming, expensive and/or impossible with current technology. In these cases, mathematical modelling plays a key role, providing an independent check of the consistency of the hypothesis and can also improve experimental design by identifying which measurements are needed to test a particular theory, and additionally, whether new hypotheses can be established from experimental results (see the review paper by Byrne ). In this regard, mathematical modelling is a theoretical description of biological phenomena that may be calibrated by experimental data comparison. Moreover, by changing the parameter values of descriptive equations, the significance and functions of variables representing specific biological features can be easily tested. Then, practically all tumor growth features can be mathematically modelled, reducing the biological modelling complexity and offering a powerful tool to better understand tumor biology, facilitating drug development and also pre-clinical and clinical patient management."
620,hal-01223150.training.tei.xml,2,"In this work we describe the basic principles of mathematical modelling, as well as, suggest some experimental design in order to obtain relevant biological or clinical data necessary to estimate relevant parameters of mathematical modelling. We also define hybrid approach as the feedback capacity between mathematical modelling and biological experimental design which is required, as it has been shown in literature, for better understanding cancer, fitting parameters of a given model with a specific biological scenario, and for obtaining models with predictive capability. This work is not intended to deepen in the previous aspects, but it can serve as a first bridge of communication between applied mathematicians, biologist and physicians or even to be a tool to emphasize research in this field."
621,hal-01223150.training.tei.xml,3,"From a point of view of materials science, a cancerous tumor is a very complex, multiscaled material. The underlying genetic defects (nano-scale) affect cellular-level (micro-scale) properties, that in turn affect material properties at the tumor level (millimeter-scale). The larger scales may even feedback to the smaller scales. For example, a change in cellular stress may affect the type and occurrence of genetic mutations. Cancer can be studied at any of thethree scales as suggested by Zheng et al. and the references therein."
622,hal-01223150.training.tei.xml,4,"In this work we focus at the millimeter scale, i.e., tumor tissue level. Schematically,three successive stages can be identified in the growth of a solid tumor due to deregulation of cell division: initial avascular growth, angiogenesis and finally vascular growth, which in turn may induce metastatic spread. For a summary of models and main mathematical references discussed in this section, the reader is addressed to Table 1."
623,hal-01223150.training.tei.xml,5,"During the avascular growth phase, oxygen and other nutrients are delivered to the tumor cells, and the waste products are removed from the tumor via diffusion from nearby blood vessels; under this condition the tumor cells proliferate rapidly consuming more oxygen than the host cells. However, considering that healthy tissue has approximately 7 % oxygen (53 mmHg) tension and the diffusion distance of oxygen in tissue is ∼ 100 μm , tumor growth is limited in size  and maintained during a short period of time. Under these conditions, the neoplastic compartment rapidly exceeds the diffusion distance of oxygen and becomes hypoxic. Thus, oxygen tension in a tumor can range from physiological (7 %) to severe hypoxic (< 1 % oxygen) or even anoxic . Moreover, the immediate molecular response to low oxygen is the hypoxia-induced factor (HIF) protein stabilization , which in turn triggers the expression of target genes involved in hypoxia adaptation such as vascular endothelial growth factor (VEGF). In addition, under severe hypoxia, it is found surrounding areas of necrosis, which is a common characteristic of solid tumors. Nevertheless, quiescent cells generation in the periphery of culture dish was observed in vitro during a nutrient-deficient medium and hypoxia exposure in tumor cells . Therefore schematically, in the hypoxic phase, a tumor grows within the limits of its local environment forming three characteristic layers: 1) cells towards the center, deprived of vital nutrients, will die and give rise to a necrotic core; 2) proliferating cells can be found in the outer cell layers; and 3) a layer of quiescent (or hypoxic) cells, which survive without dividing with slow metabolism, is found between the two others layers. In summary, tumor growth is severely restricted and in order to continue its developing needs to find additional nutrient sources."
624,hal-01223150.training.tei.xml,6,"As a result of hypoxic pressure some tumor cells secrete a number of diffusive chemical substances-called tumor angiogenic factors (TAF), such as VEGF, into the surrounding tissue, which encourage the body to vascularize the tumor and therefore provide new nutrients. Indeed, transition from the avascular to the vascular state, depends on the tumor ability to induce new blood vessels formation from the surrounding tissue. These blood vessels sprout towards the tumor and then gradually surround and penetrate it, providing an adequate micro-circulation and blood supply. Tumor-induced angiogenesis, the process by which new blood vessels develop from an existing vasculature, through sprouting, proliferation and fusion of endothelial cells, therefore is a critical step in solid tumor growth. Under hypoxia, cancer stem cells (CSC), that reside inside the tumor, could differentiate toward endothelial progenitor cells and mature endothelium, which in turn generates new blood vessels inside the tumor . This last process is called neovasculogenesis, and in the past was thought to happen only intrauterine, but nowadays it is known that this event happens also in adulthood. In cancer field, presence of CSC has been associated with tumor recurrence, resistance to chemotherapy, tumor metastasis, and in general with poor clinical prognosis . Tumor-induced angiogenesis is believed to start when a small avascular tumor exceeds a critical diameter (∼ 2 mm), above which normal tissue vasculature is no longer able to support its growth. By the time a tumor has grown to a size whereby it can be detected by clinical means, there is a strong likelihood that it has already reached the vascular growth phase (see [10] and the references therein)."
625,hal-01223150.training.tei.xml,7,"Tumor-induced angiogenesis is characterized by a chaotic tumor vessels development associated with both angiogenesis and vasculogenesis. Thus, endothelial cells proliferate and capillaries are rapidly formed allowing tumor growth, but in cancer framework theses processes are deregulated. Indeed, vessel diameter isfive times bigger than in normal tissue , rarely differentiated into arterioles or venules, with frequently blind endings, and incomplete and abnormal endothelial cell lining . All these abnormalities generate an irregular blood flow , perpetuate the intermittent low oxygen delivery, increase HIF activity and promote pro-angiogenic signals generation ."
626,hal-01223150.training.tei.xml,8,"Over the past decade much work has been performed to understand the angiogenic process. For instance, it is well-known that increased density of blood vessels (the socalled ""hot spots""), and high VEGF plasma levels are a powerful prognosis tool in many human tumor types. In fact, the capacity to modify this process has been considered as a keystone for cancer treatment, which includes some molecules that reduce the ability of TAF, and in particular of the VEGF, to provide blood supply toward the tumor and thus controlling tumor growth. Indeed, a recent meta-analysis  including 24 randomized trials with 8 different types of cancers, in which the synthetic antagonist of VEGF (Bevacizumab®) was used in combination with chemotherapy, shown a statistically significant improvement in the overall survival and progression of free cancer survival in patients who received Bevacizumab® compared with those who did not receive this drug. These beneficial effects were more evident in patients with colon cancer and renal cell carcinoma, but less evident in those who had breast, pancreatic or prostate cancer. This suggests that despite the overall benefit of Bevacizumab®, some patients and types of cancer are more resistant to antiangiogenic therapy. A possible explanation to this phenomenon is the presence of CSC, whose differentiation could occur independently of VEGF ."
627,hal-01223150.training.tei.xml,9,"Altogether these last evidences indicate that angiogenesis during tumor growth is a complex process which study demands the use of many experimental approaches, and actually experimental analysis is in general expensive and/or difficult to be carried out and moreover no experiment can fully explain this process. For these reasons, mathematical modelling might theoretically combine a broad range of biological events for giving a better vision of the overall tumor progression including angiogenesis."
628,hal-01223150.training.tei.xml,10,"Since the seminal work of Greenspan, the mathematical modelling of avascular solid tumor growth has been rapidly expanding. Most models in this area consist of nonlinear partial differential equation systems (e.g. see References [25][26][27][28]), and may be described as a macroscopic approach. In this work we will focus on tumor-induced angiogenesis and on some of the key aspects of vascular tumor growth, so we will omit a bibliographical discussion about avascular tumor growth in this manuscript, but we refer to exceptional work [3,[29][30][31] and the references therein."
629,hal-01223150.training.tei.xml,11,"Modelling tumor-induced angiogenesis has a well-established history beginning with the work of Balding and McElwain. These authors proposed a simple model of tumor angiogenesis to describe experiments in which tumor cells implanted in the rabbit cornea stimulated formation, growth and migration of new blood vessels from the corneal limbus to the tumor . Since the pioneer work of these authors , much of the mathematical modelling has focused on the way in which TAF initiate and coordinate capillary growth . The next phase in tumor development, namely vascular tumor growth, has received less attention than avascular growth and angiogenesis in mathematical modelling literature. See also [34,35] for early work on vascular tumor growth and invasion."
630,hal-01223150.training.tei.xml,12,"In order to show some examples in which mathematical modelling combined with biological experiments have contributed to a better cancer understanding, we will highlight some selected results centered on targeted therapies efficacy. In the papers [36][37][38], the authors studied multiscale models to address questions related to prediction of chemotherapy and radiotherapy efficacy. Likewise, Lignet et al. and Panovska et al.  developed models to investigate the combined effect of anti-proliferative therapy with anti-TAF or anti-vascular therapy. Additionally, Billy et al.  studied the efficacy of a new anti-angiogenesis treatment and provide some indications about the best way to optimize this cancer treatment strategy. In our very recent work , we dealt with tumor drug resistance modelling in hepatic gastrointestinal stromal tumor (GIST) metastases, which exhibit resistance to two standard treatments: imatinib and sunitinib. Based on an accurate analysis of medical images, we provided a patient-dependent model that reproduces qualitatively and quantitatively spatial tumor evolution, as shown the followup clinical data that we have carried out. Interestingly, specific aspects of tumor growth as spatial heterogeneity and treatment failures could be explained by our model. See Section Looking for a hybrid approach."
631,hal-01223150.training.tei.xml,13,"Additionally, also concerning the combination of mathematical modelling with clinical data, another recent work from our group described the basic principles of mathematical modelling, as well as, the advantages, limitations and future prospects using both oncology imaging and modelling. Combination of imaging and modelling can resolve complex problems and describe many aspects of tumor growth or response to treatment, and therefore nowadays is possible to consider its clinical use in the medium term. Accordingly, in the present work we also describe some aspects of tumor growth modelling giving more details and, at the same time, we suggest some biological experiments, as well as, the way of combining them to provide new insights in tumor progression, angiogenesis and response to treatments. To do this, in the next section we focus on deterministic models which analyze tumor growth as a macroscopic mechanical process involving the whole tumor tissue rather than analyzing individual cells, in the same way as models presented so far in this section. In addition, in the Section Looking for a hybrid approach, we give some insights about how mathematical modelling should be modified for fitting with the biological scenario of a specific cancer."
632,hal-01223150.training.tei.xml,14,"Taking into account the complexity of the processes involved in all the stages of tumor growth, it is not difficult to understand that mathematical models are to a large extent basically phenomenological and simplified compared to what is happening in a biological context. Roughly speaking, from a biological point of viewtwo main types of modelling can be distinguished: those that only consider the tumor as a whole, and those that also consider the spatial distribution of tumor components ."
633,hal-01223150.training.tei.xml,15,"The key challenge is to develop mathematical models for tumor progression including the aspects mentioned above, in which optimal combination of drugs is simulated in order to suggest the best protocols for improving clinical outcomes. In addition, these models may be used to shed light about the existence of new mechanisms that could explain phenomena clinically observed and may have predictive capability. To do this, parameter estimation of these models is a key issue for which performing experiments or collecting clinical data are required. Then, in this work we want to emphasizetwo main aspects; the first one is the mathematical modelling of tumor progression; and the second one is how data coming from experimental biology or clinical data may contribute to parameter estimation."
634,hal-01223150.training.tei.xml,16,"We present below some simple models considering only the change in volume at which tumor growth occurs, without taking into account tumor environment. We focus on models based on ordinary differential equations (ODE). To do this, we borrow some ideas by Byrne. See Appendix A for a more detailed discussion."
635,hal-01223150.training.tei.xml,17,"One of the simplest models that can be used to describe the way in which the number of cells N(t) within a solid tumor changes over time is the exponential growth law. In this model, there are no constraints on cell growth: all nutrients and other vital growth factors are assumed to be available in abundance. In consequence, the model predicts that the population will increase exponentially, without limit. See Fig. 1."
636,hal-01223150.training.tei.xml,18,"Whilst the exponential growth law is not realistic enough, it provides an accurate description of the early stages of a tumor development. In particular, reduced growth and eventual saturation are observed when avascular tumors are grown in vitro or when vascular tumors develop in vivo. This discrepancy arises because as the tumor increases in size, competition for nutrients and other vital resources, such as space, can no longer be neglected. A simple modification of the exponential growth law which takes account of competition for resources (without specifying what those resources are) is the logistic growth law."
637,hal-01223150.training.tei.xml,19,"Whilst the logistic growth law predicts almost exponential growth of small tumors and growth saturation when the tumor reaches its carrying capacity (see Fig. 1), the symmetry of N(t) about its point of inflection means that it is not particularly flexible to fit or describe experimental data. A more general family of curves, which, depending on the choice of a parameter α can saturate more or less fastly than the logistic growth law, is given by where k >0 represents the net rate at which tumor cells proliferate and θ > 0 denotes the carrying capacity. We remark that the logistic growth law is a special case of (1) (set α = 1) and that the Gompertzian growth law is recovered in the limit as α → 0 + , which states that the net proliferation rate of tumor cells exponentially decreases with time."
638,hal-01223150.training.tei.xml,20,"In order to compare thefour models presented above, we plot in Fig. 1 growth curves for each model for fixed values of the proliferation rate k and the carrying capacity θ."
639,hal-01223150.training.tei.xml,21,"These models are too simple to provide useful, reliable and reproducible information about the tumor from the phenomenological change in its volume, which varies between different tumors and patients. They may, however, be made more complex to incorporate tumor cell heterogeneity (coming from the cell cycle) or to take into account different processes such as angiogenesis; see Reference [43]."
640,hal-01223150.training.tei.xml,22,"In order to get models with predictive capability an important issue is parameter estimation, which has to be performed for fitting modelling with experimental or clinical data. For example, estimating parameters k and θ for the logistic model (see (13) in appenA) is roughly made by minimizing the error"
641,hal-01223150.training.tei.xml,23,"wherê N i , i = 1,. .. , M are M experimental or clinical measurements collected at different times, say t i (i = 1,. .. , M), during the tumor evolution, corresponding to the observed numbers of tumor cells at the time instants t i , and N(t i ) are the numbers of tumor cells predicted by the model at time instants t i. The minimization of the error E(k, θ) (2) is made in this case with respect to the parameters k and θ in such a way that the responses of the model N(t i ) approximately match the datâ N i. When the minimum of (2) is reached for some ( ˆ k, ˆ θ), the error E( ˆ k, ˆ θ) is called the least-squares error whereas ( ˆ k, ˆ θ) is known as the least-squares estimate of the parameter (k, θ). It is worth noting that the optimization process can be quite complicated depending on the model, the number of parameters and of the availability of data; the interested reader is referred to the textbook."
642,hal-01223150.training.tei.xml,24,"Classically the mathematical models used for clinical applications are based on systems of non-linear ODE, which do not consider the spatial aspect of tumor growth. Despite of it, these models have great interest in biological applications and typically they are parametrized using statistical methods and may provide a prognosis of tumor volume, but neither shape nor location of tumor can be estimated. See references [46,47]."
643,hal-01223150.training.tei.xml,25,"In order to appreciate the impact of parameter estimation in the clinical context, for example we refer to a recent meta-analysis which describes natural development of meningiomas, a kind of neurological tumor. In [48], 22 studies reporting 675 patients with untreated meningiomas were found, followed by serial magnetic resonance imaging (MRI) during 5 years. From the analysis, authors show that tumors which initial diameter was > 2.5 cm exhibit a linear growth rate > 10 % per year, which leads to the highest risk for developing progressive symptoms. By contrast, untreated meningiomas which initial diameter was < 2.5 cm do not show tumor growth over a follow-up period of 4.6 years. Then, the finding of the parameter 2.5 cm as a threshold for meningiomas in vivo allows to describe tumor biologically more aggressive and classify patients who might require or not surgery, which may avoid excessive intervention."
644,hal-01223150.training.tei.xml,26,"We describe below generic modelling, based on partial differential equations (PDE), which consider the effect that changes in the composition of the medium surrounding the tumors have on their growth. We focus on the study ofthree key issues: vascular tumor growth, tumor-induced angiogenesis and invasion, and efficacy of treatments such as anti-proliferative and anti-angiogenic therapies. See Appendix B for a more detailed discussion."
645,hal-01223150.training.tei.xml,27,The most basic principle for all quantitative models is conservation of mass. Conservation of mass of a component in a dynamic and open system states that:
646,hal-01223150.training.tei.xml,28,Net rate of change of mass of component in the system
647,hal-01223150.training.tei.xml,29,Rate of production of the component by transformations
648,hal-01223150.training.tei.xml,30,"The local mass balances are the mathematical form of equality, which can be written as"
649,hal-01223150.training.tei.xml,31,"where t is time; C is the concentration of the component in the system; J is the mass flux of the component; and r is the net production (or growth) rate of the component. This is the equation of continuity for a component, either tumor cell densities or chemical concentration of a growth rate limiting (e.g. oxygen or glucose), or of TAF, or of drugs against cancer. The transport processes that regularly are considered in cancer models are advection (for cell densities) and molecular diffusion (for chemical concentrations). The general expression to model the specific mass flux of a component is"
650,hal-01223150.training.tei.xml,32,where v is the velocity field due to the mechanical forces which act on the system (e.g. the pressure within the tumor) and D is the diffusion coefficient.
651,hal-01223150.training.tei.xml,33,"We assume that transport of tumor cells is driven only by advection. Consequently, applying the principle of mass balance, the spatiotemporal dynamics of cell populations are formulated as the generic PDE:"
652,hal-01223150.training.tei.xml,34,"where r(f ) is the net growth rate of the cell density f , which depends on the cell population considered, and ∂f ∂t stands for the partial derivative with respect to time (representing the rate of change of the mass density f ). For example, if f represents the proliferating tumor cells density, designed as f = P, then"
653,hal-01223150.training.tei.xml,35,"In replacing r(P) in (5), one arrives to"
654,hal-01223150.training.tei.xml,36,"This formula is called a reaction-advection equation, because of the presence of the advection term ∇ · (vP) and the reaction term P − δP. The term P represents the net tumor growth rate, which is determined by local oxygen concentration. For example"
655,hal-01223150.training.tei.xml,37,"where C represents the local oxygen concentration, C hyp is a parameter which describes the sensitivity of cells to hypoxia and γ 0 stands for the maximal proliferation rate. is chosen according to (8) in such a way that it satisfies the following properties: → γ 0 as C C hyp , i.e. cells proliferate as the oxygen concentration is high enough, and → 0 as C C hyp , i.e. cells do not proliferate as the oxygen concentration is low enough (hypoxia). Moreover R >0 is a numerical smoothing parameter, in such a way that be a smooth version of a Heaviside function. The term δP represents the necrosis due to a generic anti-proliferative drug, where the parameter δ represents the doses of such drug. Eq. 7 is a basic form for modelling tumor proliferation and obviously there are others terms which we could add, for example, a term taking into account the passage to quiescence (see Appendix B)."
656,hal-01223150.training.tei.xml,38,"Similarly, we assume that transport of chemical concentrations is driven only by diffusion. Consequently, applying the principle of mass balance, the spatiotemporal dynamics of chemical concentrations are formulated as the generic PDE:"
657,hal-01223150.training.tei.xml,39,"where r(g) is the reaction term of the chemical concentration g. This formula is called reaction-diffusion equation. For example, the local oxygen concentration is given by:"
658,hal-01223150.training.tei.xml,40,"where D C stands for the diffusion coefficient of local oxygen in the blood flow, h C is the production of oxygen in the functional blood vessels, and g C is the consumption of oxygen mainly due to proliferation tumor cells. Likewise, the anti-proliferative drug concentration is given by"
659,hal-01223150.training.tei.xml,41,"where D M is the diffusion coefficient of anti-proliferative drug in the blood flow, λ is the mean-life of such drug, a is the consumption rate due mainly to proliferation tumor cells, ρ represents the blood flow, and finally δ M denotes the doses of the drug administrated at different time intervals. Similar equations also hold for the blood flow ρ, the anti-angiogenic drug concentration and the TAF concentration, particularly the VEGF. Mathematical modelling described above relies on systems of non-linear PDE, in which a set of parameters takes into account the complexity of the underlying biological phenomena. In order to apply such models in practical situations, these parameters need to be identified, that is to say, biologically meaningful values have to be estimated. One way to determine their values is by means of inverse problems theory, exploiting data coming from medical images, as achieved in References [50,51]. One of the main difficulties is that the amount of data for system identification is scarce. Although medical scans allow an accurate localization of tumors in space, little information can be inferred regarding cellular nature or nutrient distribution inside tumors. In addition, usually onlytwo scans are available before treatment, which makes estimation of tumor evolution a challenging problem."
660,hal-01223150.training.tei.xml,42,"On the other hand, retrieving tumor shape evolution may provide useful information since PDE based models are spatially distributed. We will return to this important issue in the next section. One possible approach to formulate the inverse problem is by optimal control theory, as was done for instance in [50]. In this approach a PDE-constrained optimization problem has to be solved. For instance, one can consider a functional to be minimized by matching spatiotemporal evolution of tumor density predicted by mathematical modelling with the corresponding tumor density maps estimated from serial scans for one particular patient. In order to carry out the minimization process an adjointbased algorithm is used for evaluating the gradient of the functional. This algorithm is expensive, since each optimization iteration requires solving a number of forward problems equal to number of variables. A different approach, as was done for instance in [51], consists in using the difference between a variable predicted by the model and its corresponding observed value (the residuals of the model) within a Newton method to solve the inverse problem. This identification procedure is based on proper orthogonal decomposition (POD), a way to identify complex natural processes with models that are intrinsically much simpler than biological scenarios. In this approach the solution is sought in a given low-dimensional functional space, which basis gives an optimal representation of a sufficiently large number of solution samples. In particular, the parameter space is sampled in such a way that all the possible different biological behaviors are represented."
661,hal-01223150.training.tei.xml,43,"We finish this section by showing some results which are part of a work in progress. We have carried out some numerical simulations of a tumor growth model under normal conditions, i.e., in absence of treatments. These simulation results are depicted in Fig. 2, which shows the distribution of endothelial cells (right), as well as, the distribution of the sum of the proliferative and quiescent tumor cells (left). The time unit is12 hours. Because of the configuration (four blood vessels initially placed far from tumor cells), the proliferative and quiescent tumor cells initially decrease, which can be clearly seen in Fig. 2. At the same time, this would produce an increment of the VEGF in the region where the quiescent cells are concentrated. This would trigger the proliferation of endothelial cells making them to move by chemotaxis towards the tumor (see Fig. 2). Further, as a result a lack of production in VEGF may be observed which it is thought to be consumed by endothelial cells. At this stage, the main effect is the reactivation of the proliferation of peripheral tumor cells due to high levels of nutrients in a neighborhood of the new blood vessels. However, after some time the oxygen concentration would be consumed by proliferative cells to levels that produce the usual distribution in three layers of tumor cells (see Fig. 3) described in the schedules of fully developed tumors . Accordingly vascular collapse and regression in tumors has been suggested , and it is thought that is a consequence of the biomechanical stresses and of the action of the interstitial pressure ."
662,hal-01223150.training.tei.xml,44,"Parameter estimation is highly required in order to improve models for tumor progression and provide new clinical insights. In this regard, we have defined hybrid approach as the feedback capacity between mathematical modelling and biological experimental design. This approach may lead to a better understanding cancer, by fitting mathematical parameters with biological processes, in order to achieve accurate biological predictive models. In this section we will givetwo examples which illustrate the hybrid approach."
663,hal-01223150.training.tei.xml,45,"The first example is concerned with tumor cell proliferation. As mentioned in Section Introduction, tumor cell proliferation can be correlated to oxygen concentration, and indeed one could think that the higher the oxygen concentration the higher the tumor cell proliferation. However, this is not always like that. In particular, elevation of anaerobic metabolism in tumor cells during avascular or even vascular tumor growth has been described, a phenomenon called Warburg effect, which involves a shift in metabolism away from oxidative phosphorylation (i.e., aerobic) towards anaerobic glycolisis. Therefore, tumor cells are resistant to hypoxia which indeed, depending on the threshold, may stimulate cell proliferation. In this regard, unpublished results from our lab using two ovarian cancer cell lines, HEY and UCI, were cultured under two different oxygen concentrations, 21 % (normoxia) and 5 % oxygen (hypoxia) respectively. Under these conditions, UCI cells were significantly more sensitive to oxygen variations than HEY cells. Indeed under 21 % oxygen UCI cell proliferation was accelerated about 3 times compared to 5 % oxygen, whereas in the case of HEY cells, the proliferation did not significantly change from both different oxygen concentrations (see Fig. 4). Considering these last results into . Data is presented as the logarithm of t 50 (replication time) ± SEM. N = 4 per group and analyzed time. *p < 0.05 vs HEY at 21 % oxygen. **p < 0.05 vs UCI at 21 % oxygen the mathematical modelling described before, net tumor growth rate in Eq. 8 need to be remodelled. In Fig. 4, it is clearly seen that UCI cells behave as one would expect, i.e., the higher the oxygen concentration the higher the tumor cell proliferation; in contrast, no matter the oxygen concentration, HEY cells undergo the same proliferation rate. Consequently, whereas net proliferation rate for UCI cells behaves as function defined by (8), one should modify the equation for HEY cells. In addition, parameters C hyp and γ 0 could be estimated using the same experimental setting."
664,hal-01223150.training.tei.xml,46,"The second example is concerned with tumor drug resistance in gastrointestinal stromal tumors (GIST) liver metastases. In our very recent work, we have modelled and simulated resistance to two standard treatments: imatinib and sunitinib. In this regard, the specific tyrosine kinase inhibitors, imatinib, is used as standard first-line treatment. In most of the cases this drug leads to tumor reduction during several months, but then, most of the patients relapse and the tumor is no longer responding to treatment. Then the standard care switches to a second-line treatment using a multi-targeted tyrosine kinase inhibitor (sunitinib), which has both cytotoxic and antiangiogenic effects. After use this second drug, metastasis is controlled for some additional time before a new therapeutical failure occurs. Considering this clinical data, we have splitted the density P of proliferative cells (P-cells) into 3 subpopulations P 1 , P 2 and P 3 , such that P = P 1 + P 2 + P 3 , where"
665,hal-01223150.training.tei.xml,47,"• P 1 denotes the fraction of proliferative cells that are sensitive to the first-line treatment, based on imatinib molecule, and also to the second-line treatment, based on sunitinib;"
666,hal-01223150.training.tei.xml,48,• P 2 describes the density of proliferative cells that are resistant to the first-line treatment and sensitive to the second-line treatment; and • P 3 stands for proliferative cells that are resistant to both treatments.
667,hal-01223150.training.tei.xml,49,"According to the clinical observations, it seemed relevant to consider that thethree cell sub-populations are present when the GIST metastasis is detected. In the mathematical models, we describe tumor growth evolution in terms of tumor area, as well as, its spatial structure and compared with the CT-scans measurements. We believe that intra-tumoral spatial heterogeneity may be related to an increase in the cellular activity, which would mean that a resistant phenotype should be emerging. Indeed according to our modeling, such heterogeneity may be seen as the first stage of the treatment failure, while Response Evaluation Criteria In Solid Tumors (RECIST) do not bring any information about this fact. Therefore our work can be seen as a first step in developing new tools to evaluate tumor response to treatment based on tyrosine kinase inhibitors. Consequently, as mentioned in the previous section, retrieving the evolution of the tumor shape may provide useful information for clinical applications."
668,hal-01223150.training.tei.xml,50,"According to the discussion above, data coming from clinical observations were crucial in order to develop a complex model that takes into account tumor heterogeneity and treatments resistance. In addition, our model parameters were calibrated in a patientdependent way, in particular, estimating parameters fortwo patients and fitted with their specific tumor evolution. Consequently, in this example, mathematical modelling discussed before (in particular Eq. 7 among others) was adapted from cross-talk between physicians and mathematicians based on clinical observations, leading to a new modelling capable to provide better insights for enhancing clinical meaning of mathematical modelling."
669,hal-01223150.training.tei.xml,51,"In this work we described and discussed mathematical modelling of tumor progression, including some aspects as vascular tumor growth, angiogenesis and effectiveness and/or resistance to therapies. We also emphasized what we have called hybrid approach in which inverse and progressive confirmation by biological experimentation and use of data from patients with cancer is suggested for enhancing clinical meaning of mathematical modelling. We believe that hybrid approach is a key tool for better understanding cancer, which has been illustrated bytwo specific examples carry out by our group."
670,hal-01223150.training.tei.xml,52,"Hybrid approach also involves a big challenge for researchers in cancer field, since despite cancer is deeply analyzed from the particular perspective in each disciplines, when mathematical modelling and biological research face each other, an easier and common language is required in order to go beyond understanding of the pathology. This is not easy to deal with, because there are many differences in the expertise, nomenclature, language and even in the way of thinking that could slow down the communication. Then,one of the main challenges to be overcame is listening and learning each other, maintaining academic discussions where we could bridge the gap on these differences. With hope we have seen a growing number of publications and working groups which are seeking to combine science, but it is still insufficient. In addition, at the same time another step that can be done is the inclusion of physicians in this discussion/analysis in order to look forward potential clinical interest and application of the generated knowledge from basic sciences. These interactions not only will enhance knowledge of cancer, but also improve clinical meaning of mathematical modelling."
671,hal-01223150.training.tei.xml,53,"Parameter estimation in mathematical modelling may ask very specific questions that need to be designed and tested in vivo or in vitro. These questions might constitute a challenge for biologist due to limitation in available technology, and also because some parameters are not easy to determine or even not necessarily represent an in vivo system, but they are only associated with. For instance, reaction-diffusion Eq. 10 models the balance between variation, diffusion, production and consumption of local oxygen. Despite in mathematical modelling constitutes a usual equation, in biological terms series of experimentation are required in order to analyze how oxygen is delivered towards tumor, how is diffused from vessels to tumor, how tumor cells may control blood flow and oxygen delivery, among others. In particular, future directions should focus in a suitable design of in vitro or in vivo models for answering such mathematical questions. In this regard, another experimental setting we would like to suggest is isolation of endothelial cells derived from patient's tissue after biopsies, as it has been already previously developed. This particular cell type might be used for testing specific questions such as how and how much VEGF released from tumor cells may enhance tube formation capacity of that endothelium. Interestingly, anti-angiogenic drugs could also be tested using this experimental setting, which results might be used for parameter estimation."
672,hal-01223150.training.tei.xml,54,"Future directions in tumor growth modelling should include a cross-talk between biological and theoretical researchers in order to unify effort and generate an active interaction between them. A more common language and understanding each other will be helpful in order to enhance generation of high impact knowledge in tumor growth. In this regard, combination of in vivo and in vitro experimentation with theoretical analysis is a major challenge which hopefully is changing. In this manuscript we intend to contribute in this field, which of course is limited in comparison with what is needed to be done in order to achieve a better understanding of tumor growth, but it constitutes an example which may alert both mathematicians and clinicians."
673,hal-01223150.training.tei.xml,55,In this appendix we give some details about of the non spatially-structured models of tumor growth based on ordinary differential equations.
674,hal-01223150.training.tei.xml,56,One of the simplest models that can be used to describe the way in which the number of cells N(t) within a solid tumor changes over time is the exponential growth law which states
675,hal-01223150.training.tei.xml,57,"whose solution is N(t) = N 0 exp(kt). In Eq. 12, k >0 represents the net rate at which the cells proliferate, and N 0 denotes the number of cells initially present within the tumor. In this model, there are no constraints on cell growth: all nutrients and other vital growth factors are assumed to be available in abundance. In consequence, the model predicts that the population will increase exponentially, without limit. Whilst the exponential growth law is not realistic enough, it provides an accurate description of the early stages of a tumor development. In particular, reduced growth and eventual saturation are observed when avascular tumors are grown in vitro or when vascular tumors develop in vivo. This discrepancy arises because as the tumor increases in size, competition for nutrients and other vital resources, such as space, can no longer be neglected. A simple modification of (12) which takes account of competition for resources (without specifying what those resources are) is the logistic growth law"
676,hal-01223150.training.tei.xml,58,By using elementary calculus one arrives to
677,hal-01223150.training.tei.xml,59,"In (13), θ >0 represents the carrying capacity of the tumor cells population. Whilst the logistic growth law predicts almost exponential growth of small tumors and growth saturation when the tumor reaches its carrying capacity (N = θ), the symmetry of N(t) about its point of inflection (where d 2 N dt 2 = 0 and N = θ/2) means that it is not particularly flexible to fit or describe experimental data. A more general family of curves, which, depending on the choice of a parameter α can saturate more or less fastly than (13), is given by (1), which solution is"
678,hal-01223150.training.tei.xml,60,We remark that the logistic growth law is a special case of (1) (set α =1) and that the Gompertzian growth law is recovered in the limit as α → 0 +. Gompertzian growth law is given by
679,hal-01223150.training.tei.xml,61,whose solution is
680,hal-01223150.training.tei.xml,62,The Gompertzian growth law states that the net proliferation rate of tumor cells exponentially decreases with time. This is apparent because of that
681,hal-01223150.training.tei.xml,63,and noting that k exp(kt) ln θ
682,hal-01223150.training.tei.xml,64,"In order to compare thefour models presented above, we plot in Fig. 1 growth curves for each model for fixed values of the proliferation rate k, the carrying capacity θ and the initial condition N 0 ."
683,hal-01223150.training.tei.xml,65,"In this appendix we give some details about of the spatially-structured model of tumor growth based on partial differential equations, from which we have obtained the simulation results depicted in Figs. 2 and 3. It is worth noting that this is a work in progress, therefore the previous simulation results are not yet definitive. Moreover, we have not included yet in this model the influence of drugs."
684,hal-01223150.training.tei.xml,66,"Our model consists in a system of partial differential equations of type advectiondiffusion-reaction. This system describes the evolution of the cell densities and of the molecules (oxygen and VEGF), as well as, the interaction between them. The rules that regulate such interactions are, in general, non-linear expressions of the coefficients in the equations. All of these variables undergo temporal dependency (noted by t) and spatial x). However, to simplify the notation, such a dependence will be omitted and by instance P shall be written instead of P( x, t)."
685,hal-01223150.training.tei.xml,67,"The notations used in the model are shown in Table 2, and the full model proposed for the evolution of tumor growth coupled with tumor angiogenesis is shown in Table 3."
686,hal-01223150.training.tei.xml,68,"In above equations, represents the specific tumor proliferation rate, given by Eq. 8; f QP the specific rate of transition from the quiescent to the proliferative state; conversely, f PQ the specific rate of transition from the proliferative to the quiescent state; f PN and f QN represent the specific rate of necrosis, from the proliferative and the quiescent state, respectively. These transition functions are given by:"
687,hal-01223150.training.tei.xml,69,"where C sev hyp is the severe hypoxia threshold, that is to say, the oxygen concentration under which proliferative tumor cells die, and similarly C nec is the oxygen concentration under which quiescent tumor cells die. One obviously has that0 < C nec < C sev hyp < C hyp to take into account the fact that it is more difficult the quiescent tumor cells die."
688,hal-01223150.training.tei.xml,70,"The transport of endothelial cells, which are responsible by tumor vascularization, is mainly driven by chemotaxis toward the source of VEGF. This transport term is modelled by g CH = χ ρ ∇α,"
689,hal-01223150.training.tei.xml,71,whereas the source of endothelial cells is given by
690,hal-01223150.training.tei.xml,72,"In Eq. 23, χ denotes the sensibility to the chemotaxis, which is given by"
691,hal-01223150.training.tei.xml,73,"with χ chemo is the maximum effect of VEGF on endothelial chemotaxis; χ prol is the maximum effect of VEGF on the proliferation of endothelial cells; α half is a half-proliferation constant, i.e., it denotes the VEGF concentration such that the proliferation of endothelial cells is exactlyhalf of its maximum (0.5 χ prol ). For the oxygen concentration C the source and the consumption are given respectively by:"
692,hal-01223150.training.tei.xml,74,"where λ C is the oxygen half-life, ε C is the consumption rate by the proliferating cells, β C is the consumption rate by the quiescent cells, C sou the maximum oxygen level available in functional blood vessels and C max the maximum oxygen level in the host tissue. In a similar way, the source and consumption terms for the VEGF concentration are given respectively by: (27) where λ α is the VEGF half-life, α sou is the source of VEGF located at quiescent tumor cells, α max the maximum VEGF concentration and π denotes the binding rate of VEGF to the receptors of the endothelial cells."
693,hal-01223150.training.tei.xml,75,We have assumed that the total number of cells per volume unit is constant (and without loss of generality equal to1):
694,hal-01223150.training.tei.xml,76,"Adding the equations for the cell densities P, Q, S, N and ρ (see Table 3), we get:"
695,hal-01223150.training.tei.xml,77,"Thus by imposing the Darcy's law v = −k∇ϕ for the advection velocity, where k is the permeability of the medium, we deduce the equation for the pressure ϕ (see Table 3)."
696,hal-01223150.training.tei.xml,78,"Finally, we have noted D j the diffusion coefficients for the different molecules j =1, 2 (oxygen and VEGF):"
697,hal-01223150.training.tei.xml,79,"where D j,max is the maximum diffusion coefficient of the molecule j and ε >0 is a parameter that represents the percentage of diminution of the diffusion within the tumor, due to the higher density of this last one with respect to the host tissue. For the boundary conditions, we assumed that the molecules will not go out or in through of the boundary of the computational domain, that is to say, we assumed homogeneous Neumann boundary conditions for C and α."
698,hal-01223150.training.tei.xml,80,"For the pressure field ϕ we imposed ϕ| ∂∂ =0. This homogeneous Dirichlet condition is used since we consider that the domain of interest is not isolated, and the outer medium does not impose a pressure on the tumor. This assumption is valid for small tumors that are not mechanically constrained by the extratumoral region."
699,hal-01223150.training.tei.xml,81,"Finally, Table 4 below shows the parameter values for which simulation depicted in Figs. 2 and 3 was carried out. In Table 4 we use the following notations: h(hour), mm(millimeter), M(molarity)."
700,hal-01223150.training.tei.xml,82,"The work of P. C. was founded by PIA-CONICYT grant PFBasal-01, DIUBB GI 153209/C and DIUBB GI 152920/EF. The work of A. C. was founded by DIUBB 121909 GI/C. C. C. thanks partial support from Ecos-Conicyt C13E05. He is also partially supported by PFBasal-01 and PFBasal-03 projects and by Fondecyt 1140773. The work of C. E. was founded by Fondecyt 1140586, Fondequip EQM140104, DIUBB GI153109/EF, GI 152920/EF. We would like to acknowledge Dr. Gareth Owen"
701,hal-01223150.training.tei.xml,83,The authors declare that they have no competing interests.
702,hal-01223150.training.tei.xml,84,"Authors' contributions PC, AC, CC and CQ designed and simulated mathematical modelling. CE designed and conceived biological background. PC and CE designed and conceived the study. All authors approved the final manuscript."
703,EP-0505085-B2.training.tei.xml,0,"Rotorwelle einer elektrischer Maschine, aus Stahl folgender Gewichtszusammensetzung: C0,15 bis 0,3% Si weniger als 0,05%Mn weniger als 0,5%, mindestens 0,1% Ni 3,5 bis 5% Cr mindestens 2,05%, weniger als 3,5% (Mo + W) 0,1 bis 1,0%, wobei gegebenenfalls W vorhanden istV0,03 bis 0,35% Al 0.0005% bis 0.006% gegebenenfalls von 0,001 bis 0,05% eines Elements der Gruppe IIa oder IIIa, gegebenenfalls bis zu 0,2% Ti, Zr, Hf, Nd und/oder Ta, Rest Fe, abgesehen von Verunreinigungen."
704,EP-0505085-B2.training.tei.xml,1,"Welle nach Anspruch 1, wobei der Stahl0,001 bis 0,05 Gew-% des Elements der Gruppe IIa oder IIIa enthält."
705,EP-0505085-B2.training.tei.xml,2,"Welle nach Anspruch 1 oder 2, wobei das Verhältnis Ni : Cr in dem Stahl kleiner als2,1 ist."
706,EP-0505085-B2.training.tei.xml,3,"Welle nach einem der Ansprüche 1 bis 3, wobei Cr in dem Stahl im Bereich von2,05 bis 2,6 Gew-% liegt."
707,EP-0505085-B2.training.tei.xml,4,"Welle nach einem der vorhergehenden Ansprüche, wobei der Stahl eine gleichförmige Bainit-Struktur aufweist."
708,EP-0505085-B2.training.tei.xml,5,"Welle nach einem der vorhergehenden Ansprüche, wobei der Stahl eine Zugfestigkeit beiRaumtemperatur von mindestens 93kg/mm2, eine Übergangstemperatur bei 50% Bruchaussehen (FATT) unter 0° C, eine 0,02%-Dehngrenze von mindestens 74 kg/mm2 und eine magnetische Feldstärke bei 21kG von weniger als 990 AT/cm aufweist"
709,EP-0505085-B2.training.tei.xml,6,"Rotorwelle einer elektrische Maschinen, aus Stahl folgender Zusammensetzung C0,15 bis 0,3%,Si weniger als 0,05% Mn weniger als 0,5%, mindestens 0,1% Ni 5% Cr mindestens 3,5 bis 2,05%, bis zu 3,5% (Mo + W) 0,1 bis 1% (wobei W wahlweise vorhanden ist) V 0,03 bis 0,35%Al weniger als 0,006% (P + S + Sn + Sb + As) weniger als 0,03% Rest Fe, abgesehen von Verunreinigungen."
710,EP-0505085-B2.training.tei.xml,7,"Welle nach Anspruch 7, wobei der Gehalt an (P + S + Sn + Sb + As) in dem Stahl nicht mehr als0,025 Gew-% beträgt."
711,EP-0505085-B2.training.tei.xml,8,"Welle nach Anspruch 7 oder 8, wobei das Produkt der Gewichtsprozentanteile von (a) Si und (b) (P + S + Sn + Sb + As) in dem Stahl nicht mehr als0,003 beträgt."
712,EP-0505085-B2.training.tei.xml,9,"Welle nach einem der Ansprüche 7 bis 9 mit einer Zugfestigkeit beiRaumtemperatur von mindestens 93kg/mm2, einer Übergangstemperatur bei 50% Bruchaussehen (FATT) unter 0°C, einer 0,02%-Dehngrenze von mindestens 74 kg/mm2 und einer magnetischen Feldstärke bei 21kG von weniger als 990 AT/cm."
713,EP-0505085-B2.training.tei.xml,10,"Rotorwelle nach einem der vorhergehenden Ansprüche mit einem Durchmesser von mindestens1m und einer Länge, die das 5,5- bis 6,5-Fache dieses Durchmessers beträgt."
714,EP-0505085-B2.training.tei.xml,11,"An electric machine rotor shaft of steel having the composition (by weight) : C0.15 to 0.3% Si less than 0.05% Mn less than 0.5%, at least 0.1% Ni 3.5 to 5% Cr at least 2.05%, less than 3.5% (Mo + W) 0.1 to 1.0%, W being optional V 0.03 to 0.35% Al 0.0005% to 0.006% optionally, from 0.001 to 0.05% of Group IIa or Group IIIa element, optionally, up to 0.2% of any of Ti, Zr, Hf, Nd, Ta; and the remainder Fe apart from impurities."
715,EP-0505085-B2.training.tei.xml,12,"A shaft according to claim 1, said steel comprising from0.001 to 0.05% by weight of the Group IIa or Group IIIa element."
716,EP-0505085-B2.training.tei.xml,13,A shaft according to claim 1 or 2 in which in said steel the ratio Ni:Cr is less than2.1.
717,EP-0505085-B2.training.tei.xml,14,A shaft according to any one of claims 1 to 3 in which Cr is in the range2.05 to 2.6% by weight in said steel.
718,EP-0505085-B2.training.tei.xml,15,"A shaft according to any one of the preceding claims, said steel having uniform bainite structure."
719,EP-0505085-B2.training.tei.xml,16,"A shaft according to any one of the preceding claims, said steel having a tensile strength at room temperature of at least93kg/mm2, a 50% fracture appearance transition temperature (FATT) below 0°C, 0.02% yield strength of at least 74kg/mm2, and magnetic field strength at 21kG less than 990AT/cm."
720,EP-0505085-B2.training.tei.xml,17,"An electric machine rotor shaft of steel having the composition: C0.15 to 0.3% Si less than 0.05% Mn less than 0.5%, at least 0.1% Ni 3.5 to 5% Cr at least 2.05%, up to 3.5% (Mo + W) 0.1 to 1%, (W being optional)V0.03 to 0.35% Al less than 0.006% (P + S + Sn + Sb + As) less than 0.03% and the remainder Fe apart from impurities."
721,EP-0505085-B2.training.tei.xml,18,A shaft according to claim 7 in which the content of (P + S + Sn + Sb + As) in said steel is not more than0.025% by weight.
722,EP-0505085-B2.training.tei.xml,19,A shaft according to any one of claims 7 and 8 in which in said steel the product of the weight percentage proportions of (a) Si and (b) (P + S + Sn + Sb + As) is not more than0.003.
723,EP-0505085-B2.training.tei.xml,20,"A shaft according to any one of claims 7 to 9 having a tensile strength atroom temperature of at least 93kg/mm2, a 50% fracture appearance transition temperature (FATT) below 0°C, 0.02% yield strength of at least 74 kg/mm2 and magnetic field strength at 21 kG less than 990 AT/cm."
724,EP-0505085-B2.training.tei.xml,21,A rotor shaft according to any one of the preceding claims having a diameter of at least1m and a length 5.5 to 6.5 times said diameter.
725,EP-0505085-B2.training.tei.xml,22,"Arbre de rotor de machine électrique en acier ayant la composition suivante (en poids) : C de0,15 à 0,3% Si moins de 0,05% Mn moins de 0,5%, au moins 0,1% Ni de 3,5 à 5% Cr au moins 2,05%, moins de 3,5% (Mo + W) de 0,1 à 1,0%, le W étant facultatif V de 0,03 à 0,35% Al de 0,0005% à 0,006% facultativement, de 0,001 à 0,05% d'un élément du groupe IIa ou du groupe IIIa, facultativement, jusqu'à 0,2% de l'un quelconque parmi Ti, Zr, Hf, Nd, Ta ; et le reste étant du Fe, en dehors des impuretés."
726,EP-0505085-B2.training.tei.xml,23,"Arbre selon la revendication 1, ledit acier comprenant entre0,001 et 0,05% en poids d'un élément du groupe IIa ou du groupe IIIa."
727,EP-0505085-B2.training.tei.xml,24,"Arbre selon la revendication 1 ou 2, dans lequel, dans ledit acier, le rapport Ni:Cr est inférieur à2,1."
728,EP-0505085-B2.training.tei.xml,25,"Arbre selon l'une quelconque des revendications 1 à 3, dans lequel le Cr est situé dans la plage comprise entre2,05 et 2,6% en poids dans ledit acier."
729,EP-0505085-B2.training.tei.xml,26,"Arbre selon l'une quelconque des revendications précédentes, ledit acier ayant une structure de bainite uniforme."
730,EP-0505085-B2.training.tei.xml,27,"Arbre selon l'une quelconque des revendications précédentes, ledit acier ayant une résistance à la traction à la température ambiante d'au moins93kg/mm2, une température de transition d'apparition de fracture (FATT) à 50% inférieure à 0°C, une limite apparente d'élasticité à 0,02% d'au moins 74kg/mm2, et une intensité de champ magnétique à 21 kG inférieure à 990 AT/cm."
731,EP-0505085-B2.training.tei.xml,28,"Arbre de rotor de machine électrique en acier ayant la composition suivante : C de0,15 à 0,3% Si moins de 0,05% Mn moins de 0,5%, au moins 0,1% Ni de 3,5 à 5% Cr au moins 2,05%, jusqu'à 3,5% (Mo + W) de 0,1 à 1% (le W étant facultatif) V de 0,03 à 0,35% Al moins de 0,006% (P + S + Sn + Sb + As) moins de 0,03% et le reste étant du Fe, en dehors des impuretés."
732,EP-0505085-B2.training.tei.xml,29,"Arbre selon la revendication 7, dans lequel la proportion de (P + S + Sn + Sb + As) dans ledit acier n'est pas supérieure à0,025% en poids."
733,EP-0505085-B2.training.tei.xml,30,"Arbre selon l'une quelconque des revendications 7 et 8, dans lequel, dans ledit acier, le produit des proportions de pourcentage en poids de (a) Si et de (b) (P + S + Sn + Sb + As) n'est pas supérieur à0,003."
734,EP-0505085-B2.training.tei.xml,31,"Arbre selon l'une quelconque des revendications 7 et 9, ayant une résistance à la traction à la température ambiante d'au moins93kg/mm2, une température de transition d'apparition de fracture (FATT) à 50% inférieure à 0°C, une limite apparente d'élasticité à 0,02% d'au moins 74kg/mm2, et une intensité de champ magnétique à 21 kG inférieure à 990 AT/cm."
735,EP-0505085-B2.training.tei.xml,32,"Arbre selon l'une quelconque des revendications précédentes, ayant un diamètre d'au moins1m et une longueur comprise entre 5,5 et 6,5 fois celle dudit diamètre."
736,EP-0505085-B2.training.tei.xml,33,This invention relates to steel rotor shafts for electric machines.
737,EP-0505085-B2.training.tei.xml,34,"In recent years, energy production has experienced a shift from petroleum towards coal as a source of thermal power. As a result,one technical problem which has arisen is the need to make turbine generators of increasing effectiveness. Because space is usually limited, the capacity of each individual generator tends to increase."
738,EP-0505085-B2.training.tei.xml,35,"The rotor shafts of large electric generators are made of steel. Such shafts are very special objects. The shafts for the new generation of large thermal power plants, some of which are envisaged to output as much as1,000MW or more, may weigh of the order of 80tonnes. They must withstand fast rotation, and yet remain operational for a period measured in decades."
739,EP-0505085-B2.training.tei.xml,36,"Therefore, very high strength and very high toughness are needed. It is well known that high strength tends to cause low toughness, and vice versa. That is one problem. Furthermore, because of the use of the material, it needs to have suitable magnetic properties."
740,EP-0505085-B2.training.tei.xml,37,"ASTM Standard Specification A469-88 describes types of special steel which are presently used for generator rotor shafts. Classes 6, 7 and 8 are the strongest. These specify contents as follows: C less than0.28% Mn less than 0.60% P less than 0.015% Si 0.15 to 0.30% Ni 3.25 to 4.00% Cr 1.25 to 2.00% Mo 0.30 to 0.60% V 0.05 to 0.15% and the remainder substantially Fe."
741,EP-0505085-B2.training.tei.xml,38,"The Class 8 steel is the strongest of all, having tensile strength of84kg/mm2, 0.02% yield strength of 70.4kg/mm2, elongation of more than 16%, reduction of area of more than 45% and 50% fracture appearance transition temperature (FATT) below 4°C."
742,EP-0505085-B2.training.tei.xml,39,"In the patent literature, JP-B-47/25248 describes a low alloy steel for generator rotor shafts having the composition C0.14 to 0.20% Si 0.05 to 0.4% Mn 0.1 to 0.6% Ni 1.5 to 2.8% Cr 0.75 to 1.8% Mo 0.1 to 0.5% V 0.01 to 0.12% and the remainder is Fe."
743,EP-0505085-B2.training.tei.xml,40,"JP-A-60/230965 describes low alloy steels for turbine generator shafts, having a composition C0.13 to 0.30% Si<0.10% Mn 0.06 to 2.00% P< 0.010% Cr 0.40 to 2.00% Ni 0.20 to 2.50% Mo 0.10 to 0.50% V 0.05 to 0.15% Al 0.005 to 0.040% N 0.0050 to 0.0150% the remainder being Fe."
744,EP-0505085-B2.training.tei.xml,41,"EP-A-225425 describes low alloy steels having good stress corrosion cracking resistance, for steam turbine shafts. Examples are disclosed in the following ranges: C0.2 - 0.23 Si 0.03 - 0.16 Mn 0.01 - 0.5 Ni 3.42 - 3.68 Cr 1.57 - 1.72 Mo 0.3 - 0.39 V 0.09 - 0.16 P+S 0.003 - 0.016 Fe balance apart from impurities."
745,EP-0505085-B2.training.tei.xml,42,"US-A-4985201 relates to low-alloy steels for electric generator rotors, including a high chromium steel ""A90"" of the composition: C max.0.28 Si not stated Mn max. 0.4 Ni 3.25 - 3.75 Cr 1.5 - 2.0 Mo 0.25 - 0.45 V 0.09 - 0.15 Fe balance apart from impurities."
746,EP-0505085-B2.training.tei.xml,43,"Our EP-A-384181 discloses as comparative examples (Specimens 6, 22) steels with the composition: C0.23 Si 0.05 Mn 0.30 Ni 3.56 Cr 1.66 Mo 0.40 V 0.12 P 0.009 S 0.012 Fe balance apart from impurities, both having a 50% FATT at -20°C, vgl: see Tables 2 and 6."
747,EP-0505085-B2.training.tei.xml,44,"The existing steels are good, but they are not good enough for the new large generators which are envisaged. For example, we have calculated that, for a900MVA class generator the rotor shaft material will require a tensile strength of at least 93kg/mm2, 0.02% yield strength of at least 74kg/mm2, FATT of below 0°C, and a magnetic characteristic such that magnetic field strength at 21 kG is less than 990 AT/cm. For a 1200MVA generator rotor shaft, the calculated tensile strength is at least 100 kg/mm2, and for a 1300MVA generator rotor shaft, at least 104kg/mm2."
748,EP-0505085-B2.training.tei.xml,45,"It will be appreciated that, for example, the ASTM Class 8 material mentioned above is quite inadequate for making a rotor shaft material for such generators. Firstly, it is not strong enough. Furthermore, as strength is intensified, toughness (which can be gauged by FATT) tends to decrease. Hence none of the known recipes leads the way to satisfying these new requirements."
749,EP-0505085-B2.training.tei.xml,46,"As a result of studies, the inventors have discovered certain ways in which high strength and toughness can be achieved, without compromising the magnetic properties. They have been able to prepare steels which satisfy even the preferred criteria set out above."
750,EP-0505085-B2.training.tei.xml,47,"The invention provides a low alloy steel, and also a rotor shaft made from said steel, having the composition: C0.15 to 0.3% Si less than 0.05% Mn less than 0.5%, at least 0.1% Ni 3.5 to 5% Cr at least 2.05%, less than 3.5% (Mo + W) 0.1 to 1.0%, W being optional V 0.03 to 0.35% Al 0.0005% to 0.006% optionally, from 0.001 to 0.05% of Group IIa or Group IIIa element, optionally, up to 0.2% of any of Ti, Zr, Hf, Nd, Ta; and the remainder Fe apart from impurities."
751,EP-0505085-B2.training.tei.xml,48,"In particular, this composition has higher chromium than has been used in this field in the prior art. It has previously been believed that steel containing more than2% chromium will have inadequate magnetic properties. The present inventors have found that if one or more other components are kept below specified limits, the chromium content can be increased (thereby improving hardness and toughness) without spoiling the magnetic properties. In particular, this aspect specifies less than 0.05% of silicon in the composition."
752,EP-0505085-B2.training.tei.xml,49,The manganese content is also quite low: less than0.05% 0.5%.
753,EP-0505085-B2.training.tei.xml,50,"Reduction in certain other constituents has also been found to have useful significance. In a further aspect, the invention provides a steel, or a rotor shaft made from such steel, having a composition C0.15 to 0.3% Si less than 0.05% Mn less than 0.5%, at least 0.1% Ni 3.5 to 5% Cr at least 2.05%, up to 3.5% (Mo + W) 0.1 to 1% (W being optional) V 0.03 to 0.35% Al less than 0.006%, (P+S+Sn+Sb+As) < 0.03% and the remainder Fe apart from impurities."
754,EP-0505085-B2.training.tei.xml,51,"The inventors have found that pronouncedly low levels of aluminium, and of the sum total of the impurities phosphorus, sulphur, tin, antimony and arsenic, are also conducive to good properties."
755,EP-0505085-B2.training.tei.xml,52,"The total content of thefive impurity elements mentioned is most preferably not more than 0.01%, and the product of the silicon concentration and that of said five impurities is preferably not more than 0.003."
756,EP-0505085-B2.training.tei.xml,53,"The ratio between nickel and chromium also has significance for the strength and toughness of the material. The ratio Ni:Cr is preferably less than2.3, more preferably less than 2.1, more preferably less than 2.05."
757,EP-0505085-B2.training.tei.xml,54,"The preferred structure for the steel is a uniform bainite structure, containing little or no ferrite."
758,EP-0505085-B2.training.tei.xml,55,"The steel or rotor shaft made thereof has a tensile strength at room temperature of at least93kg/mm2, a 50% fracture appearance transition temperature (FATT) below 0°C, 0.02% yield strength of at least 74kg/mm2, and magnetic field strength at 21kG less than 990AT/cm."
759,EP-0505085-B2.training.tei.xml,56,"A method of making one of the steel compositions as described, comprises melting in air; vacuum ladle refining or electroslag remelting: casting and hot forging; quenching at800°C to 900°C, and tempering at 525°C to 650°C for at least 10hours."
760,EP-0505085-B2.training.tei.xml,57,"Preferred features, technical concepts relating to the invention, and applications thereof are now described in some detail with reference to the accompanying drawings in which: Figure 1 is a graph showing a relationship between chromium content and tensile strengthFigure 2 is a graph showing a relationship between tensile strength and ratio of nickel to chromium;Figure 3 is a graph showing a relationship between tensile strength and silicon content;Figure 4 is a graph showing a relation between FATT, nickel content and chromium content;Figure 5 is a graph showing a relationship between FATT and silicon content;Figure 6 is a graph showing a relation between FATT and aluminium content;Figure 7 is graph showing a relation between magnetic properties and silicon content;Figure 8 is a graph showing a relation between magnetic properties and the total content of certain, generally non-metallic, impurities;Figure 9 is a graph showing a relation between magnetic properties and aluminium content;Figure 10 is a graph showing a relationship between magnetic properties and a parameter which is a product of various impurity contents;Figure 11 is a sectional view of a turbine generator;Figure 12 is a perspective view of a rotor shaft of the generator, andFigure 13 is a perspective view of the assembled rotor."
761,EP-0505085-B2.training.tei.xml,58,"Firstly, the steel composition is discussed with reference to the various individual components thereof."
762,EP-0505085-B2.training.tei.xml,59,"Carbon is an element necessary for improving hardenability, necessary for strength. If less than0.15% is present, insufficient hardenability is achieved and soft ferrite structure tends to form around the steel article so that insufficient tensile strength and yield strength are achieved. With more than 0.3%, toughness is reduced. Hence the carbon content is 0.15 to 0.3%, or preferably 0.20 to 0.28%."
763,EP-0505085-B2.training.tei.xml,60,"Conventionally, these elements have been added as deoxidizers. However, new steel-making technology such as the carbon deoxidising process using vacuum ladle refining, and the electroslag re-melting process, have obviated the need for such elements in making a sound article. To prevent brittleness due to tempering, the quantities of silicon and manganese should be kept low. The silicon content is less than0.05%, and that of manganese less than 0.5%, more preferably less than 0.25%, and most preferably less than 0.2%. Silicon is generally contained as an impurity from 0.01 to 0.1%, without the need to add it specially. However it is usually desirable to add some manganese; the quantity should be at least 0.05%(not claimed) or preferably at least 0.1% (according to the invention)."
764,EP-0505085-B2.training.tei.xml,61,"Nickel is essential for improving hardenability and toughness. With less than3.0%, there is insufficient toughness. If a large amount is used, over 5%, harmful residual austenite structure appears so that the desired uniform tempered bainite is not achieved. Therefore at least 3.5% is used. Conversely, the amount should be less than 5% and preferably less than 4.5%."
765,EP-0505085-B2.training.tei.xml,62,"Chromium has a remarkable effect in improving hardenability and toughness. It also improves the resistance to corrosion. More than3.5% tends to cause residual austenite structure. At least 2.05% is used, but preferably less than 3% and more preferably less than 2.6%."
766,EP-0505085-B2.training.tei.xml,63,"Molybdenum precipitates fine carbide in the crystal grain during tempering, intensifying tensile strength and yield strength by a carbide dispersion strengthening action. It also acts to restrict the segregation of impurities at the crystal grain boundary. It can prevent brittleness due to tempering. At least0.1% is required to secure these effects. Over 1.0%, however, the effects tend to be saturated. The preferred range is 0.25 to 0.6%, more preferably 0.35 to 0.45%. However, Mo may to some extent be substituted by W: see below."
767,EP-0505085-B2.training.tei.xml,64,"Like Mo, V precipitates fine carbide with the same desirable effects. To achieve the effects, at least0.03% should be used, preferably at least 0.05% and more preferably at least 0.1%. Over 0.35%, the effects tend to be saturated. Not more than 0.2% is preferred, more preferably not more than 0.15%."
768,EP-0505085-B2.training.tei.xml,65,"We have found that excessive quantities of aluminium reduce toughness and desirable magnetic properties. A complete absence of Al completely reduces strength, so at least0.0005% should be used in making the steel. However, the quantity should be kept low so that toughness and magnetic characteristics are good. Not more than 0.006% and more preferably not more than 0.005%, should be used."
769,EP-0505085-B2.training.tei.xml,66,The relation between Si and Al is not entirely clear as regards embrittlement.
770,EP-0505085-B2.training.tei.xml,67,"It is usual for most or all of these to be present as impurities. However they reduce toughness and magnetic characteristics. The total quantity is desirably less than0.03%, more preferably less than 0.025%. It is difficult to eliminate the elements entirely, but it is particularly desirable to get the total down to less than 0.01%."
771,EP-0505085-B2.training.tei.xml,68,"We have also found a correlation between the total amount of these impurities, and the amount of Si, as regards the magnetic properties of the steel. A product of the proportion of Si and a value X (the sum of the concentrations of thefive above-identified impurities) is preferably less than 0.003, more preferably less than 0.0015."
772,EP-0505085-B2.training.tei.xml,69,"The ratio of these components is related to tensile strength. The ratio should usually be less than2.3, preferably less than 2.1 and more preferably less than 2.05. The preferred range is 1.2 to 2.05, the more preferred range is 1.4 to 2.05. The Ni content is more than 3%."
773,EP-0505085-B2.training.tei.xml,70,"One or more Group IIa elements (Be, Mg, Ca) and/or one or more Group IIIa elements (Sc, Y, Lanthanides) may be incorporated, in an amount up to 0.1%. These elements have a strong deoxidising effect and can improve toughness and magnetic characteristics. A preferred quantity is 0.001 to 0.05%. The non-radioactive elements are preferable from the point of view of handling."
774,EP-0505085-B2.training.tei.xml,71,"One or more of Ti, Zr, Hf, Nd, Ta and W may be incorporated, in amounts less than 0.2% by weight, consistent with increasing strength without reducing toughness. A preferred quantity is 0.02 to 0.1%. W acts in the same way as Mo, mentioned above, so W can be substituted for part of Mo."
775,EP-0505085-B2.training.tei.xml,72,"Thus, the quantity of Mo + W may be0.1 to 1.0%. The quantity of W is preferably not more than half the total quantity. Mo must be present, but W is optional."
776,EP-0505085-B2.training.tei.xml,73,"The steel should have tempered bainite structure, and should contain less than5% ferrite. A uniform, overall structure of bainite is preferred for strength and toughness."
777,EP-0505085-B2.training.tei.xml,74,The achieving of good magnetic characteristics relies on reducingone or more of certain impurities.
778,EP-0505085-B2.training.tei.xml,75,"To reduce silicon considerably, molten metal is obtained by vacuum ladle refining or electroslag re-melting after melting in air. The molten metal is cast in a mould, and hot forged to the desired shape. Subsequently, it is quenched at from800 to 900°C and then tempered at 525 to 650°C for at least 10hours. The quenching temperature is desirably 30 to 70°C higher than the point Ac3, most preferably about 50°C higher. Tempering increases toughness. The preferred temperature is 540 to 625°C, preferably for 10 to 80 hours. After tempering, the final shape is formed by cutting. Cutting generates internal stresses, so stress relief annealing is performed at a temperature below the tempering temperature. Furthermore, homogenising annealing is done at a temperature about 50°C higher than the quenching temperature, followed by slow cooling."
779,EP-0505085-B2.training.tei.xml,76,This enables formation of bainite structure overall.
780,EP-0505085-B2.training.tei.xml,77,"As mentioned, the silicon quantity can be set in the range0.1 to 0.3%, provided that the aluminium quantity is kept below 0.01%. With higher silicon, good characteristics can also be achieved provided that the total quantity of P, S, Sn, Sb and As is kept low, desirably less than 0.025%. The skilled man knows how to reduce the quantities of the latter, although the present importance of this has not previously been disclosed."
781,EP-0505085-B2.training.tei.xml,78,Using the previously mentioned alloy steel enables the rotor shaft for electric machines to be made compact by setting the diameter of the body in which a coil is embedded more than1m and the length of the body 5.5 to 6.5 times the diameter. The ratio of less than 5.5 or over 6.5 is not desirable from the viewpoint of vibration. Particularly 5.6 to 6.0 is desirable.
782,EP-0505085-B2.training.tei.xml,79,"Although the diameter of the body needs to be enlarged together with the capacity of the generator, it should be less than0.2mm per 1MVA of the capacity plus 1000mm and over 0.2mm per 1MVA plus 900mm."
783,EP-0505085-B2.training.tei.xml,80,"Further, the diameter of the body D (m) should be set according to rotation speed (rpm), so that the value of (D2 x R2) is more than1.0 x 107. Particularly, the upper limit is desired to be 3.0 x 107 or more preferably 1.5 to 2.2 x 107 and most preferably 1.8 to 2.0 x 107."
784,EP-0505085-B2.training.tei.xml,81,"Although a larger capacity/output generator or motor tends to be larger, using high strength alloy steel as mentioned above enables a compact apparatus, particularly so that the capacity per floor area is0.08 to 0.12m2 per 1 MVA of the capacity. Consequently, energy loss decreases and efficiency rises. Further, the stator current can be reduced relative to capacity, particularly so that the current is 19.0 to 24 A per 1 MVA of generator or motor capacity. Against the capacity of 2,000MVA, it is possible to reduce the current to 19.0 to 20.0 A. At that time, the rotor is cooled by hydrogen. Depending on the output of the generator, hydrogen pressure must be raised, however, that pressure can be set to 0.003 to 0.006kg/cm2 per 1MVA. Particularly, 0.004 to 0.005kg/cm2 is desired."
785,EP-0505085-B2.training.tei.xml,82,"Such shafts may be for generators or motors. For motors, a synchronous motor, synchronous generator motor and induced synchronous motor are available. The structures of motors and generators are almost the same. Preferably, we use a high speed motor providing a rotation speed of more than5,000rpm."
786,EP-0505085-B2.training.tei.xml,83,"The tensile strength of the rotor shaft is desired to be more than93kg/mm2 or more preferably more than 100kg/mm2 and particularly it is desirable to adjust the composition so as to obtain more than 104kg/mm2. At the same time, 50% fracture appearance transition temperature is desired to be less than 0°C and more preferably, less than -20°C. The crystal grain size number is desired to be more than 4 (ASTM crystal grain size). Additionally, as magnetic characteristic, magnetic field strength is desired to be less than 990AT/cm at 21 kG in magnetic flux density, and less than 400AT/cm at 20kG. More preferably it is desired to be less than 500AT/cm in the former condition."
787,EP-0505085-B2.training.tei.xml,84,"Embodiments are now described specifically, by way of example."
788,EP-0505085-B2.training.tei.xml,85,"Table 1 shows the chemical composition of various specimen steels. A20kg ingot is made in a high frequency induction melting furnace and forged to 30mm in thickness and 90mm in width at 850 to 1,150°C. Specimens No.2 to 4, 6 and 15 are materials embodying the invention. Others are for comparison. No.1 is a material equivalent to ASTM standard A469-88 class 8 for generator rotor shaft material. No. 5 is a material containing relatively high Al content. These specimens underwent heat treatment by simulating the conditions for the large size rotor shaft centre of a large capacity generator. First, it was heated to 840°C to form austenite structure . Then, the specimen was heated and held at 575 to 590°C for 32hours . Tempering was done at such a temperature to secure tensile strength in the range of 100 to 105kg/mm2 for each specimen."
789,EP-0505085-B2.training.tei.xml,86,"No. 7 to 12 are also steels for comparison. They were heated and held at820°C for 16 to 34 hours, then heated and held at 625 to 635°C for 40 to 50 hours for tempering."
790,EP-0505085-B2.training.tei.xml,87,"No.13 and 14 are further steels for comparison. After homogenizing annealing at900°C for 2hours, they were austenitized at 850°C for 2hours, , further tempered at 575°C for 60hours, "
791,EP-0505085-B2.training.tei.xml,88,"None of No. 2 to 6 and 15 of the Ni-Cr-Mo-V steel contains proeutectoid ferrite. They possess uniform tempered bainite structure. Every crystal grain size No. of original austenite grains is7. No.1, 5 and 14 of other alloy also have uniform tempered bainite structure. In No.13, about 5% proeutectoid ferrite is found."
792,EP-0505085-B2.training.tei.xml,89,"Table 2 shows the results of tensile tests, impact tests, magnetic characteristic and electric characteristic tests. The magnetic field strengths in the Table were obtained under20 kG and 21 kG. The data shown in the Table are those under 21 kG."
793,EP-0505085-B2.training.tei.xml,90,"As shown in Table, the low alloy steels No.2 to 4, 6 and 15 have a high strength and toughness while the tensile strength is more than100kg/mm2, 0.02% yield strength is more than 78kg/mm2 and 50% fracture appearance transition temperature is far below 0°C or below -50°C. Further, the magnetic field strength satisfies the requirement of less than 990AT/cm as the magnetic field strength at 21 kG requested for generator rotor shaft over 900MVA, and the electric resistance is over 30µ-Ωcm because of high Cr content, so that this material is very useful as the rotor shaft material of a large capacity generator over 900MVA."
794,EP-0505085-B2.training.tei.xml,91,"The effects of various constituents are now considered in relation to the specific examples and comparison examples. Fig. 1 is a diagram showing the influence on the tensile strength of Cr content. The tensile strength increases as the Cr quantity increases, when the Ni quantity is2.60 to 4.15%. Particularly, when Cr quantity exceeds 1.4%, the tensile strength increases rapidly so that the effect of Cr is large. If the quantity exceeds 2.0%, a high tensile strength over 100kg/mm2 can be obtained. Fig. 2 is a diagram showing the relationship with Ni/Cr ratio. The tensile strength decreases as Ni/Cr ratio increases. Particularly, a higher strength is obtained by setting the Ni/Cr ratio lower than 2.1. While related to Ni quantity, a far higher strength over 100kg/mm2 is obtained by securing a high Ni quantity over 3.50%. This is obtained by setting Ni/Cr ratio below 2.3 and Ni below 3.5% against the objective tensile strength of 93kg/mm2. In this case, if Ni is less than 3%, that tensile strength is difficult to obtain. Fig. 3 shows the relationship with Si quantity, indicating that the strength increases as the Si quantity increases. When Si quantity is more than 0.17%, 93kg/mm2 is obtained by adjusting Cr and Ni to 1.3 to 1.8% and 2.6 to 3.5% respectively, while if Cr exceeds 2%, when Si is as low as or less than 0.1%, more than 93kg/mm2 or particularly more than 100kg/mm2 is obtained. Fig. 4 is a diagram showing the influence on 50% fracture appearance transition temperature of Ni or Cr contents. As the content of Ni or Cr increases, FATT lowers, and particularly, when Si is less than 0.1%, FATT below 0°C is obtained by making more than 0.5% Cr contained. Fig. 5 is a diagram showing the influence on FATT of Si quantity. As Si quantity decreases, FATT decreases so as to secure a high toughness. Particularly, when Ni is 2.5 to 3.0% and Cr is 1.3 to 1.8%, FATT can be lowered below 0°C by adjusting Si quantity to below 0.08%, and when Ni is 3.5 to 4.0% and Cr is 1.5 to 2.2%, the value can be lowered below 0°C by adjusting Si quantity to below 0.13%. When Cr is over 2.2% and Ni is over 3.5%, FATT can be lowered below 0°C by adjusting Si quantity less than 0.20%. Fig. 6 is a diagram showing the relationship between FATT and Al content. The Al content increases FATT. When Cr is 2.05 to 2.2% and Ni is 3 to 4%, FATT can be lowered below 0°C by adjusting Al quantity to below 0.014%. When Cr is 2.2 to 2.5% and Ni is 3.5 to 4.5%, the value can be lowered below 0°C by adjusting Al quantity to below 0.018%. When Cr is near 1.65%, even if Ni quantity is as high as 3.5%, FATT is difficult to lower below 0°C if Al quantity is reduced. Fig. 7 shows the relationship between magnetic field strength and Si quantity. Because the increase of Si quantity intensifies magnetic field strength as shown in the figure, the Si quantity should be as small as possible for present purposes. Particularly, when Cr is 1.5 to 2.5 % and Ni is 2.5 to 4.5%, magnetic field strength at 21kG can be suppressed below 990AT/cm by adjusting Si quantity to less than 0.18%. Particularly, when Si quantity is less than 0.1%, a magnetic strength of less than 700AT/cm is obtained. Fig. 8 is a diagram showing the relationship between magnetizing force and the total amount of P, S, Sn, Sb and As. These impurities are undesirable because they increase magnetic field strength and their concentration should be less than 0.040% to adjust magnetic field strength below 990AT/cm. Particularly, it should be less than 0.03% to lower it below 700AT/cm. Fig. 9 shows the relationship between magnetic field strength and Al content. As shown in the figure, Al is undesirable because it intensifies magnetic field strength. When Cr is 1.5 to 2.5% and Ni is 2.5 to 4.5% and even when Si quantities are less than 0.1%, Al quantity should be below 0.025% to obtain a magnetic field strength of less than 990AT/cm. Particularly, to obtain a magnetic field strength of less than 700AT/cm, Al quantity should be lowered below 0.015%. If Si quantity exceeds 0.1%, Al quantity should be less than 0.01%. Fig. 10 shows the influence on magnetic field strength of the quantity of Si multiplied by the total amount of P, S, Sn, Sb and Ab and the higher this quantity is, the more inappropriate it is because magnetic field strength is increased. Magnetic field strength can be lowered below 990AT/cm by adjusting the quantity to less than 70 x 10-4."
795,EP-0505085-B2.training.tei.xml,92,"Table 3 shows the results of the tensile test, impact test and magnetic characteristic test for the specimen provided by intensifying the strength of this invention steel No. 2 to 4 and 6. In this embodiment, the tempering temperature was set5°C lower than in Embodiment 1."
796,EP-0505085-B2.training.tei.xml,93,"As evident from the table, the materials embodying the invention satisfied the mechanical performance and magnetic characteristic required even for1,200MVA class and 1,300MVA class generator rotor shaft, giving tensile strength more than 105kg/mm2, 0.02% yield strength more than 82kg/mm2, FATT below -44°C and magnetic field strength less than 400AT/cm. Thus these materials can be said to be very useful, e.g. for a > 1,200MVA class large capacity generator rotor shaft."
797,EP-0505085-B2.training.tei.xml,94,Thermal power and nuclear power AC turbine generators are usually 2-pole or 4-pole cylindrical rotating field synchronous generators.
798,EP-0505085-B2.training.tei.xml,95,"Most thermal power turbine generators are 2-pole high-speed generators. The rotation speed is3,000rpm at 50Hz and 3,600rpm at 60Hz. This is because the higher the rotation speed, the better the efficiency becomes and the size becomes smaller. In most cases, a tandem compound type generator generating output with a single axis is utilized. Most large capacity machines are of cross compound type, generating output with two axes, which is capable of generating more than the tandem compound type."
799,EP-0505085-B2.training.tei.xml,96,"The nuclear power turbine generator is usually 4-pole type and used at1,500rpm or 1,800rpm. This is because a larger amount of vapor is generated from the nuclear reactor with a lower temperature and pressure, and the turbine has long blades and rotates at a low speed."
800,EP-0505085-B2.training.tei.xml,97,"As the cooling method for a turbine generator, indirect cooling method and direct cooling method are available, and air, hydrogen and water are used as cooling medium."
801,EP-0505085-B2.training.tei.xml,98,"Hydrogen cooling method is used for a large capacity machine and divided into indirect and direct methods. In both cases, an explosion proof sealed structure incorporating a gas cooler in its generator main body is utilized. In case of water cooling type, direct cooling method is used and for a large capacity machine, water cooling method is sometimes used for both the stator and rotor."
802,EP-0505085-B2.training.tei.xml,99,"Fig. 11 shows an example of a stator coil direct water cooling turbine generator, which is an embodiment of an aspect herein."
803,EP-0505085-B2.training.tei.xml,100,"The stator cage, which is made of welded steel plates, forms an air path, supports the iron core and prevents vibration. The iron core is deformed to an oval shape due to magnetic attraction force, so that double frequency vibration is generated with the rotation of the rotor. Because this vibration increases as with machine size, elastic support structure is adopted by installing the iron core and stator cage through a spring."
804,EP-0505085-B2.training.tei.xml,101,0.35 or 0.5mm thick silicon steel plate is used for the stator iron core 2 and this plate has a directivity. The iron core is formed by laminating by 50 to 60mm in axial direction and an I-shaped gap steel is inserted to form an air duct.
805,EP-0505085-B2.training.tei.xml,102,"A two-layer coil is usually used for the stator coil 7, and in case of a 2-pole type, it needs to be held firmly because the coil end is extended. In this case, because the floating load loss increases, a non-magnetic material is used for the structure at the end."
806,EP-0505085-B2.training.tei.xml,103,"The notable characteristic of the turbine generator is that it rotates at a high speed, and the rotor diameter is restricted due to a large centrifugal force. The rotor is forged as one body to secure mechanical strength preventing dangerous speeds and vibrations, and processed to have a slot, in which a field winding coil is incorporated. Figs. 12 and 13 show the shape of the rotor 1."
807,EP-0505085-B2.training.tei.xml,104,"The main shaft is made of Ni-Cr-Mo-V steel, preferably of a type as described above. Although not illustrated, the fixing ring 17 for the fan 20 is provided between the flange 15 and centering ring 18."
808,EP-0505085-B2.training.tei.xml,105,"The field winding coil 3 is distributed and wound in the slots of a rotor iron core between the teeth 12 formed by winding copper belt flat, and a layer insulator is inserted by a single turn of the conductor. The end of the winding coil is held by a retaining ring 9. Usually, a silver contained copper having an excellent creep characteristic is used for the coil instead of copper."
809,EP-0505085-B2.training.tei.xml,106,"For the retaining ring 9, non-magnetic stainless steel with less than0.1% C, more than 0.4% N 10-25% Mn and 15-20% Cr is applied. After the winding wire 3 is buried, it is fastened with a wedge 13 made of ultra duralmin alloy For the end damper ring 14, an end or overall length damper is used, and Al alloy and silver contained copper are used for the end and body respectively. 8 is a shaft, 11 is a magnetic pole and 15 is a coupling."
810,EP-0505085-B2.training.tei.xml,107,"A large capacity machine over1,000MVA is difficult to cool evenly because the iron core is long, so a duplex ventilation method is applied."
811,EP-0505085-B2.training.tei.xml,108,"According to this method, air supply chambers and exhaust chambers in several sections are arranged alternately within the stator cage in the rear of the iron core, cooling air is collected into each air supply chamber from both ends of the generator through an air duct in the stator cage to cool the stator iron core. Then, this air flows to the outside surface together with the air cooling the inside of the rotor and reaches the suction side through the cooler, circulating inside."
812,EP-0505085-B2.training.tei.xml,109,"The gas pressure for cooling with hydrogen is2atg for indirect hydrogen cooler, and 2 to 5atg for direct hydrogen cooler. Because when hydrogen gas pressure is increased, the calorific capacity of gas increases in proportion to density as heat transfer rate rises, thus the temperature rise of gas itself decreases in inverse proportion to the absolute pressure of gas so that the effect of cooling increases. Assuming that the output is 100 when 0.05atg is provided with indirect cooling type, the output from the same dimension machine is 115 under 1 atg, and 125 under 2atg."
813,EP-0505085-B2.training.tei.xml,110,"Hydrogen cooling method has a danger of explosion in such a range that hydrogen volume is10 to 70% when mixed with air. To prevent this accident, hydrogen purity is automatically maintained over 90% and a sealing device to prevent hydrogen gas from leaking outside along the axis by means of oil film is provided inside of the bearing. Gas leakage is prevented by flowing oil having a higher pressure than hydrogen gas inside into the gap on the shaft."
814,EP-0505085-B2.training.tei.xml,111,"Even when the stator is cooled indirectly in a hydrogen cooling turbine generator, the rotor is often cooled directly."
815,EP-0505085-B2.training.tei.xml,112,"When the maximum temperature of a generator coil conductor limits the output, the conductor is cooled directly with cooling medium to eliminate the difference of temperature from an insulator occupying a large portion, during a temperature rise."
816,EP-0505085-B2.training.tei.xml,113,"As cooling media, hydrogen gas, oil and water are available. Water has a heat transfer capacity about50 times air and excels as a cooling medium. (1) An example of a hydrogen gas direct cooling stator coil is shown here, and gas is fed inside a square bent tube put between strands to cool the conductor directly. Although part of heat generated in the conductor is transferred to an iron core through a main insulator with a large heat resistance, most is carried away by hydrogen gas via small cooling pipes, with a small heat resistance. As cooling liquid, pure water having a large specific heat and heat transfer coefficient by convection is utilized.Stainless steel is applied to pipes serving as a liquid path, and oxygen free copper or deoxidixed copper is used for a coil and clip at the coil end. A PTFE (teflon) tube having a high mechanical strength and flexibility, and an excellent insulation is used for an insulated connecting pipe. The stator coil is hollow in its cross section, where liquid flows.(2) As the cooling medium for the rotor, hydrogen gas or water is used and the following method is available. According to the end feed method, hydrogen gas, after being forced into the rotor coil from the rotor end, is discharged into the air gap through a hole provided at the center of the rotor. Additionally, the method to introduce hydrogen gas into the coil copper belt from an end of the rotor and discharge it from the other end is also desirable."
817,EP-0505085-B2.training.tei.xml,114,"As the sectional shape of the rotor coil, either by-pass type or hollow copper type is available. When either type is used, gas direct cooling method is applied for the stator coil also and a high pressure blower is installed on an end of the rotor."
818,EP-0505085-B2.training.tei.xml,115,"According to the air gap pickup method, a suction hole and discharge hole are provided alternately on the surface of the rotor, and using wind speed by rotation, hydrogen gas at the air gap is sucked from the coil wedge surface, made to flow within the coil copper belt at a specified distance to deprive of generated heat and then discharged to the air gap through the vent hole. Or water is made to flow within a rotating object."
819,EP-0505085-B2.training.tei.xml,116,"Water cooling method makes the structure more complicated as compared with the hydrogen gas cooling method and thus is disadvantageous in reliability. However, the weight of the generator is15 to 25% lighter so that the efficiency with partial load can be improved."
820,EP-0505085-B2.training.tei.xml,117,"In the figure, 15 is a flange connected to the turbine, 20 is a fan, 21 is a stator coil, 22 is a brush and 23 is a spring."
821,EP-0505085-B2.training.tei.xml,118,"Fig. 12 is a perspective view of a large capacity turbine generator rotor shaft having more than1,000MW in turbine output (1,120MVA in generator capacity) embodying this invention. The rotor shaft embodying this invention was produced as explained below."
822,EP-0505085-B2.training.tei.xml,119,"To aim at almost the same composition as specimen No. 2 described in embodiment 1, molten metal of about150 ton, prepared by vacuum ladle refining after melting in the air, was poured into a mold. On the next step, the casting was hot forged by press, upset (forging ratio: 1/2U) and then lengthened (forging ratio: 3S). Further, after unifying annealing was performed at 900°C, the material was cut to a specified shape, then heated and held at 840°C in a vertical furnace for 20 hours,  Then, after heating and being held at 580°C for 60 hours, . After that, it was cut to the final shape as shown in Fig. 12. This embodiment is for 2-pole type, and 11 is a magnetic pole, 12 is teeth, 17 is fan mounting ring, 18 is retaining ring fitting centering ring, and 19 is center hole. A test piece was collected from this material to inspect its mechanical, electric and magnetic characteristics. The centering ring 18 is integrated on forming the shaft and a retaining ring is shrinkage fit after cutting to ring like shape."
823,EP-0505085-B2.training.tei.xml,120,"In this embodiment, the overall length is about15m, the diameter of the body on which teeth are provided is 1.2m, and the length of the body is about 7m, about 5.7 times the diameter of the body. The machine size of this embodiment is about 10m3, thus the rotor's sensitivity to vibration is reduced, so that the sensitivity to imbalance in the same phase can be suppressed and at the same time, a high axis stability is obtained because the flexibility of the shaft drops."
824,EP-0505085-B2.training.tei.xml,121,The machine size is expressed by (outside diameter of the rotor body)2 x (length of the rotor)
825,EP-0505085-B2.training.tei.xml,122,The relationship between the machine size of rotor shaft and generator capacity (MVA) is preferably between the ranges expressed by the expressions 1 and 2.
826,EP-0505085-B2.training.tei.xml,123,"The mechanical, magnetic and electric characteristics of this embodiment are the same as the values of the alloy No.2 of the embodiment 1."
827,EP-0505085-B2.training.tei.xml,124,The specifications of this embodiment are as follows.
828,EP-0505085-B2.training.tei.xml,125,"Generator capacity:1.100MVA, stator current: 22 A per 1MVA of generator capacity, power factor: 0.9, rotation speed: 3,600 rpm, frequency: 60Hz, stator: direct water cooling, rotor direct hydrogen cooling (0.0047kg/cm2.g per 1MVA of generator capacity), casing material: SM41 steel, iron core material: directional silicon steel, coil: electrolytic copper, insulation material: epoxy resin and mica, length and diameter of the part in which a coil is embedded = 5.83, retaining material: 18% Mn-18%Cr steel containing C 0.1% or less, more than 0.4% N, Si less than 1%, overall length damper, rotor coil: silver contained copper, bearing: cast carbon steel, overall length: 16m in length, 6m in width, floor area: 96m2"
829,EP-0505085-B2.training.tei.xml,126,"The above mentioned structure ensures1,120MVA of generator capacity against the turbine output of 1,000MW class and the unit floor area for this generator per 1 MVA is 0.086m2 or about 13% smaller than the floor area per 1MVA of the conventional 800MVA class turbine generator, 0.098m2. The floor area can be reduced to 0.08 to 0.09m2per 1MVA of generator output."
830,EP-0505085-B2.training.tei.xml,127,"Concerning the low alloy steel embodying this invention, the upper and lower limit of the body diameter must be a value which can be obtained from the previously mentioned machine size, while the upper limit and lower limit of the diameter D(mm) are desired to be a value which can be obtained from the expressions 3 and 4, respectively. The length of the body is desired to be5.5 to 6.5 times the diameter."
831,EP-0505085-B2.training.tei.xml,128,"The structure as described makes it possible to reduce the rotor's sensitivity to vibration and make a compact generator unit. Because tensile strength is more than93kg/mm2, 50% fracture transition temperature is below 0°C and the magnetizing force at 21kG is less than 900AT/cm, a compact large capacity generator of more than 900MVA in capacity or synchronous motor having a rotation speed of more than 5000rpm can be produced. Hence, effective use of the installation area is enabled, so that this contributes to diversification of energy including petroleum, coal and nuclear power for power generation."
832,inserm-00752141.training.tei.xml,0,"Background: The age-related decline in performance has been investigated in swimmers, runners and triathletes. No study has investigated the age-related performance decline in ultra-triathletes. The purpose of this study was to analyse the age-related declines in swimming, cycling, running and overall race time for both Triple Iron ultra-triathlon (11.4-km swimming, 540-km cycling and 126.6-km running) and Deca Iron ultra-triathlon (38-km swimming, 1,800-km cycling and 420-km running)."
833,inserm-00752141.training.tei.xml,1,"In recent years, there has been an increased interest in investigating the effect of aging on endurance running performances. Over the last decades, the participation of master athletes (>40 years old) has increased, especially in the longer run distances such as half marathons , marathons  and ultra-marathons . However, with increasing age, the endurance performance decreases.  ."
834,inserm-00752141.training.tei.xml,2,"Considering the age-related decline in male ultraendurance athletes, Hoffman investigated ultra-marathoners competing over161 kilometres . The 30-to 39-year-old males showed the fastest races times, with athletes in younger and older age groups being slower. The 40-to 49-year age group was approximately 4.0% slower than the 30 to 39 years one . In another study of 161-km ultramarathoners, performance of the athletes in the 40-to 49-year age group was no different from the performance of the athletes in the < 30 and the 30 to 39 years age groups . Both the moderate decline in running performance and the large number of successful master athletes suggest that master athletes are able to maintain a high degree of physiological performance with increasing age . For Ironman triathletes, both an increase in participation and an improvement in performance in master athletes have recently been reported ."
835,inserm-00752141.training.tei.xml,3,"The multi-sport discipline triathlon involves successively thethree endurance disciplines: swimming, cycling and running. The traditional distances in triathlon vary from the short or Olympic distance, covering 1.5-km swimming, 40-km cycling and 10-km running , to the Ironman distance covering 3.8-km swimming, 180-km cycling and 42.2-km running . Furthermore, ultra-endurance triathlons of longer distances than the Ironman distance do exist such as the Triple Iron ultra-triathlon of 11.4-km swimming, 540-km cycling and 126.6-km running , and the Deca Iron ultra-triathlon of 38-km swimming, 1,800-km cycling and 420-km running ."
836,inserm-00752141.training.tei.xml,4,"Regarding the performance in a triathlon, the association of different characteristics in physiology, anthropometry , training  and previous experience  with race time has been investigated. Apart from these characteristics, an agerelated decline has been described for Olympic distance triathletes  as well as for Ironman triathletes . It seems that the length of an endurance performance has an influence of the age-related performance decline. In short distance triathletes, a significant decline in performance starts at the age of 45 to 50 years . In Ironman triathletes, however, the agerelated decline in performance starts after the age of 55 years ."
837,inserm-00752141.training.tei.xml,5,"To date, no study has investigated the age-related decline in performance in an ultra-triathlon longer than the Ironman distance. Therefore, the aim of the present study was to analyse the age-related declines in swimming, cycling, running and total performances for both Triple Iron ultra-triathletes and Deca Iron ultra-triathletes. Since the age-related performance decline occurred later in the longer Ironman triathlons than in the shorter Olympic distance triathlon, we hypothesised that the age-related decline in performance would occur even later for Deca Iron ultra-triathletes than for Triple Iron ultra-triathletes."
838,inserm-00752141.training.tei.xml,6,"From1992 to 2010, there were 423 Triple Iron ultratriathlon finishers and 119 Deca Iron ultra-triathlon finishers. During this period, the average number of finishers per year was 23 ± 9 (range, 7 to 38) at the Triple Iron ultra-triathlon and 11 ± 5 (range, 2 to 18) at the Deca Iron ultra-triathlon, respectively. From 1992 to 2010, the number of total finishers per age group, for both races, is shown in Table 1. The 5-year age bracket with the largest participation was 35 to 39 years in the Triple Iron ultra-triathlon and 40 to 44 years in the Deca Iron ultra-triathlon."
839,inserm-00752141.training.tei.xml,7,"The ages of the winners from1992 to 2010, for both the Triple Iron ultra-triathletes and Deca Iron ultra-triathletes, are shown in Figure 1A. The ages of the winners did not significantly change over this period, in either of these races and, therefore, no differences in the mean age of the winners was found: 35.8 ± 4.5 years (range, 31 to 46 years) in the Triple Iron ultra-triathlon and 38.0 ± 6.8 years (range, 27 to 50 years) in the Deca Iron ultratriathlon. In contrast, the mean age of the finishers was significantly (P < 0. 0 5 )h i g h e ri nt h eD e c aI r o nu l t r atriathlon compared with the Triple Iron ultra-triathlon; 41.3 ± 3.1 years in the Deca Iron ultra-triathlon and 38.5 ± 3.3 years in the Triple Iron ultra-triathlon ( Figure 1B). In addition, the mean age of the finishers significantly increased over this period for both ultratriathlon distances."
840,inserm-00752141.training.tei.xml,8,"Age-related changes in performance Total, swimming, cycling and running performances of the topten triathletes for the different age groups at the Triple Iron triathlon during the 1992 to 2010 period are shown in Table 2. There was a significant age effect for swimming (F = 62.4, P < 0.01), cycling (F = 65.7, P < 0.01), running (F = 209.9, P < 0.01) and the total (F = 158, P < 0.01) times. No significant difference in time was observed between the age groups 25 to 34 and 35 to 44 years for swimming, cycling, running and total time. The mean best time performances were 176 ± 7 min for swimming (25 to 34 years), 1,030 ± 24 min for cycling (35 to 44 years), 759 ± 38 min for running (35 to 44 years) and 2,069 ± 92 min for the total race time (35 to 44 years)."
841,inserm-00752141.training.tei.xml,9,
842,inserm-00752141.training.tei.xml,10,"Total, swimming, cycling and running performances of the topten triathletes for the different age groups at the Deca Iron triathlon during the 1992 to 2010 period are shown in Table 3. There was a significant age effect for swimming (F = 32.2, P < 0.01), cycling (F = 28.1, P < 0.01), running (F = 12.7, P < 0.01) and the total (F = 22.2, P < 0.01) times. No significant differences in times were observed for the age groups between 25 and 54 years for swimming and cycling. The mean best time performances were 732 ± 41 min for swimming (35 to 44 years), 5,758 ± 247 min for cycling (35 to 44 years), 5,055 ± 807 min for running (25 to 34 years) and 12,292 ± 616 min for total performances (35 to 44 years), respectively."
843,inserm-00752141.training.tei.xml,11,"These are the performances of the different age groups at the Deca Iron triathlon (Data were pooled from1992 to 2010 The aim of this study was to analyse the age-related declines in swimming, cycling, running and total performances in male Triple and Deca Iron ultra-triathletes. No difference in total performance was found between the ages of 25 and 44 years for both Triple and Deca Iron ultra-triathletes. However, in contrast to Triple Iron ultra-triathletes, Deca Iron ultra-triathletes can achieve the same level of performance in swimming and cycling between 25 and 54 years of age."
844,inserm-00752141.training.tei.xml,12,"A main interesting finding was that the age-related decline occurred later in swimming and cycling compared to running at the Deca Iron ultra-triathlon distance. Previous studies suggested that the age-related decline in triathlon performance was specific to the discipline, with cycling showing fewer declines in performance with age than running in both short distance and Ironman triathletes. The present results showed that the swimming and cycling performances were maintained until the age of 54 for the Deca Iron ultra-triathletes but only until the age of 44 for the Triple Iron ultra-triathletes. A potential explanation for this finding could be pre-race experience. It has been shown that previous experience was a strong predictor for a successful finish in an ultra-triathlon . Recently, Lepers et al.  analysed the performance of 73 triathletes (68 men and 5 women) who finished a Double Iron ultratriathlon, a Triple Iron ultra-triathlon and a Deca Iron ultra-triathlon. The contribution of swimming to overall ultra-triathlon performance was lower than for cycling and running. Running performance was more important to overall performance for Double Iron ultra-triathlon and Triple Iron ultra-triathlon compared to Deca Iron ultra-triathlon. The Double Iron ultra-triathlon and Triple Iron ultra-triathlon performances were significantly correlated to Deca Iron ultra-triathlon performances for swimming and cycling, but not for running."
845,inserm-00752141.training.tei.xml,13,"The main differences between cycling and running are the change from a nonweight to a weight-bearing activity and the differences in the coordination of the leg muscles with a shift from a predominantly concentric type of muscle action in cycling to a stretch-shortening activity with eccentric contractions in running. Cycling may show differences in the age-related performance decline compared to running due to the contraction types involved. Cycling as a nonweight-bearing activity uses predominantly concentric muscle activation compared with the stretchshortening activity and eccentric activations during running. As older adults show different rates of decline during fatiguing contractions that involve eccentric as opposed to concentric activations, this may explain the differences in the rate of decline in cycling versus running ."
846,inserm-00752141.training.tei.xml,14,The best age to perform in ultra-triathlons
847,inserm-00752141.training.tei.xml,15,"Since the age-related performance started to decline later in an Ironman distance triathlon than in an Olympic distance triathlon, we hypothesised that the agerelated decline in performance would occur later for Deca Iron ultra-triathletes than for Triple Iron ultra triathletes and the fastest performance in a Deca Iron ultra-triathlon would be achieved at a higher age than in a Triple Iron ultra-triathlon. In contrast to our hypothesis, the age of the best overall performance did not differ between Triple Iron ultra-triathlon and Deca Iron ultratriathlon and was comprised between25 and 44 years for both ultra-triathlon distances. However, the group with the largest participation at the Triple Iron ultra-triathlon (35 to 39 years) was younger compared to the Deca Iron ultra-triathletes (40 to 44 years). We analysed in the present study the age-related decline in Triple Iron ultra-triathletes competing for approximately 2,000 min compared to Deca Iron ultra-triathletes racing for approximately 12,000 min. A significant age effect was found for total race time with no difference between the distances. A previous study evidenced that the age-related decline in total performance was less pronounced for the Olympic distance triathlon (approximately 2 to 3 h) than for the Ironman distance triathlon (approximately9to15h) . The present data showed that, in ultra-triathlons, the athletes were able to maintain their best performances for ages comprised between 25 and 44 years, independent of the distance. In half-marathoners , marathoners  and short distance triathletes , generally athletes in the age group 20 to 30 years showed the fastest race times compared to athletes in the other age groups. For longer distances such as the Ironman triathlon, athletes in the age group 30 to 39 years were faster compared to athletes in the age group 20 to 29 years . Apart from training, also the aspect of pre-race experience seems to be important for a successful finish in an ultra-triathlon. It has been demonstrated that the personal best time in a Triple Iron triathlon, not anthropometry or training volume, was associated with total race time in a Triple Iron triathlon . Furthermore, the number of finished Triple Iron triathlons and the personal best time in a Triple Iron triathlon, but not anthropometry, were also related to Deca Iron ultratriathlon race time ."
848,inserm-00752141.training.tei.xml,16,"It would be interesting in future studies to perform a longitudinal observation and to compare the age-related decline from cross-sectional and longitudinal data. A short analysis of the present data showed thatfour ultratriathletes finished at least eight times the Triple Iron ultra-triathlon between 1992 and 2010. For these four ultra-triathletes, their total performances did not significantly change across the ages (Figure 2). These findings suggest that some exceptional ultra-triathletes are able to perform at the same level of performance during an 8-(subject A) to 16-year (subject D) period."
849,inserm-00752141.training.tei.xml,17,"Tanaka and Seals demonstrated that the changes in endurance performance with aging were attributed to reductions in VO 2 max and the decline in lactate threshold. Findings for competitive long-distance runners indicated that the decline in running times parallel the age-related reductions in VO 2 max and in lactate threshold . For runners, mean VO 2 max declined from 71.4 ml Á min −1 Á kg −1 in youth to 41.8 ml Á min −1 Á kg −1 at a mean age of 56.6 years . The decrease in an ultra-endurance performance may be attributed to an age-related loss in skeletal muscle mass. A large component of lost independence with increasing age is weakness due to a loss of lean muscle mass. Muscle power is lost at a greater rate than is endurance capacity with 3.5% versus 1.8% per year, respectively . The agerelated loss of muscle mass is primarily due to a decrease in the size of type II (fast-twitch) muscle fibres and a change in the proportions of fibre types . There is an increase in the proportion of type II fibres, which may adapt to high endurance demands ."
850,inserm-00752141.training.tei.xml,18,"The age-related decline was more pronounced in running compared to cycling for both the Triple Iron ultratriathletes and the Deca Iron ultra-triathletes. Running involves stretch-shortening cycles with eccentric muscle actions, while cycling is a nonweight-bearing activity with predominant concentric muscle actions. With increasing age, the fast-twitch muscle fibres atrophy more than slow-twitch muscle fibres. Since the fast-twitch fibres seem to be more susceptible to damage than the slow-twitch fibres during stretch-shortening cycles, the greater reduction in running performance compared with cycling could, therefore, be related to muscle typology changes with increasing age . However, the changes in muscle fibre type distribution such as the percentage of type I muscle fibres with advancing age seem to be less pronounced in well-trained master athletes compared to untrained older adults ."
851,inserm-00752141.training.tei.xml,19,"A further aspect in these ultra-endurance races is the loss of substantial body mass such as skeletal muscle mass and fat mass. In ultra-endurance races, a loss in body mass has been described, contributing to a decrease in solid masses. An ultra-endurance performance leads to a decrease in both skeletal muscle and fat masses . In Ironman , Triple Iron ultratriathletes , and Deca Iron ultra-triathletes , a significant decrease in skeletal muscle mass has been described. It is, therefore, likely that the more pronounced age-related decline in running performance in the Deca Iron ultra-triathletes was rather due to the considerably longer nature of the race with the corresponding loss in skeletal muscle mass than the effect of age. The decrease in skeletal muscle mass is most probably due to the increased mechanical stress due to the eccentric nature of running. It has been demonstrated that longer distance running in an ultra-marathon induced more impact-stress on the skeletal muscle than a marathon ."
852,inserm-00752141.training.tei.xml,20,"The first limitation of this cross-sectional study is the problem arising from the data collection. Firstly, it is not sure at100% that there are no data errors (age, race time) when taking them from the race websites and the official rankings. Secondly, we cannot assure that the age-related performance decline is not at least partly due to non-participation of older athletes or simply to a selfchosen lower intensity of older athletes. In addition, race participants may represent selected sub-groups of the total population of ultra-triathletes, e.g. some potential participants may have been prevented from participating due to many reasons."
853,inserm-00752141.training.tei.xml,21,"It was not possible to collect other performance-related factors such as characteristics of training, anthropometry  and previous experience . Thus, these factors had to be ignored in this study. In addition, we have not included environmental conditions. It has been shown that an ultra-performance  progressively slows down when the ambient temperature increases ."
854,inserm-00752141.training.tei.xml,22,"This cross sectional study found that the magnitudes of age-related declines in ultra-triathlon performance for Triple Iron and Deca Iron triathlon are quite similar. For both Triple Iron and the Deca Iron ultra-triathletes, the fastest race times were achieved between25 and 44 years of age, although participants at the Triple Iron triathlon were on average younger compared to the Deca Iron triathlon. Further studies should investigate the motivation and training of ultra-triathletes to gain better insights in ultra-triathlon performance."
855,inserm-00752141.training.tei.xml,23,The data sets supporting the results of this article are available in http://www.triathlonlensahn.de and http:// www.multisport.com.mx.
856,inserm-00752141.training.tei.xml,24,"This study was approved by the Institutional Review Board of St. Gallen, Switzerland, with a waiver for the requirement of an informed consent given by the subjects since the study involved the analysis of publicly available data. Fortwo annual ultra-triathlons, one Triple Iron triathlon and one Deca Iron triathlon, the age of the athlete in the year of the race together with the swimming, cycling, running and total race times in the year of each race were analysed from 1992 to 2010. During this period, both the Triple Iron triathlons in Lensahn, Germany and the Deca Iron triathlons in Monterrey, Mexico were held regularly. The data set from this study was obtained from the race websites www.triathlonlensahn.de for the Triple Iron triathlon Germany in Lensahn, www.multisport.com.mx for the Deca Iron triathlon in Monterrey, Mexico and from the race directors involved."
857,inserm-00752141.training.tei.xml,25,"The Triple Iron ultra-triathlon in Germany took place in Lensahn, Schleswig-Holstein, Germany, and was comprised of11.6-km swimming, 540-km cycling and 126.6km running. The swimming was held in a 50-m heated outdoor pool at a temperature of approximately 25°C, and wetsuits were allowed. After passing through the transition area, the participants cycled 67 laps of 8 km each on a hilly course in the surroundings of the town. At the next transition, the athletes changed and ran 96 laps of a 1.31-km per lap flat run course in the town of Lensahn. The cycling course was nearly free of road traffic, and the run course was completely free of traffic and illuminated during the night. All the athletes had their own support crew to provide nutrition and changes of clothes or equipment. The athletes had to arrive at the finish line within 58 h of the race start. The Deca Iron ultra-triathlon took place in Monterrey, Mexico. The athletes completed a total distance of 38-km swimming, 1,800-km cycling and 422-km running. Swimming commenced in the 50-m outdoor pool in Monterrey in the 'Sociedad Cuauhtemoc & Famosa Park' , 3 km away from the cycle and run track in 'Parque Niños Héroes'.T h e pool was not heated and the water temperature was approximately 27°C. The laps of 100 m were counted by personal lap counters for each athlete. After completing the swimming, the athletes changed in the transition area, and due to the high traffic volume, were transferred by car to 'Parque Niños Héroes'. A period of 30 min was allowed for the transfer from pool to park, which was deducted from the final race time. The park was closed to traffic, completely illuminated and with a 1.915-km cycle/run track that is approximately 95% flat but included an inclination of approximately 5%. After the cycling, the athletes changed and went directly to the run course which was on the same track but in the opposite direction. Drafting in the cycle section was strictly prohibited and controlled by the race director. The laps on both the cycle and run courses were counted electronically using a microchip system. The athletes can be helped by their own support crew for nutrition and changes of equipment and clothes. During the whole race, accommodation is offered in the Sports Village inside the park, about 250 m away from the race site. The athletes and their support crews have a room with bed, toilet and shower. For nutrition, the organiser offered a variety of food in a restaurant, at the race site, that was open 24 h a day."
858,inserm-00752141.training.tei.xml,26,"In each race year, the age of the winner and the mean age of the male finishers were analysed at both the Triple Iron ultra-triathlons and the Deca Iron ultra-triathlons from1992 to 2010."
859,inserm-00752141.training.tei.xml,27,"In order to analyse the age-related changes in swimming, cycling, running and the total race performances, we pooled the data from1992 to 2010 for both distances.  Performance times were converted to minutes. For the Triple and Deca Iron ultra-triathlons, the fastest ten swimming, cycling and running times, together with the total event times for each age group were determined for the 19-years period."
860,inserm-00752141.training.tei.xml,28,"Data are reported as means ± standard deviations (±SD) in the text and the figures. Linear regressions were used for estimating the changes in the age of winners and the mean age of finishers per year. Pearson's correlation coefficients were used to assess the association between the age and the years. One-way ANOVA was used to compare the ages of the Triple and the Deca Iron ultratriathlon winners and the mean ages of the finishers. One-way ANOVA was used to compare the swimming, cycling, running and total race times between the different age groups for both distances. Tukey's post hoc analyses were used to test differences within the ANOVA when appropriate. Statistical significance was accepted at p <0.05 (Statsoft, Version 6.1, Statistica, Tulsa, OK, USA)."
861,inserm-00752141.training.tei.xml,29,performed the statistical analyses and helped draft the manuscript. All authors read and approved the final manuscript.
862,inserm-00752141.training.tei.xml,30,The authors would like to thank the race directors for providing the race results and Mary Miller for her help in translation.
863,inserm-00752141.training.tei.xml,31,The authors have no conflict of interest.
864,1408.2792.training.tei.xml,0,"Although the nucleus of comet C/2013 A1 (Siding Spring) will safely pass Mars inOctober 2014, the dust in the coma and tail will more closely approach the planet. Using a dynamical model of comet dust, we estimate the impact fluence. Based on our nominal model no impacts are expected at Mars. Relaxing our nominal model's parameters, the fluence is no greater than ∼ 10 −7 grains m −2 for grain radii larger than 10 µm. Mars orbiting spacecraft are unlikely to be impacted by large dust grains, but Mars may receive as many as ∼ 10 7 grains, or ∼ 100 kg of total dust. We also estimate the flux of impacting gas molecules commonly observed in comet comae."
865,1408.2792.training.tei.xml,1,"Comet C/2013 A1 (Siding Spring) will pass Mars with, and a relative speed of 55.96 km s −1 on 2014 Oct 19 at 18:29±:03 UTC (3-σ uncertainties; Farnocchia et al. 2014). The nucleus will miss the planet, its moons, and orbiting spacecraft. However, given the right combination of ejection velocity, ejection time, and response to radiation pressure, dust grains from the comet can reach the planet. Farnocchia et al. (2014) predict that Mars will miss the comet's orbit by 2.7 × 10 4 km at 20:10 UTC. This second close approach potentially reduces the energy required to place dust grains on impacting orbits. We present models of the dust and gas based on the summary of the comet's activity by Farnham et al. (in preparation) and estimate the impact hazard for Mars and its satellites as well as the comet gas flux at Mars."
866,1408.2792.training.tei.xml,2,"To assess the impact hazard, we generatedtwo simulations of 10 9 particles each, picked from broad parameter ranges. These raw simulations act as guides to determine which combinations of size, ejection speed, and ejection time may result in impacts. Next, we define more limited parameter sets that are carefully chosen to match known parameters of the comet. We select and weight particles from the raw simulations that match those sets, and use them to estimate the fluence at Mars. Below we describe our dynamical model, the raw simulations, and four parameter sets used to estimate the impact hazard."
867,1408.2792.training.tei.xml,3,"The circumstances of the encounter are simulated with the dynamical model of Kelley (2006). For this study we msk@astro.umd.edu use the JPL ephemeris solution #46 ( Farnocchia et al. 2014). In order to reduce the required computational time, we modified the model to use thetwo-body (Keplerian) propagation functions from NASA's Navigation and Ancillary Information Facility SPICE toolkit. Dust grains are parameterized by β, the ratio of the force from solar radiation pressure to the force from solar gravity: β = 0.57Q pr /ρa, where Q pr is the radiation pressure efficiency, ρ is the grain density in units of g cm −3 , and a is the grain radius in units of µm ( Burns et al. 1979). In the Keplerian solution, the gravitational force from the Sun is reduced by the factor (1 − β)."
868,1408.2792.training.tei.xml,4,"The magnitude of the error introduced by neglecting planetary perturbations can be estimated by comparingzero-ejection velocity syndynes (lines of constant β with variable ejection times; Finson & Probstein 1968) generated using the Keplerian solution to those generated using the original code. The distances between the syndynes define the error. For grains ejected up to 4 years before the closest approach, the error is at most 300 km for dust found within 10 6 km from the nucleus. We also considered whether the gravitational pull of Mars is significant. Ignoring the atmosphere, particles grazing the surface are displaced < 100 km at closest approach, and the cross-section enhancement factor from gravitational focusing by Mars is 1.008 (Jones & Poole 2007). The Keplerian solution is sufficient for our purposes."
869,1408.2792.training.tei.xml,5,"Simulation 1 contains10 9 particles selected from the following parameters, based on observations of the comet with a generous conservative margin: ages range uniformly from 0 to 4 yr (out to r h = 13 AU); expansion speeds range uniformly from 0 to v ref (a/1 mm) −0.5 (r h /5 AU) −1 , where v ref = 1.9 m s −1 is the expansion speed of 1 mm grains ejected at 5 AU from the Sun; ejection velocities are radial and isotropically distributed around the nucleus; and, radii are selected from a distribution uniform in log-space (dn/d log a ∝ 1) ranging from 10 to 10 4 µm. The logarithmic distribution ensures our final results will have a statistically uniform representation of each size decade. For the conversion from radius to β we assume a grain density of 1 g cm −3 , and Q pr = 1. J.Y. Li et al. (in preparation) imaged Siding Spring at 4.6, 3.8, and 3.3 AU from the Sun with the Hubble Space Telescope WFC3 instrument. The high spatial resolution of the images (40 mas per pixel, corresponding to ≥ 100 km per pixel) resolve the inner coma, and allow investigations of the dust grain expansion velocities. Farnham et al. (in preparation) analyzed those images and found that the dust that comprises the bulk of the coma and tail has speeds best matched by v ref (a/1 mm) −0.6 (r h /5 AU) −1.5 , for v ref = 0.42 m s −1. Grains with these speeds are a subset of simulation 1."
870,1408.2792.training.tei.xml,6,"For an alternative scenario we consider the analysis of the Hubble images by Li et al.. Using the distance from the nucleus to the outer edge of the coma in the sunward direction, they find that the heliocentric distance dependence of the resulting speeds follow v ej ≈ 800 r −2 h m s −1 for r h =3.3 − 4.6 AU, assuming β = 1 grains (v ref ≈ 0.76 m s −1 ). Speeds based on this relationship for r h < 2.0 AU exceed those in simulation 1. Therefore, we ran a second 10 9 particle simulation (simulation 2) with speeds picked uniformly from 0 to v ref (a/1 mm) −0.5 (r h /5 AU) −2 for v ref = 1.9 m s −1. However, we note that these speeds, derived from some of the fastest moving grains in the coma, are much higher than the Farnham et al. results, derived from the coma and tail morphologies. Analyses based on the Li et al. speeds will serve as upper-limit cases for fast moving grains not accounted for in the Farnham et al. approach. In order to transform the simulations into an impact hazard at Mars, we first rotate the position vectors from the ecliptic J2000 coordinate frame into a reference frame defined at closest approach: the x-axis is given by the comet-Mars position vector, the y-axis by the cometMars velocity vector, and the z-axis by the right-hand rule. Simulations 1 and 2 projected into this reference frame is shown in Fig. 1. The relative timing of grains arriving at Mars is based on their y-axis position; 10 5 km corresponds to a time difference of 29.8 min. We then weight each particle to remove the bias introduced by our raw simulation's grain size distribution, and to provide a real estimate of the grain's frequency of occurrence in the comet coma. The particle weights are based on the parameter sets in Table 1."
871,1408.2792.training.tei.xml,7,"Set A is the nominal case, directly based on the results of Farnham et al. (in preparation). Sets B, C, and D are variations chosen to provide upper-limit estimates on the impacting dust. First, consider that the tail analysis of Farnham et al. best describes the smallest grains (a10 µm) and closest heliocentric distances (3 AU ≤ r h 5 AU), and is extrapolated to larger a and r h. Therefore, as an alternative scenario, we have defined set B using a radius and heliocentric distance dependence in closer agreement to theoretical predictions:"
872,1408.2792.training.tei.xml,8,"Set C has the same parameters as set B, but the dust is shifted closer to Mars according to the ephemeris's 3σ error ellipse ( Farnocchia et al. 2014). In our closest approach reference frame described above, the dust is displaced by ∆X =4471 km, ∆Z = −1722 km."
873,1408.2792.training.tei.xml,9,"Set D is based on our nominal case, but uses simulation 2 and the ejection speeds found by Li et al. (in preparation). As described above, these speeds are representative of the fastest moving grains, and not the whole coma."
874,1408.2792.training.tei.xml,10,"The future activity of comet Siding Spring is challenging to predict, but must be based on our present set of observations. We make two predictions of the gas production rate at Mars, based on these measurements of CO 2 and H 2 O."
875,1408.2792.training.tei.xml,11,"First, we propagate the measured CO 2 production rate from3.1 AU to 1.4 AU, then compute the water production rate using an assumed CO 2-to-H 2 O mixing ratio. The mean mixing ratio, based on Table 1 To estimate the gas fluence and peak volume density at the top of the Martian atmosphere, we modeled the coma with a modified three-generation Haser model (Festou 1981; Combi et al. 2004;Bodewits et al. 2011), considering H 2 O, CO 2 , CO, and CN. The relative abundances of CO and CN are based on the mean CO-to-H 2 O mixing ratio (0.07) from A' Hearn et al. (2012), and the ""typical"" composition of dynamically new comets from A' Hearn et al. (1995). We also include OH, H, and O as photodissociation products of H 2 O. Our results are given in Table 2."
876,1408.2792.training.tei.xml,12,"The raw simulations provide a guide to understanding what combination of grain size, ejection velocity, and age results in an impact hazard at Mars. We define impacts at Mars as any particle found within a distance of10,000 km from the center of the planet. This distance includes the orbit of Phobos (9400 km semi-major axis), and the apoapsis of the MAVEN spacecraft's nominal science orbit (6000 km; provided by NASA JPL). We also investigate a 5000 km region centered on the posi Table 1 Model parameters and results."
877,1408.2792.training.tei.xml,13,Set A Set B Set C Set D
878,1408.2792.training.tei.xml,14,"Gas production rate at3.1 AU, Qg (kg s −1 ) 25.7 tion of Deimos (23,000 km semi-major axis) at the time of closest approach. A third region of interest is based on the 48,000 km apoapsis of MAVEN 's orbit, assuming a 5-week delay in science operations. Given these criteria, the grain parameters that yield impacts are presented in Fig. 2. The only particles in simulation 1 that reach the Martian system are those ejected with speeds of a few meters per second, have radii of 0.7-3.6 mm, and are ejected at least 1.5 yr prior to the encounter."
879,1408.2792.training.tei.xml,15,"3.2. Impact Hazard For each parameter set, the impact hazards are computed by taking the set of particles found in each region of interest, removing those speeds outside the set's range, and weighting remaining particles according to the set's total production rate, grain size distribution, etc. Table 1 lists, for Mars, the raw number of impacting grains (i.e., before particle weighting), the impact fluence (i.e., after particle weighting), and the start and stop times of the hazard. No impacts are expected based on our nominal model, nor the v ej ∝ r −2 h model (sets A and D) because the ejection speeds of dust grains at r h >3 AU are too low to place particles within the vicinity of Mars (Fig. 2). In addition, displacing the comet dust in these two models according to the ephemeris 3-σ uncertainty ellipse does not result in any impacts."
880,1408.2792.training.tei.xml,16,"In order to attain impacting particle trajectories, higher ejection speeds are needed at r h >3 AU. This requirement is accomplished with the size-r h-speed relationship of parameter set B, resulting in a total fluence of 1 × 10 −7 grains m −2. Based on the comet's current predicted closest approach time, the grains arrive between 19:57 and 20:17 UTC (time at Mars), nearly centered on the epoch at which Mars crosses the comet's orbital plane (Farnocchia et al. 2014). Note that nongravitational forces frequently act upon comet nuclei, but we can neglect these effects because the dust is ejected at heliocentric distances where activity and resulting nongravitational forces were low (see Fig. 5 of Farnocchia et al. 2014). However, the current nucleus ephemeris uncertainty is still valid for the dust. Displacing the dust in parameter set B closer to Mars (set C) shifts the arrival time 5 min earlier and increases the fluence by a factor of 4."
881,1408.2792.training.tei.xml,17,"The fluence results are summarized for Mars and MAVEN in Fig. 3. The grains are limited to1 to 3 mm in radius, due to the combined effects of radiation pressure, grain age, and ejection speed. The timing of the hazard at MAVEN 's distant apoapsis is 25 min later than the hazard at Mars, and the fluences are a factor of 2 higher. In all simulations, the Deimos region of interest remained dust free. Gas outflow and nucleus gravity limits the size of dust grains that can be placed into heliocentric orbit. Meech & Svore˘ n (2004) present the critical dust radius, a crit , that can be lifted off a spherical nucleus by estimating the drag force on a spherical grain, and integrating the resulting equation of motion,"
882,1408.2792.training.tei.xml,18,"where µ is the atmoic weight of the driving gas, m H is the mass of hydrogen, Q is the gas production rate (molecules s −1 ), v th is the expansion speed of the gas, ρ g and ρ n are the grain and nucleus densities, R n is the nucleus radius, and G is the gravitation constant. Let v th = 0.6 (r h /3.8 AU) −0.5 km s −1 to be consistent with Farnham et al. (in preparation), and let the nucleus density be0.3 g cm −3. The radius of the nucleus has not been measured, but Bodewits et al. (2014) estimate R > 0.34 km based on water projection rates from Swift photometry. Therefore we consider two values: 0.5 and 2.0 km. For our adopted parameters, CO 2 can lift grains larger than 100 µm at heliocentric distances inside of 14 and 8 AU for R n = 0.5 and 2.0 km, respectively. However, our parameter set ejection speeds only place particles on impacting trajectories for r h > 11 AU (Fig. 2). Thus, given this simplistic model, an impact hazard may not be expected if the nucleus radius is ∼ 2 km or larger."
883,1408.2792.training.tei.xml,19,"Overall, we do not expect any impacts on Marsorbiting spacecraft. Based on the cross-sectional area of Mars and our10,000 km average fluence, the planet may receive up to ∼ 10 7 grain impacts from 1-to 3-mm-radius grains, totaling ∼ 100 kg, based on our models. At most, a few impacts may be expected on Phobos ( 100), and no impacts at Deimos. (in preparation) data, the same as our investigation, but with slightly different interpretations. They both predict little to no risk of impacts for the Mars-orbiting spacecraft, despite their independent approaches. In particular, Ye & Hui (2014) used observations of Siding Spring and the similarly bright comet C/2012 S1 (ISON) to derive the dependence of ejection speed on size and r h :"
884,1408.2792.training.tei.xml,20,"best matched their data, assuming a2.5-km-radius nucleus and 0.3-g cm −3 dust, resulting in no impacts for a > 0.1 mm, and a fluence of 2.6 × 10 −6 grains m −2 for grains down to 10 µm in radius. In our parameterization, these speeds correspond to v ref = 1.0 m s −1 , higher than our set B. Using their speeds and our production rate history, we find the same fluence, 2 × 10 −6 grains m −2 , but all impacts are millimeter sized. Finally, we compare our results to the natural background of meteoroids estimated for the Mars Reconnaissance Orbiter. Over a 5-year period, the total fluence for meteoroids with a > 1 mm is 0.0021 grains m −2 , and for a > 0.1 mm, it is 3.1 grains m −2 (Newell 2005 of magnitude smaller than these values, and we conclude the comet poses little additional hazard to the spacecraft."
885,1408.2792.training.tei.xml,21,"The neutral coma gases will enter the atmosphere with a relative velocity of56 km s −1. The kinetic energies of these molecules greatly exceeds their dissociation energies, e.g., for H 2 O, the kinetic energy is 293 eV, and the dissociation energy (H 2 O → OH + H) is 5 eV (Darwent 1970). The upper atmosphere of Mars consists of CO 2 , with few percent contributions from N 2 and Ar, and trace amounts of several other species (Krasnopolsky 2002). The comet gases will collide with the Mars gases, and quickly dissociate into atoms, erasing any molecular trace of the comet in the Martian atmosphere. The peak total particle and kinetic energy fluxes in Table 2"
886,1408.2792.training.tei.xml,22,"Based on the observations of Farnham et al. (in preparation) and Li et al. (in preparation), we simulated the coma and tail of comet Siding Spring for grains with radii between10 µm and 1 cm (ρ = 1.0 g cm −3 ), and particle ages out to 4 years (13 AU) before closest approach to Mars. We predict no dust impacts at Mars from the close flyby of the comet in October 2014. Variations of our nominal comet model suggest a total fluence of 10 −7 grains m −2 is possible, with radii ranging from 1 to 3 mm, and encounter times between 19:52 and 20:17 UTC (time at Mars). Mars orbiting spacecraft are unlikely to be impacted by any large dust grains, but Mars may receive as many as ∼ 10 7 grains (∼ 100 kg). Following Vaubaillon et al. (2014), the meteor shower at Mars is an Earth-equivalent zenith hourly rate 600 h −1 (assuming a human perception correction factor of 3 given our meteor size range; Koschack & Rendtel 1990). The gas coma will reach the upper atmosphere of Mars with peak fluxes of order 10 12 molecules m −2 s −1 , and the molecules will be quickly dissociated, due to the high impact speeds. This research was supported by a contract to the University of Maryland by the NASA JPL Mars Critical Data Products Program."
887,1408.2792.training.tei.xml,23,"The work of D. Farnocchia was conducted at the Jet Propulsion Laboratory, California Institute of Technology under a contract with NASA."
888,1408.2792.training.tei.xml,24,"Simulations were performed on the YORP cluster administered by the Center for Theory and Computation, part of the Department of Astronomy at the University of Maryland."
889,1408.2792.training.tei.xml,25,"This research made use of Astropy, a communitydeveloped core Python package for Astronomy (Astropy Collaboration et al. 2013)."
890,hal-00987664.training.tei.xml,0,"Sjögren's syndrome (SS) is a chronic autoimmune disorder, affecting exocrine glands, mainly salivary and lacrimal glands, leading to the designation of SS as an autoimmune exocrinopathy or autoimmune epithelitis (Mavragani and Moutsopoulos, 2013). The clinical manifestations include dry mouth (xerostomia), dry eyes (keratoconjunctivitis sicca), and systemic features. Histological examination shows peri-epithelial mononuclear cell infiltrates in exocrine glands and parenchymal organs such as kidney, lung, and liver. Patients with SS have a20-40 fold increased risk of developing lymphoma (Voulgarelis et al., 1999;Baimpa et al., 2009;Tobon et al., 2010). Furthermore, SS is characterized by the presence of circulating autoantibodies (Ab) against the sicca syndrome (SS)A/Ro and SSB/La ribonucleoprotein particles ( Hu et al., 2011)."
891,hal-00987664.training.tei.xml,1,"The pathogenesis of SS remains unclear. Its etiology is multifunctional and includes a combination of genetic predispositions, environmental factors, and epigenetic factors ( Cobb et al., 2008;Mavragani and Moutsopoulos, 2010;Le Dantec et al., 2012; Liu and La Cava, 2013). Genetic factors associated with SS are particular HLA-DR allele subtypes and specific gene polymorphisms including STAT4, IL-12A, TNIP1, IRF5, BLK, and CXCR5 ( Lessard et al., 2013). Most of these genetic mutations are far from DNA-coding regions and they are suspected to alter disease-susceptibility genes by altering their expression indirectly via an action on the epigenetic machinery ( Dozmorov et al., 2014)."
892,hal-00987664.training.tei.xml,2,"Epigenetics can be defined as changes in gene expression, that are inheritable and that do not entail changes in the DNA sequence."
893,hal-00987664.training.tei.xml,3,"Epigenetics also explains how cells can differentiate into alternative cell types and how a phenotype can be passed from one cell to its daughter cells (Delgado-Vega et al., 2010). Epigenetic mechanisms are important to control the pattern of gene expression during development, the cell cycle, and in response to biological or environmental changes. As a consequence epigenetic dysregulations have been linked with autoimmune diseases including SS ( Brooks et al., 2010;Renaudineau, 2010)."
894,hal-00987664.training.tei.xml,4,"Chromatin is located in the nucleus of eukaryotic cells, and consists of DNA, histones, transcription complexes, chromatin modifying enzymes, and other proteins to form chromosomes. Chromatin is divided intotwo forms, euchromatin and heterochromatin. Euchromatin is a loosely packaged form of chromatin that has a high concentration of genes and is often involved in active transcription or is at least potentiated for transcription. In contrast, heterochromatin is a denser packaging of DNA and proteins, which has many varieties. Heterochromatin is considered to be transcriptionally inactive since the genes are less accessible to transcription factors (Figure 1). Changes in chromatin's structure are affected by chemical modifications of histones such as methylation and acetylation, and by the binding of other proteins and ions to the DNA. Most of the modifications are reversible, providing dynamics to the chromatin structure and activity that allows for opening of previously sequestered genes or suppression of previously active genes in response to stimuli and cell cycle progression. Charges on the histones are shown as blue (positive), red (negative), and white (neutral). The positive charges of histones facilitate binding to negatively charged DNA and thereby packaging the DNA to reduce the overall volume needed for the chromatin. Euchromatin is an extended form of chromatin that allows access to the underlying genes. Heterochromatin is a condensed form with histone H1 (orange) bound to the linker DNA between nucleosomes. This can compact the chromatin further and sequesters the underlying genes. The chromatin is further organized into loops that are usually tightly coiled and stacked into chromosomes but the loops can rapidly uncoil into extended states as depicted to allow access to sequestered genes."
895,hal-00987664.training.tei.xml,5,"The basic unit of chromatin is the nucleosome, a nucleoprotein complex that consists of DNA wrapped around an octameric core of histones (two each of histones H2A, H2B, H3, and H4). Nucleosomes occur on average every 200 base pairs (bp) in chromatin with 145 bp (core DNA) in close contact with the histone core and another approximately 55 bp in the linker section (linker DNA) between nucleosome cores. The abundance of positively charged arginine and lysine residues in histones are sufficient to counter roughly half of the negative charges in the core DNA, thereby reducing the self-repulsion of the DNA and allowing a sevenfold compaction of the length held in DNA coils around the histone core. Histones are small globular proteins (11-15 kD) with flexible N-terminal tails that project from the nucleosome core. Epigenetic modification sites in the chromatin structure consist primarily of lysine, arginine and serine residues in the N-terminal tails of core histones that are post-translationally modified by acetylation, methylation, phosphorylation, ubiquitination/sumoylation, ADP ribosylation, deimination/citrullination, protein conjugation, or β-N-acetylglucosamination. As a consequence, the N-terminal tails can undergo modifications that add to or reduce the interactions of the histones on neighboring histones, DNA and nuclear proteins therefore affecting gene transcription ( Huber et al., 2007;Dieker and Muller, 2010). For example, positively charged lysine residues can non-covalently bind to the negatively charged phosphate groups in the DNA but acetylation of a lysine residue removes the positive charge and reduces the histone-DNA interaction. An additional histone, histone H1, can bind to the linker DNA between nucleosomes allowing stacking of nucleosomes to provide further compaction of the chromatin. Histone H1 is more abundant in heterochromatin. The main histone modifications are listed in Table 1."
896,hal-00987664.training.tei.xml,6,"Besides post-translational modifications of histones, epigenetic control can be affected by the histone variants present in chromatin. For example, macroH2A, a variant of the canonical H2A histone, greatly increases stability of nucleosomes, reduces chaperone-mediated exchange of H2A-H2B dimers and it suppresses acetylation of histones in nucleosomes (Bonisch and Hake, 2012). The inactive X chromosome in females is kept in a heterochromatic state due in part to a high incorporation of macroH2A. On the other hand, other histone variants provide epigenetic modification sites that allow nucleosome dynamics. The modifications can be as described above and can also include N-terminal proteolytic sites by which the histone is irreversibly altered to remove potential modifications and interactions of the N-terminal tail."
897,hal-00987664.training.tei.xml,7,"DNA methylation is the covalent addition of a methyl group from the methyl donor S-adenosylmethionine (SAM) to the cytosine residues of CpG dinucleotides. It takes place at position 5 of the pyrimidine ring within the CpG dinucleotides.70% of CpG sites in human DNA are methylated in pairs, while most of the unmethylated CpGs are situated in CpG islands. DNA methylation is catalyzed by DNA methyltransferases (DNMTs). DNMTs are enzymes, which transfer a methyl group to CpG sites in DNA molecules, thereby influencing transcriptional activity."
898,hal-00987664.training.tei.xml,8,"Three phylogenic DNMTs exist in mammals: DNMT1, DNMT3a, and DNMT3b. DNMT1 shows a preference for hemimethylated DNA in vitro are making it the main DNMT, whereas DNMT3a and DNMT3b methylate unmethylated and methylated DNA at an equal rate and serve in a de novo DNMT role."
899,hal-00987664.training.tei.xml,9,"The insertion of a methyl group into DNA leads to structural changes of chromatin and is associated with gene silencing, by binding methyl-CpG-binding proteins, such as MeCP2 and MBD2, which then recruit chromatin inactivation complexes containing histone deacetylases (HDACs) and histone methyltransferases (HMTs)."
900,hal-00987664.training.tei.xml,10,DNA methylation and histone modifications regulate gene expression by modulating the packaging of the DNA inside the nucleus. DNA methylation may also interfere with the binding of some transcription factors.
901,hal-00987664.training.tei.xml,11,"A link between demethylating drugs and SS has been known since Cannat and Seligmann (1968) demonstrated that oral administration of hydralazine or isoniazid to mice forseveral weeks led to development of SS with immunological features of a systemic lupus erythematosus (SLE)-like disease including antinuclear antibody detection. This effect disappeared after discontinuation of the drug and variations were observed depending on the animal strain, age, and sex (Cannat and Seligmann, 1968). Because of their capacity to inhibit DNMT activity and to remove a methyl group from cytosines present in CpG islands, hydralazine and procainamide were referred to as demethylating agents (Taylor, 1968). Used to prevent hypertension, hydralazine was reported to induce SS in humans ( Darwaza et al., 1988;Brooks, 2010)."
902,hal-00987664.training.tei.xml,12,"Following this, Richardson's group demonstrated that hypomethylated CD4 + T cells become autoreactive. Experiments with passive transfer of CD4 + T cells pretreated with either oftwo distinct DNMT inhibitors, 5-aza-2 񮽙-deoxycytidine or procainamide, into mice showed an induction of anti-dsDNA antibody (Ab) production with the characteristics of a severe immune complex glomerulonephritis ( Quddus et al., 1993). Similar results were obtained when B cells were used instead of CD4 + T cells ( Mazari et al., 2007)."
903,hal-00987664.training.tei.xml,13,"Human endogenous retroviruses (HERVs) have spread, by reversing the normal flow of genetic information from DNA to RNA, throughout the human genome and represent up to7% of the genome ( Balada et al., 2009). For the most part, HERV genes contain deletions, stop codons or frame shifts, such that no retroviral protein is expressed. However, a few copies have retained their ability to generate functional protein but are normally epigenetically suppressed."
904,hal-00987664.training.tei.xml,14,"The HERV-K and HERV-E families contain some of the most active retroviral elements in the human genome and increase the possibility that HERVs may have a role in human diseases ( Renaudineau et al., 2005a,b). The HERV-K family is the only group of HERVs that can produce intact viral particles and it's also one of the most transcriptionally active families as its members retain intact open reading frames (ORFs) which encode the viral particles (Tugnet et al., 2013)."
905,hal-00987664.training.tei.xml,15,"Human endogenous retroviruses are believed to play a role in the pathophysiology of several autoimmune diseases, especially rheumatic diseases such as RA and SLE; and, in addition, numerous reports have identified HERV elements in salivary gland epithelial cells (SGECs) from SS patients (Le Dantec et al., 2012;Tugnet et al., 2013). As a consequence of this expression, retroviral antigens are produced, and antibodies (Ab) to Gag and Env regions of HERVs have been reported in patients with autoimmune diseases. Garry et al. (1990) detected and identified human intracisternal A-type retroviral particle (hIAP), which is an endogenous antigen related to human immunodeficiency virus (HIV), in lymphoblastoid cells when co-cultured with homogenates of labial salivary glands from SS patients ( Garry et al., 1990;Brookes et al., 1992;La Placa et al., 2004). At the same time, Brookes et al. (1992) mentioned that the human T cell lymphotropic virus (HTLV) related endogenous sequence, HRES1, was overexpressed in the epithelium of labial salivary glands obtained from patients with primary SS (pSS). HRES1 regulation by DNA methylation was recently provided ( Garaud et al., 2009;Fali et al., 2013) Later, the HERVK113, and HRV-5 retroviral elements were found overexpressed in pSS patients ( Murovska et al., 2000;Moyes et al., 2005). More recently, using a RT-PCR approach we have observed that at leastone HERV-E element was detected when testing labial salivary glands from SS patients (Le Dantec et al., 2012)."
906,hal-00987664.training.tei.xml,16,"Using immunoblotting to test HIV antigen, Talal et al. (1990) observed that14/47 (30%) of pSS patients reacted with the HIV p24 capsid antigen although sera from these patients were not reactive against HIV gp41 and gp120 envelope antigens. Yamano et al. (1997) confirmed anti-HIV p24 reactivity in SS and provided, in addition, molecular and serological arguments to exclude HIV or HTLV1 viruses' infections. The explanation came with Brookes et al. (1992) who used synthetic peptides derived from HRES1 endogenous retrovirus and demonstrated that anti-HRES1 Ab, detected in 35% of pSS patients, cross-react with the HTLV1 p19 capsid antigen. Similarly, Hishikawa et al. (1997) prepared a recombinant p30 gag protein from HERV-E clone 4-1 and related Western blotting experiments showed that anti-HERV 4-1 p30 gag Ab were detected in 35% of patients with pSS, whereas no HERV 4-1 anti-p30gag antibodies were found in healthy donors."
907,hal-00987664.training.tei.xml,17,"There are many disagreements about the role of the X chromosome in the development of SS. First of all, the correlation between SS and the ratio of9:1 between females and males strongly suggests involvement of the X chromosome and, second, there are numerous reports of trisomy X (47, XXX) and of a super female phenotype (mosaic of XXXXX/XXXX/XXX/XX/XO) in female patients with SS (Ozcelik, 2008)."
908,hal-00987664.training.tei.xml,18,"Females inherit both maternal and paternal X chromosomes whereas men receive only the maternal X. The majority of X-linked genes are not sex-specific. Therefore, the majority of X-linked genes should have equivalent expression in males and females. X chromosome inactivation (XCI) is an epigenetic event initiated in each cell early in development that leads to the transcriptional silencing of one of thetwo X chromosomes in females. Consequently, the mechanism results in equal dosage of genes in males and females such that only one X chromosome is transcriptionally active in both sexes with equal amounts of the products of X-linked genes synthesized in cells (Brooks, 2010). The inactive chromosome is referred to as Xi and the active as Xa. The choice of which X chromosome to inactivate, the maternally derived or the paternally derived, is a random choice made in somatic cells early in the development of the embryo ( Lessing et al., 2013)."
909,hal-00987664.training.tei.xml,19,"As an example of the scale of chromatin, theeighth largest human chromosome, the X chromosome, has 150 million bp containing 1,098 genes, which is a relatively low gene density ( Ross et al., 2005). Approximately 750,000 nucleosomes are involved in the packaging of the X chromosome. In the case of the inactive X chromosome, roughly 85% of the genes are kept silent in order to attain dosage compensation of X-linked genes. Besides the histone modifications described above, silencing of the inactive X chromosome exemplifies the use of two other major epigenetic mechanisms to suppress genes: DNA methylation and non-coding RNAs. DNA methylation in which a methyl group is added to cytosine bases in a DNA strand alters the topology in the major groove of DNA, which can mask binding sites from transcription factors, thereby suppressing expression of the underlying gene. RNA can also play a role in epigenetics in four different ways: (1) as a structural RNA, such as the X inactivation specific transcript (XIST RNA) which coats the inactive X chromosome; (2) as an agent recruiting histone modifying enzymes, such as deacetylases, to a site; (3) as a direct block to prevent transcription factors from initiating gene transcription; and (4) as an agent to hybridize with nascent RNA transcripts leading to degradation by ribonucleases that target double-stranded RNA (Bernstein and Allis, 2005)."
910,hal-00987664.training.tei.xml,20,"It is known that women are affected more often by autoimmune diseases than men. In SLE, it was observed that increased DNA demethylation of the Xi was associated with over-expression of the X-linked CD40 ligand in CD4 + T cells purified from SLE women ( Lu et al., 2007). Recently, Belkhir et al. (2013) demonstrated that membrane CD40L was overexpressed in ex vivo activated CD4 + T cells from female patients with pSS. However, this overexpression occurs through non-epigenetic regulatory mechanisms as demonstrated when using DNA demethylating drugs and when testing the DNA methylation status of the regulatory regions of CD40L. This discordance between SLE and SS patients, when testing CD4 + T cells, may be interpreted in terms of cellular specificity based on the observation that DNA demethylation affects predominantly SGEC (see below). In addition, it is important to note that a sexual dimorphism is not restricted to the immune cells but sexual dimorphism was also reported in epithelial cells from salivary glands ( Konttinen et al., 2010)."
911,hal-00987664.training.tei.xml,21,"Patients with pSS have a higher risk of developing lymphoma, and especially a non-Hodgkin B cell lymphoma. The pro-apoptotic death associated protein kinase (DAP-kinase) gene is regulated by DNA methylation and Toso et al. (2009) demonstrated that DNA demethylation of this gene was restricted to the SS subgroup, compared to the non-SS subgroup, in which the gene was aberrantly hypermethylated. Another argument linking DNA methylation and lymphoma predisposition in SS is related to the hypermethylation of the runt-related transcription factor (RUNX1) gene in CD4 + T cells from SS patients (Altorok et al., 2014). Tobon et al. (2013) showed increased levels of Flt-3L in the sera of18 patients with pSS and previous lymphoma in contrast with patients with pSS without lymphoma. Additionally, the analysis of biological parameters showed that lymphocytopenia, low levels of C4 and high levels of Flt-3L were associated with previous occurrence of lymphoma ( Tobon et al., 2013). An epigenetic control of FLt3-L in SGECs is suspected and experiments should be conducted in SS patients to test this hypothesis."
912,hal-00987664.training.tei.xml,22,"Recently, we have reported in pSS patients, that global DNA methylation was reduced in labial salivary glands when comparing biopsy sections from pSS patients to controls, and that this defect was conserved when SGEC were primarily cultured . At the molecular level, SGEC global DNA demethylation was associated with a decrease in the methylating enzyme DNMT1 and an increase in the demethylating partner Gadd45alpha. These observations support an active DNA demethylation process in SGEC from SS patients. Furthermore, SGEC DNA methylation levels were inversely correlated with SS severity and B cell infiltration. The contribution of B cells in the demethylation process was further explored in SS patients treated with rituximab, a chimeric anti-CD20 monoclonal antibody from the TEARS study (Devauchelle-Pensec et al., 2007;Thabet et al., 2013). Indeed, global DNA methylation levels were higher4 months after B cell depletion in comparison with the minor salivary gland biopsy obtained at the initiation of rituximab therapy, thus suggesting that DNA demethylation in SGEC may be attributed in part to the presence of infiltrating B cells. This hypothesis was confirmed in vitro, revealing in co-culture that B cell mediated DNA demethylation in SGEC works through an alteration of the Erk/DNMT1 pathway."
913,hal-00987664.training.tei.xml,23,"Treatment with 5-aza-2'-deoxycytidine results in the expression of the aquaporin 5 (AQP5) gene in the human salivary gland ductal cell line NS-SV-DC, and to the increased fluid secretion in the murine aging model C57BL/6CrSkc ( Motegi et al., 2005;Yamamura et al., 2012). Accordingly, it was proposed to treat SS patients with DNA demethylating drugs in order to restore the abnormal salivary flux that characterized SS patients. However, AQP5 is not repressed but overexpressed in SGEC from SS patients, in agreement with the DNA demethylation status observed in these cells, and the defect is at the protein level with an abnormal subcellular localization as recently described ( Lee et al., 2013)."
914,hal-00987664.training.tei.xml,24,"Observing an increase of the bullous pemphigoid antigen 1 (BP320) protein coded by the epithelial splice variant of the dystonin (DST) gene in acinar cells from SS patients, Gonzalez et al. (2011) have explored DST promoter methylation status and observed a hypermethylation status. Further explorations are mandatory in order to reveal whether this dichotomy is related to an alternative promoter usage (Elliott et al., 2012) and/or post-translational modifications."
915,hal-00987664.training.tei.xml,25,"A genome-wide analysis of DNA methylation in naïve CD4 + CD45RA + T cells was recently performed in pSS patients. Among485,000 CpG sites tested within the entire genome, 753 CpG motifs were differentially methylated with the majority (311 genes, 75%) being demethylated. Demethylated genes in pSS patients include lymphotoxin-α [previously known as tumor necrosis factor (TNF)-β] involved in T cell activation, genes implicated in the type I interferon pathway, and genes encoding for membrane water channel proteins."
916,hal-00987664.training.tei.xml,26,"Analysis of gene promoter DNA methylation status in CD4 + T cells has revealed a demethylation and overexpression of CD70 (TNSF7), and an absence of epigenetic regulation for interferon regulatory factor (IRF)5 ( Yin et al., 2010;Gestermann et al., 2012)."
917,hal-00987664.training.tei.xml,27,"Lu's group noted a DNA hypermethylation profile at the forkhead box P3 (FoxP3) promoter in pSS patients compared to healthy controls ( Sharma et al., 2009). In turn, they observed significant decreases in expression of FoxP3 mRNA and protein in peripheral CD4 + T cells isolated from pSS patients but not in healthy controls. Such an observation is in agreement with the reports describing a quantitative and qualitative defective FoxP3 Treg subset in SS ( Li et al., 2007;Alunno et al., 2013). Of note, FoxP3 Treg frequency in labial salivary glands correlates positively with the inflammation grade and risk factors for lymphoma development, such as C3 and/or C4 hypocomplementemia and cryoglobulinimia ( Christodoulou et al., 2008;Sarigul et al., 2010)."
918,hal-00987664.training.tei.xml,28,"In CD4 + T cells from SLE patients, it was demonstrated that DNA demethylation was associated with a global histone H3 and H4 hyperacetylation, and a histone H3 dimethylation at lysine 4 ( Zhou et al., 2011). In additions, such modifications were linked with the development of specific autoantibodies as demonstrated with the lupus-derived monoclonal antibodies BT-164 and KM-2 that recognize histone H3 trimethylated lysine 27, and histone H4 acetylated lysine 8,  and 1216, respectively (Dieker et al., 2007;van Bavel et al., 2011). Last but not least, HDAC inhibitors such as trichostatine A and suberoylanilide hydroxamic acid revert histone modifications and improve SLE disease without affecting autoantibody titers ( Mishra et al., 2003;Reilly et al., 2004). These results suggest (i) that the DNA demetlylation observed in SGEC from SS patients is associated with histone modification, (ii) that among anti-histones autoantibodies detected in SS some of them target histone post-translational modifications, and (iii) that targeting histone modifications may be considered as a new treatment in SS."
919,hal-00987664.training.tei.xml,29,"miRNAs are small non-coding and single-strand RNA of1922nucleotides in length, which regulate gene expression at the post-transcriptional level and are important in a wide range of physiological and pathological processes. miRNAs are generated in the nucleus as primary miRNA transcripts by RNA polymerase II, and are cleaved by an RNAse III enzyme, called Drosha. After, they are transported to the cytoplasm by exportin 5 for further processing by Dicer into mature miRNA duplexes;, they separated into single strands at the core of the multiprotein RNA-induced silencing complex (RISC) by argonaut proteins to generate miRNAs. Most miRNAs bind to the 3' untranslated region (UTR) of the targeted mRNAs which leads to either mRNA degradation or translation or repression."
920,hal-00987664.training.tei.xml,30,"Dysregulation of miRNA expression is found to be associated with the onset and progression of inflammatory autoimmune diseases including SS. Consequently, in very recent years extensive research has been done on miRNAs and their connection with autoimmune diseases (Zare-Shahabadi et al., 2013)."
921,hal-00987664.training.tei.xml,31,"miRNAs have been investigated in minor salivary glands from SS patients by different groups using different technical approaches (Alevizos et al., 2011;Kapsogeorgou et al., 2011;Tandon et al., 2012). From these studies it appears that miRNA expression is differentially expressed when SS patients were compared with controls. The predictive analysis of biologic pathways under miRNA control suggests regulation of the neurologic pathways controlling salivation (Alevizos et al., 2011), as well as the lack of transcriptional regulation of thetwo main SS's autoantigens SSA/Ro and SSB/La by the miRNa let-7b which is repressed in pSS ( Kapsogeorgou et al., 2011)."
922,hal-00987664.training.tei.xml,32,"To date, only a few studies have been conducted on miRNAs in peripheral blood mononuclear cells (PBMCs) from SS patients and from SS-prone mouse models. According to Pauley's group (Pauley et al., 2009), both miR146a and miR155 are up-regulated in response to the adaptive immune response in SS when testing humans and mice. Authors have shown, in pSS patients, that miR146a increases prior to disease onset in PBMCs, and during full-blown disease in the salivary glands thus suggesting that mir146a may be involved in early disease pathogenesis. In addition to being present in PBMC, miRNAs are also present in exosomes which are microvesicles secreted by a variety of cells including lymphocytes ( Gallo et al., 2012)."
923,hal-00987664.training.tei.xml,33,"Mir146a plays a critical role in increasing phagocytic activity and repressing inflammatory cytokine production in human monocytic THP1 cells. Mir146a is activated by NF kappa B, that controls the TLR/INF pathway through the TNF-associated factor 6 (TRAF6), the IL-1 receptor associated kinase (IRAK1), the signal transducer and activator of transcription 1 (STAT1), and IRF5 (Pauley et al., 2011). Additionally, Zilahi et al. (2012) measured the expression of miR146a and miR146b, and their target genes IRAK1, IRAK4, TRAF6 in PBMCs of patients with pSS and from healthy controls. By quantitative RT-PCR they found miR146a/b, and the gene of TRAF6, overexpressed in pSS patients, whereas the expression of IRAK1 was significantly decreased. They proposed that the TRAF6 gene contributes to the increased activation state of the NF-κB pathway by the involvement of PKCξ existing normally in the disease, and perhaps the TRAF6 gene could be a new biomarker of SS ( Zilahi et al., 2012)."
924,hal-00987664.training.tei.xml,34,"Experiments for mir155 have shown an effect on the response of toll-like receptors (TLRs) and interleukin-1 receptors (TIRs) that may affect the immune response. FoxP3 transcription factor, which is detected in a subset of T cells infiltrating SS salivary glands, has been shown to induce mir155 expression."
925,hal-00987664.training.tei.xml,35,"The etiology and many aspects of the pathogenesis of SS are still unknown. Nowadays many researchers are trying to provide an explanation based on epigenetic studies as to how epigenetic modifications may influence the course of SS and affect autoreactivity (Thabet et al., 2012). Improvements in our knowledge of epigenetics give us the opportunity to find new causes that may explain the etiology of autoimmune diseases ( Lu et al., 2010). Epigenetic studies offer us the opportunity to identify new targets with potential impact on the prevention or progression of autoimmune diseases."
926,hal-00987664.training.tei.xml,36,"This research has been co-financed by the European Union (European Social Fund-ESF) and Greek national funds through the Operational Program ""Education and Lifelong Learning"" of the National Strategic Reference Framework (NSRF)-Research Funding Program: Thales. We are also grateful to the ""association Française du Gougerot-Sjögren et des syndromes secs"" for their support, and to Genevieve Michel and Simone Forest for their help typing of the paper. Yorsa Thabet is funding by the Islamic development bank merit scholarship program."
927,hal-00987664.training.tei.xml,37,"Alevizos, I., Alexander, S., Turner, R. J., and Illei, G. G. (2011"
928,EP-0505058-B1.training.tei.xml,0,"Patentansprüche für folgende Vertragsstaaten : AT, BE, CH, DE, DK, FR, GB, IT, LI, LU, MC, NL, PT, SE"
929,EP-0505058-B1.training.tei.xml,1,Thienopyridinderivat der Formel (I):  in der: newein pharmazeutisch verträgliches Säureadditions- oder Metallsalz davon.
930,EP-0505058-B1.training.tei.xml,2,"Verbindung nach Anspruch 1, wobei in der Formel A eine Gruppe der Formel -S- ist, und B eine Gruppe der Formel -CH= ist."
931,EP-0505058-B1.training.tei.xml,3,"4-(n-Butyl)-4,5-dihydro-7-hydroxy-5-oxo-N-(4-pyridyl)thieno-[3,2-b]-pyridin-6-carbonsäureamid und seine pharmazeutisch verträglichen Säureadditions- und Metallsalze."
932,EP-0505058-B1.training.tei.xml,4,"Arzneimittel, umfassend eine Verbindung oder ein Salz nach einem der Ansprüche 1-3 im Gemisch mit einem pharmazeutisch verträglichen Träger oder Verdünnungsmittel."
933,EP-0505058-B1.training.tei.xml,5,"Immunregulierende pharmazeutische Zubereitung, umfassend eine Verbindung oder ein Salz nach einem der Ansprüche 1-3 als Immunregulator."
934,EP-0505058-B1.training.tei.xml,6,"Pharmazeutische Zubereitung gegen Osteoporose, umfassend eine Verbindung oder ein Salz nach einem der Ansprüche 1-3 als Mittel gegen Osteoporose."
935,EP-0505058-B1.training.tei.xml,7,Zur Verwendung als Immunregulator: Verbindung oder Salz nach einem der Ansprüche 1-3 .
936,EP-0505058-B1.training.tei.xml,8,Zur Verwendung als Mittel gegen Osteoporose: Verbindung oder Salz nach einem der Ansprüche 1-3 .
937,EP-0505058-B1.training.tei.xml,9,Verbindung oder Salz nach einem der Ansprüche 1 bis 3 zur Verwendung als Arzneimittel.
938,EP-0505058-B1.training.tei.xml,10,Verwendung einer Verbindung oder eines Salzes nach einem der Ansprüche 1 bis 3 zur Herstellung eines Immunregulators oder eines Mittels gegen Osteoporose.
939,EP-0505058-B1.training.tei.xml,11,Verwendung einer Verbindung oder eines Salzes nach einem der Ansprüche 1 bis 3 zur Herstellung eines Arzneimittels zur Unterdrückung des Immunsystems durch Inhibierung der Antikörperbildung.
940,EP-0505058-B1.training.tei.xml,12,"Patentansprüche für folgende Vertragsstaaten : ES, GR"
941,EP-0505058-B1.training.tei.xml,13,Verfahren zur Herstellung eines Thienopyridinderivats der Formel (I):  in der:
942,EP-0505058-B1.training.tei.xml,14,"Verfahren nach Anspruch 1, wobei die Umsetzung in Gegenwart einer Base durchgeführt  			wird."
943,EP-0505058-B1.training.tei.xml,15,"Verfahren nach Anspruch 1 oder 2, wobei die Umsetzung bei einer Temperatur im Bereich von-30 bis 200°C, bevorzugt -10 bis 100°C, durchgeführt wird."
944,EP-0505058-B1.training.tei.xml,16,"Verfahren nach einem der Ansprüche 1-3, wobei die Verbindung der Formel (I) als pharmazeutisch verträgliches Säureadditions- oder Metallsalz gewonnen wird oder anschließend darin umgewandelt wird."
945,EP-0505058-B1.training.tei.xml,17,"Verfahren nach einem der Ansprüche 1-4, wobei in den Formeln A eine Gruppe der Formel -S- ist, und B eine Gruppe der Formel -CH= ist."
946,EP-0505058-B1.training.tei.xml,18,"Verfahren nach einem der Ansprüche 1-4, wie es auf die Herstellung der Verbindung 4-(n-Butyl)-4,5-dihydro-7-hydroxy-5-oxo-N-(4-pyridyl)thieno-[3,2-b]-pyridin-6-carbonsäureamid oder seiner pharmazeutisch verträglichen Säureadditions- oder Metallsalze angewendet wird."
947,EP-0505058-B1.training.tei.xml,19,"Verfahren nach einem der Ansprüche 1-6, das den weiteren Schritt des Mischens der Verbindung der Formel (I), gegebenenfalls nach deren Umwandlung in ein pharmazeutisch verträgliches Säureadditions- oder Metallsalz, mit einem pharmazeutisch verträglichen Träger oder Verdünnungsmittel umfaßt."
948,EP-0505058-B1.training.tei.xml,20,"Claims for the following Contracting States : AT, BE, CH, DE, DK, FR, GB, IT, LI, LU, MC, NL, PT, SE"
949,EP-0505058-B1.training.tei.xml,21,A thienopyridine derivative of the formula (I):  wherein:a pharmaceutically acceptable acid addition or metal salt thereof.
950,EP-0505058-B1.training.tei.xml,22,"A Compound according to claim 1, wherein, in said formula, A is - S-, and B is -CH=."
951,EP-0505058-B1.training.tei.xml,23,"4-(n-Butyl)-4,5-dihydro-7-hydroxy-5-oxo-N-(4-pyridyl)thieno[3,2-b]pyridine-6-carboxamide, and its pharmaceutically acceptable acid addition and metal salts."
952,EP-0505058-B1.training.tei.xml,24,A pharmaceutical composition comprising a compound or salt as defined by any one of claims 1-3 in a admixture with a pharmaceutically acceptable carrier or diluent.
953,EP-0505058-B1.training.tei.xml,25,An immunoregulatory pharmaceutical preparation comprising as the immunoregulator a compound or salt according to any one of claims 1-3.
954,EP-0505058-B1.training.tei.xml,26,An antiosteoporosis pharmaceutical preparation comprising as the antiosteoporosis agent a compound or salt according to any one of claims 1-3.
955,EP-0505058-B1.training.tei.xml,27,For use as an immunoregulator: a compound or salt as claimed in any one of claims 1-3.
956,EP-0505058-B1.training.tei.xml,28,For use as an antiosteoporosis agent: a compound or salt as claimed in any one of claims 1-3.
957,EP-0505058-B1.training.tei.xml,29,A compound or salt according to any one of claims 1 to 3 for use as a pharmaceutical.
958,EP-0505058-B1.training.tei.xml,30,Use of a compound or salt according to any one of claims 1 to 3 in the manufacture of an immunoregulator or an antiosteoporosis agent.
959,EP-0505058-B1.training.tei.xml,31,Use of a compound or salt according to any one of claims 1 to 3 in the manufacture of a medicament to suppress the immune system by inhibiting antibody production.
960,EP-0505058-B1.training.tei.xml,32,"Claims for the following Contracting States : ES, GR"
961,EP-0505058-B1.training.tei.xml,33,A method for the preparation of a thiefiopyridine derivative of the formula (I):  wherein:
962,EP-0505058-B1.training.tei.xml,34,"A method according to claim 1, wherein the reaction is carried out in the presence of a base."
963,EP-0505058-B1.training.tei.xml,35,"A method according to claim 1 or 2, wherein the reaction is performed at a temperature in the range-30 to 200°C, preferably -10 to 100°C."
964,EP-0505058-B1.training.tei.xml,36,"A method according to any one of claims 1-3, wherein the compound of formula I is recovered as or is subsequently converted into a pharmaceutically acceptable acid addition or metal salt."
965,EP-0505058-B1.training.tei.xml,37,"A method according to any one of claims 1-4, wherein in said formulae A is -S-, and B is -CH=."
966,EP-0505058-B1.training.tei.xml,38,"A method according to any one of claims 1-4 as applied to the preparation of the compound 4-(n-butyl)-4,5-dihydro-7-hydroxy-5-oxo-N-(4-pyridyl)thieno[3,2-b]pyridine-6-carboxamide,or its pharmaceutically acceptable acid addition or metal salts."
967,EP-0505058-B1.training.tei.xml,39,"A method according to any one of claims 1-6, which comprises the further step of mixing the compound of formula I, optionally after conversion thereof into a pharmaceutically acceptable acid addition or metal salt, with a pharmaceutically acceptable carrier or diluent."
968,EP-0505058-B1.training.tei.xml,40,"Revendications pour les Etats contractants suivants : AT, BE, CH, DE, DK, FR, GB, IT, LI, LU, MC, NL, PT, SE"
969,EP-0505058-B1.training.tei.xml,41,"Dérivé de thiénopyridine, de formule (I) :  dans laquelle l'un des symboles A et B représente un chaínon -S- et l'autre représente un chaínon -CH=, R représente un atome d'hydrogène ou un groupe alkyle en C1-C6, et Z représente un groupe pyridyle, ou sel de métal ou d'addition d'acide d'un tel dérivé, admissible en pharmacie."
970,EP-0505058-B1.training.tei.xml,42,"Composé conforme à la revendication 1, dans lequel, dans ladite formule, A représente un chaínon -S- et B représente un chaínon -CH=."
971,EP-0505058-B1.training.tei.xml,43,"4-(n-butyl)-4,5-dihydro-7-hydroxy-5-oxo-N-(4-pyridyl)thiéno[3,2-b]pyridine-6-carboxamide, et ses sels de métal ou d'addition d'acide, admissibles en pharmacie."
972,EP-0505058-B1.training.tei.xml,44,"Composition pharmaceutique comprenant un composé ou un sel conforme à l'une des revendications 1 à 3, mélangé avec un véhicule ou un diluant admissible en pharmacie."
973,EP-0505058-B1.training.tei.xml,45,"Préparation pharmaceutique immunorégulatrice, qui contient, comme agent immunorégulateur, un composé ou un sel conforme à l'une des revendications 1 à 3."
974,EP-0505058-B1.training.tei.xml,46,"Préparation pharmaceutiques anti-ostéoporose, qui contient, comme agent anti-ostéoporose, un composé ou un sel conforme à l'une des revendications 1 à 3."
975,EP-0505058-B1.training.tei.xml,47,"Composé ou sel conforme à l'une des revendications 1 à 3, destiné à être utilisé comme agent immunorégulateur."
976,EP-0505058-B1.training.tei.xml,48,"Composé ou sel conforme à l'une des revendications 1 à 3, destiné à être utilisé comme agent anti-ostéoporose."
977,EP-0505058-B1.training.tei.xml,49,"Composé ou sel conforme à l'une des revendications 1 à 3, destiné à être utilisé comme agent pharmaceutique."
978,EP-0505058-B1.training.tei.xml,50,"Utilisation d'un composé ou d'un sel conforme à l'une des revendications 1 à 3, pour la fabrication d'un agent anti-ostéoporose ou d'un agent immunorégulateur."
979,EP-0505058-B1.training.tei.xml,51,"Utilisation d'un composé ou d'un sel conforme à l'une des revendications 1 à 3, pour la fabrication d'un médicament immunodépresseur qui agit en inhibant la production d'anticorps."
980,EP-0505058-B1.training.tei.xml,52,"Revendications pour les Etats contractants suivants : ES, GR"
981,EP-0505058-B1.training.tei.xml,53,"Procédé de préparation d'un dérivé de thiénopyridine, de formule (I) :  dans laquelle l'un des symboles A et B représente un chaínon -S- et l'autre représente un chaínon -CH=, R représente un atome d'hydrogène ou un groupe alkyle en C1-C6, et Z représente un groupe pyridyle, qui comporte le fait de faire réagir un composé de formule (II) :  avec une amine de formule (III) : H2N - Z dans lesquelles formules (II) et (III) A, B, R et Z ont les définitions indiquées ci-dessus et L représente un atome d'halogène ou un groupe alcoxy, aryloxy, alcanoyloxy ou aroyloxy."
982,EP-0505058-B1.training.tei.xml,54,"Procédé conforme à la revendication 1, dans lequel on effectue la réaction en présence d'une base."
983,EP-0505058-B1.training.tei.xml,55,"Procédé conforme à la revendication 1 ou 2, dans lequel on effectue la réaction à une température située dans l'intervalle allant de-30°C à 200°C, de préférence de -10°C à 100°C."
984,EP-0505058-B1.training.tei.xml,56,"Procédé conforme à l'une des revendications 1 à 3, dans lequel on récupère le composé de formule (I) sous la forme d'un sel de métal ou d'addition d'acide, admissible en pharmacie, ou l'on transforme ultérieurement ce composé en un tel sel."
985,EP-0505058-B1.training.tei.xml,57,"Procédé conforme à l'une des revendications 1 à 4, dans lequel, dans lesdites formules, A représente un chaínon -S- et B représente un chaínon -CH=."
986,EP-0505058-B1.training.tei.xml,58,"Procédé conforme à l'une des revendications 1 à 4, mis en oeuvre pour préparer du 4-(n-butyl)-4,5-dihydro-7-hydroxy-5-oxo-N-(4-pyridyl)thiéno[3,2-b]pyridine6-carboxamide, ou un sel de métal ou d'addition d'acide, admissible en pharmacie, de ce composé."
987,EP-0505058-B1.training.tei.xml,59,"Procédé conforme à l'une des revendications 1 à 6, qui comporte l'étape supplémentaire consistant à mélanger le composé de formule (I), après l'avoir éventuellement converti en un sel de métal ou d'addition d'acide, admissible en pharmacie, avec un véhicule ou un diluant admissible en pharmacie."
988,EP-0505058-B1.training.tei.xml,60,This invention relates to thienopyridine derivatives which are useful as immunoregulators and for the prevention and treatment of osteoporosis and pharmaceutical preparations containing them.
989,EP-0505058-B1.training.tei.xml,61,EP-A-0 059 698 discloses carboxamide compounds that apparently enhance cell-mediated immunity. These compounds are cyclic and include a benzene ring.
990,EP-0505058-B1.training.tei.xml,62,"Thienopyridine derivatives represented by Formula (A), possess the 4-hydroxythieno[2,3-b]pyridin-6-one skeleton and are described in J. Chem. Res. (S), 214 (1985) and J. Chem. Res. (S),122 (1986):   wherein Ro represents hydrogen or methyl and Y represents hydrogen or ethoxycarbonyl."
991,EP-0505058-B1.training.tei.xml,63,"Furthermore, thienopyridine derivatives represented by Formula (B), possess the 7-hydroxythieno[3,2-b]-pyridin-5-one skeleton, and are described in J. Chem. Res. (S), 6 (1980) and J. Chem. Res. (S), 84 (1984):   wherein Ro represents hydrogen or methyl and Y represents hydrogen, ethoxycarbonyl, nitrile, acetyl or the like."
992,EP-0505058-B1.training.tei.xml,64,"In compounds (A) and (B), their pharmacological activities are unknown."
993,EP-0505058-B1.training.tei.xml,65,"The present invention relates to thienopyridine derivatives [hereinafter referred to as Compound (I)] represented by formula (I):  wherein one of A and B represents -S-, and the other represents -CH=; R represents hydrogen or lower alkyl, and Z represents pyridyl; or a pharmaceutically acceptable salt thereof."
994,EP-0505058-B1.training.tei.xml,66,"In the definition of each group in formula (I), the lower alkyl means a straight or branched alkyl having1 to 6 carbon atoms, such as methyl, ethyl, propyl, isopropyl, butyl, isobutyl, sec-butyl, tert-butyl, pentyl, neopentyl, hexyl, etc."
995,EP-0505058-B1.training.tei.xml,67,"The pharmaceutically acceptable salt of Compound (I) includes acid addition salts, metal salts, etc. The acid addition salt includes, for example, an inorganic acid salt such as hydrochloride, sulfate, phosphate, etc.; an organic acid salt such as acetate, maleate, fumarate, tartarate, citrate, etc. The metal salt includes for  example, salts of alkali metal such as sodium, potassium, etc., salts of alkaline earth metal such as magnesium, calcium, etc.; aluminum salts, zinc salts and the like."
996,EP-0505058-B1.training.tei.xml,68,"Next, a process for preparing Compound (I) is described."
997,EP-0505058-B1.training.tei.xml,69,"In the process shown below, in cases where the defined group(s) change under the conditions or are inappropriate for the practice of the process, the process can be easily operated by applying thereto means conventionally used in organic synthetic chemistry, for example, protection of functional groups, removal of protective groups, etc."
998,EP-0505058-B1.training.tei.xml,70,"Compound (I) may be obtained by reacting Compound (II) represented by formula (II):  wherein L represents a leaving group; and A, B and R have the same significance as described above, with Compound (III) represented by formula (III): H2N - Z  wherein Z has the same significance as described above, preferably in the presence of a base."
999,EP-0505058-B1.training.tei.xml,71,"Herein as the leaving group denoted by L, halogen such as chlorine, bromine, iodine, etc.; alkoxy such as 	methoxy, ethoxy, etc.; aryloxy such as phenoxy, etc.; alkanoyloxy such as propionyloxy, etc.; aroyloxy such as benzoyloxy, etc. are used."
1000,EP-0505058-B1.training.tei.xml,72,"As the base, alkali metal bicarbonates such as sodium bicarbonate, potassium bicarbonate, etc.; alkali metal carbonates such as sodium carbonate, potassium carbonate, etc.; alkali metal hydrides such as sodium hydride, etc.; alkali metal alkoxides such as sodium methoxide, sodium ethoxide, etc.; alkali metal salts such as butyl lithium, etc. are used."
1001,EP-0505058-B1.training.tei.xml,73,"As the solvent used in the reaction, any solvent may be usable, as long as it is inert to the reaction. For example, ethers such as tetrahydrofuran, dioxane, etc.; amides such as dimethylformamide, dimethylacetamide, etc.; ketones such as acetone, methyl ethyl ketone, etc.; alcohols such as methanol, ethanol, isopropyl alcohol, etc.; halogenated hydrocarbons such as methylene chloride, chloroform, dichloroethane, etc.; esters such as ethyl acetate, etc.; aromatic hydrocarbons such as benzene, toluene, xylene, etc.; dimethylsulfoxide and the like may be used singly or in combination."
1002,EP-0505058-B1.training.tei.xml,74,"The reaction is carried out at-30 to 200°C, preferably -10 to 100°C and generally completed in 30 minutes to 20 hours."
1003,EP-0505058-B1.training.tei.xml,75,"The starting compound (II) can be synthesized by known methods [J. Chem. Res. (S), 6 (1980); ibid., 84 (1984); ibid., 214 (1985); J. Chem. Res. (M), 113 (1980); ibid., 771 (1984); ibid., 2501 (1985)] or by a modified method of these methods."
1004,EP-0505058-B1.training.tei.xml,76,"The desired product in the process described above can be isolated and purified by means of purification conventionally used in organic synthetic chemistry, for example, by filtration, extraction, washing, drying, concentration, recrystallization, various chromatographies, etc."
1005,EP-0505058-B1.training.tei.xml,77,"Where it is desired to obtain the salts of Compound (I), Compound (I) may be purified as it is in case that Compound (I) is obtained in the form of its salt. In case that Compound (I) is obtained in its free form, Compound (I) is dissolved or suspended in an appropriate solvent and an appropriate acid or base is added to the solution or suspension to form its salts."
1006,EP-0505058-B1.training.tei.xml,78,Compound (I) and a pharmaceutically acceptable salt thereof may also be present in the form of addition products with water or various solvents. These addition products are also included in the present invention.
1007,EP-0505058-B1.training.tei.xml,79,Furthermore Compound (I) includes all possible steric isomers and mixtures thereof.
1008,EP-0505058-B1.training.tei.xml,80,Specific examples of Compound (I) obtained by the process described above are shown in Tables 1 and 2.
1009,EP-0505058-B1.training.tei.xml,81,"Next, the immunoregulating activity, activity of inhibiting bone absorption and acute toxicity of Compound (I) are described by referring to test examples."
1010,EP-0505058-B1.training.tei.xml,82,"The methods developed by Jerne [Science, 140, 405 (1963)] and Yamamoto, et al[Drugs. Exptl. Clin. Res., 8, 5 (1982)] were modified for plaque forming cell assay."
1011,EP-0505058-B1.training.tei.xml,83,"That is, Balb/c strain male mice (age of 7 weeks, Charles River Japan Inc.) were sensitized with1 x 108 sheep red blood cells (Bio Test Research Institute) and the spleen was extirpated on the sixth or seventh day. The cells obtained from the spleen were treated with ACT solution (Tris-ammonium chloride isotonic buffer) to remove red blood cells. The cells were washed three times with RPMI1640 medium (Nissui Pharmaceutical Co.). The cells (1 x107) were incubated in RPMI-1640 medium containing 10% calf fetal serum (Gibco Co.), 50 µg/ml streptomycin, 50 IU/ml of penicillin, 2-mercaptoethanol (5 x 10-5 M), sheep red blood cells (5 x 106 cells) and a test compound dissolved in dimethyl sulfoxide supplied on a microculture plate (NUNC Co., 24 wells) in a carbon dioxide gas incubator (TABAI ESPEC CORP) at 37°C for 5 days."
1012,EP-0505058-B1.training.tei.xml,84,"After completion of the incubation, the cells were transferred to a plastic test tube and centrifuged at2000 rpm. After the supernatant was removed, the cells were resuspended in 1 ml of RPMI-1640 medium. The cell suspension was sealed in a Cunnigham chamber (Takahashi Giken Co.) together with sheep red blood cells and guinea pig complement (Cedarlane Research Institute) according to the method of Cunnigham [Immunology, 14, 599 (1968)] and incubated at 37°C for 1 to 2 hours. Direct plaque forming cell (PFC) count was counted."
1013,EP-0505058-B1.training.tei.xml,85,A rate of inhibiting antibody production by the test compound was determined by the following equation. Inhibition rate (%) = A - BA x 100 A :PFC count in the absence of test compound (dimethylsulfoxide alone) B :PFC count in the presence of test compound
1014,EP-0505058-B1.training.tei.xml,86,The results are shown in Table 3.
1015,EP-0505058-B1.training.tei.xml,87,Autoimmune diseases such as chronic articular rheumatism or the like are considered to result from tissue injury due to accentuation of B cells as the result of hypofunction of T cells. It is thus expected that Compound (I) would be effective against autoimmune disease by inhibiting antibody production.
1016,EP-0505058-B1.training.tei.xml,88,"A calvaria of a5 to 6 day-old dd mouse was aseptically cut off, washed with Dulbecco's modified phosphate buffered saline not containing calcium and magnesium (manufactured by Gibco Oriental Co.) and separated along the sutura of its center. One half of the calvaria so separated was cultured in 1.5 ml of Dulbecco's modified Eagle medium (manufactured by Gibco Oriental Co.) containing 15% of thermally inactivated (at 56°C for 2 minutes) horse serum and 2.5% of fetal calf serum. The test compound was dissolved in dimethyl sulfoxide, and 10 µl (final concentration: 1 x 10-4 M or 1 x 10-5 M) of the solution so prepared was added to the culture. Parathyroid hormone (human PTH 1-34, manufactured by Sigma Co.) was dissolved in 0.15 M sodium chloride solution (pH 3), and 3 µl (final concentration: 1 x 10-8 M) of the solution so prepared was added to the culture. The cultivation was carried out for 96 hours at 37°C in an atmosphere consisting of 95% of air and 5% of carbon dioxide. The culture medium was once replaced with a fresh one after 48 hours from the beginning of the cultivation. The concentration of dissolved calcium (i.e., absorption of bone) from the PTH-intensified bone was determined by measuring the quantity of calcium accumulated in the culture collected in 96 hours of cultivation, whereby the concentration of total calcium contained in the culture was measured with Calcium C-Test Wako (manufactured by Wako Pure Chemicals Co., Ltd.), and the inhibition rate was calculated therefrom in accordance with the equation set forth below. The results are shown in Table 4. Inhibition rate (%) = Cp - CdCp - Co x 100 Cd :Total calcium concentration in the culture treated with both test compound and PTH Cp :Total calcium concentration in the culture treated with PTH alone Co :Total calcium concentration in the culture treated with neither test compound nor PTH 			"
1017,EP-0505058-B1.training.tei.xml,89,A test compound was orally administered tothree dd-strain male mice weighing 20 ± 1 g. The minimum lethal dose (MLD) was determined by observing the mortality for 7 days after the administration.
1018,EP-0505058-B1.training.tei.xml,90,The results are shown in Table 5.
1019,EP-0505058-B1.training.tei.xml,91,"Compound (I) or a pharmaceutically acceptable salt thereof may be used as it is, or in various pharmaceutical forms. The pharmaceutical composition of the present invention can be prepared by uniformly mixing an effective amount of Compound (I) or a pharmaceutically acceptable salt thereof as the active ingredient with pharmaceutically acceptable carriers. The pharmaceutical compositions are desirably in a single dose unit suited for oral or parenteral administration."
1020,EP-0505058-B1.training.tei.xml,92,"In preparing the composition suited for oral administration, any pharmaceutically acceptable carrier may be used. Liquid preparations suited for oral administration, for example, a suspension and a syrup can be prepared using water; sugars such as sucrose, sorbitol, fructose, etc.; glycols such as polyethylene glycol, propylene glycol, etc.; oils such as sesame oil, olive oil, soybean oil, etc.; antiseptics such as p-hydroxybenzoic acid ester, etc.; flavors such as strawberry flavor, pepper mint, etc. Further a capsule, a tablet, a powder and a granule can be prepared using an excipient such as lactose, glucose, sucrose, mannitol, etc.; a disintegrator such as starch, sodium alginate, etc.; a lubricant such as magnesium stearate, talc, etc.; a binder such as polyvinyl alcohol, hydroxypropyl cellulose, gelatin, etc.; a surfactant such as a fatty acid ester, etc.; a plasticizer such as glycerine, etc. A tablet and a capsule are most useful single dose unit for oral administration because their administration is easy."
1021,EP-0505058-B1.training.tei.xml,93,"Effective dose and number of administration of Compound (I) or a pharmaceutically acceptable salt thereof may vary depending upon modes of administration, age and body weight, conditions, etc. of a patient but it is generally preferred to administer Compound (I) in a dose of1 to 1,000 mg/60 kg by dividing into one to four times."
1022,EP-0505058-B1.training.tei.xml,94,The present invention is described by referring to Examples and Reference Examples below.
1023,EP-0505058-B1.training.tei.xml,95,"A mixture of2.43 g (10.2 mmols) of ethyl 4,5-dihydro-7-hydroxy-5-oxothieno[3,2-b]pyridine-6-carboxylate [J. Chem. Res. (S), 6 (1980); J. Chem. Res. (M), 113 (1980)], 1.00 g (10.6 mmols) of 3-aminopyridine, 50 ml of xylene and 10 ml of dimethylformamide was heated at 140°C for an hour. After completion of the reaction, insoluble matters were filtered and recrystallized from dimethylformamide to give 1.56 g (yield: 54%) of Compound 1. 			 IR (KBr) cm-1:3450(br), 1638, 1594, 1547, 1480, 1408, 1364, 1264, 1228, 799, 761  NMR (CF3 CO2 D) δ (ppm):9.79(1H, s), 8.81(1H, d, J= 8.8Hz), 8.63(1H, d, J=5.1Hz), 8.15(1H, m), 8.10 (1H, d, J=5.4Hz), 7.28(1H, d, J=5.4Hz)"
1024,EP-0505058-B1.training.tei.xml,96,"A mixture of2.48 g (10.4 mmols) of ethyl 4,5-dihydro-7-hydroxy-5-oxothieno[3,2-b]pyridine-6-carboxylate [J. Chem. Res. (S), 6 (1980); J. Chem. Res. (M), 113 (1980)], 1.01 g (10.7 mmols) of 4-aminopyridine, 50 ml of xylene and 10 ml of dimethylformamide was heated at 140°C for an hour. After completion of the reaction, insoluble matters were filtered and tritylated with dimethylformamide with heating to give 1.99 g (yield: 67%) of Compound 2. 			 IR (KBr) cm-1:3440(br), 1662, 1632, 1575, 1536, 1498, 1411, 1370, 1212, 1006, 826, 751   NMR (CF3 CO2 D) δ (ppm):8.64(2H, d, J=7.0Hz), 8.46  (2H, d, J=7.0Hz), 8.11(1H, d, J=5.4Hz), 7.27(1H, d, J=5.4Hz)"
1025,EP-0505058-B1.training.tei.xml,97,"A solution of1.18 g (4.00 mmols) of the Compound a obtained in Reference Example 1, 0.39 g (4.13 mmols) of 4-aminopyridine and 20 ml of toluene was heated to reflux for 2 hours. After cooling, the reaction mixture was poured into 1 N sodium hydroxide aqueous solution, and washed twice with chloroform. 2 N Hydrochloric acid aqueous solution was added to the aqueous layer and the precipitated white crystals were filtered and dried to give 0.73 g (yield: 53%) of Compound 3.  Melting point:211.9-216.5°C  MS (EI) m/e:343 (M+) IR (KBr) cm-1:3420(br), 1661, 1617, 1591, 1546, 1509, 1393, 1196, 796, 758  NMR (DMSO-d6) δ (ppm):13.59(1H, s), 8.79(2H, d, J= 6.6Hz), 8.38(1H, d, J=5.1Hz), 8.21(2H, d, J= 6.6Hz), 7.57(1H, d, J=5.1Hz), 4.24(2H, t, J= 7.6Hz), 1.66(2H, m), 1.40(2H, m), 0.93(3H, t, J= 7.1Hz) "
1026,EP-0505058-B1.training.tei.xml,98,"Compound 4 was obtained (yield:72%) in a manner similar to Example 3 except for using 3-aminopyridine in place of 4-aminopyridine.  Melting point:179.7 -182.6°C  MS (EI) m/e:343 (M+) IR (KBr) cm-1:3388, 1627, 1540, 1390, 798, 770, 668  NMR (DMSO-d6) δ (ppm):13.03(1H, s), 9.20(,1H, d, J= 2.2Hz), 8.62(1H, d, J=4.4Hz), 8.57(1H, dd, J= 2.2Hz, 8.5Hz), 8.34(1H, d, J=5.4Hz), 7.90(,1H, dd, J=4.4Hz, 8.5Hz), 7.55(1H, d, J=5.4Hz), 4.24(2H, t, J=7.5Hz), 1.65(2H, m), 1.40(2H, m), 0.93(3H, t, J=7.3Hz) "
1027,EP-0505058-B1.training.tei.xml,99,"Compound 5 was obtained (yield:78%) in a manner similar to Example 3 except for using Compound b obtained in Reference Example 2 in place of Compound a.  Melting point:131.6 -139.4°C  MS (EI) m/e:343 (M+) IR (KBr) cm-1 :2952, 1614, 1507, 1380, 1289, 1229, 1197, 834, 663  NMR (DMSO-d6) δ(ppm):13.34(1H, s), 8.78(2H, d, J= 6.4Hz), 8.20(2H, d, J=6.4Hz), 7.48(1H, d, J= 5.6Hz), 7.39(1H, d, J=5.6Hz), 4.13(2H, t, J= 7.4Hz), 1.75(2H, m), 1.41(2H, m), 0.95(3H, t, J=7.3Hz) "
1028,EP-0505058-B1.training.tei.xml,100,"Compound 6 was obtained (yield:76%) in a manner similar to Example 3 except for using Compound b obtained in Reference Example 2 in place of Compound a and using 3-aminopyridine in place of 4-aminoyyridine.  Melting point:158.0 -158.4°C  MS (EI) m/e:343 (M+) IR (KBr) cm-1:1616, 1585, 1561, 1542, 1535, 1482, 752  NMR (DMSO-d6) δ (ppm):15.87(1H, s), 12.49(1H, s), 8.80(1H, d, J=2.1Hz), 8.37(1H, d, J=3.7Hz), 8.11 (1H, d, J=8.2Hz), 7.39-7.44(1H, m), 7.42(1H, d, J=5.5Hz), 7.35(1H, d, J=5.5Hz), 4.10(2H, t, J= 7.5Hz), 1.74(2H, m), 1.40(2H, m), 0.95(3H, t, J= 7.3Hz) "
1029,EP-0505058-B1.training.tei.xml,101,"Compound 7 was obtained (yield:58%) in a manner similar to Example 3 except for using ethyl 6,7-dihydro-4-hydroxy-6-oxothieno[2,3-b]pyridine-5-carboxylate [J. Chem. Res. (S), 214 (1985)] in place of Compound a.  Melting point:>300°C  MS (EI) m/e:287 (M+) IR (KBr) cm-1:1660, 1633, 1573, 1544, 1487, 1426, 1356, 1009, 799, 560, 465  NMR (DMSO-d6) δ (ppm):15. 58 (1H, bs), 12. 80-12. 98 (2H, m), 8.51(2H, d, J=6.4Hz), 7.64(2H, d, J=6.4Hz), 7.29(2H, s) "
1030,EP-0505058-B1.training.tei.xml,102,"Compound 8 was obtained (yield:75%) in a manner similar to Example 3 except for using ethyl 6,7-dihydro-4-hydroxy-6-oxothieno[2,3-b]pyridine-5-carboxylate [J. Chem. Res. (S), 214 (1985)] in place of Compound a, and using 3-aminopyridine in place of 4-aminopyridine.   Melting point:294.8 8 - 295.9°C  MS (EI) m/e:287 (M+) IR (KBr) cm-1:1648, 1601, 1562, 1482, 1427, 1356, 1263, 801, 554, 472  NMR (DMSO-d6) δ (ppm):15.85(1H, s), 12.97(1H, s), 12.61(1H, s), 8.80(1H, d, J=2.5Hz), 8.37(1H, dd, J=1.1Hz, 4.7Hz), 8.04-8.13(1H, m), 7.42(1H, dd, J=8.2Hz, 4.5Hz), 7.29(1H, d, J=4.5Hz), 7.29(1H, dd, J=9.9Hz, 5.4Hz) "
1031,EP-0505058-B1.training.tei.xml,103,A tablet having the following ingredients is prepared in a conventional manner.
1032,EP-0505058-B1.training.tei.xml,104,A syrup preparation having the following ingredients is prepared in a conventional manner.
1033,EP-0505058-B1.training.tei.xml,105,"A) To a solution of15.7 g (0.100 mol) of methyl 3-aminothiophene-2-carboxylate and 15.2 g (0.110 mol) of potassium carbonate in 200 ml of dimethylformamide was added 34.1 ml (0.300 mol) of n-butyl iodide at 25°C. The mixture was stirred at 120°C for 10 hours. After cooling, the solvent was evaporated under reduced pressure and 200 ml of ethyl acetate was added to the residue. An inorganic salt was removed by filtration. The filtrate was again concentrated under reduced pressure. The residue was purified acetate/n-hexane = 1/9 v/v) to give 10.2 g (yield: 48%) of methyl 3-(n-butylaminothiophene-2-carboxylate (Compound a-1).  NMR (CDCl3) δ (ppm):7.35(1H, d, J=5.3Hz), 7.01-7.30 (1H, br), 6.98(1H, d, J=5.3Hz), 3.83(3H, s), 3.28 (2H, m), 1.21-1.88(4H, m), 0.95(3H, t, J=7.5Hz)  B) 10.0 g (46.9 mmols) of Compound a-1 was dissolved in a solvent mixture of 90 ml of 1,2-dichloroethane and 9 ml of 1,4-dioxane. 16.9 ml (0.141 mol) of trichloromethyl chloroformate was dropwise added to the solution at 25°C. The mixture was stirred at 75°C for 7 hours. After cooling, 0.50 g of activated carbon was added to the reaction mixture followed by reflux for an hour in a nitrogen flow. After cooling, activated carbon was removed by filtration. The filtrate was concentrated under reduced pressure and 15 ml of ethyl acetate and 50 ml of n-hexane were added to the residue. The mixture was then stirred. The precipitated white crystals were filtered and dried to give 6.96 g (yield: 66%) of 4-(n-butyl)-5H-thieno[3,2-d]oxazine-5,7(4H)-dione (Compound a-2). NMR (CDCl3) δ (ppm):7.95(1H, d, J=5.0Hz), 6.97(1H, d, J=5.0Hz), 4.01(2H, t, J=7.2Hz), 1.17-1.98(4H, m), 0.98(3H, t, J=7.4Hz)  C) Under ice cooling, 552 mg (24.0 mmols) of sodium hydride was added to 67.4 ml (0.444 mol) of ethyl malonate. The mixture was stirred at 25°C for 30 minutes. To the solution mixture was added 5.00 g (22.2 mmols) of Compound a-2 and the mixture was stirred at 150°C for an hour. After cooling, 300 ml of water was added to the reaction mixture. The mixture was washed twice with chloroform and 6 N hydrochloric acid aqueous solution was added to the aqueous layer. The precipitated crystals were filtered and 	dried to give 3.33 g (yield: 51%) of Compound a. NMR (CDCl3) δ (ppm):7.69(1H, d, J=5.0Hz), 7.02(1H, d, J=5.0Hz), 4.18(2H, q, J=7.0Hz), 3.64(2H, t, J= 7.5Hz), 1.08-1.76(4H, m), 1.22(3H, t, J=7.0Hz), "
1034,EP-0505058-B1.training.tei.xml,106,"A) Methyl 2-butyl(n-)aminothiophene-3-carboxylate (Compound b-1) was obtained (yield:23%) in a manner similar to Reference Example 1,A) step except for using methyl 2-amino-3-thiopenecarboxylate [Chem. Ber., 98, 3571 (1965)] in place of methyl 3-aminothiophene-2-carboxylate.  NMR (CDCl3) δ (ppm):7.08-7,38(1H, br), 7.03(1H, d, J=5.5Hz), 6.14(1H, d, J=5.5Hz), 3.83(3H, s), 3.23 (2H, q, J=6.2Hz), 1.22-1.90(4H, m), 0.96(3H, t, J=7.4Hz)  B) 7-(N-butyl)-6H-thieno[2,3-d]oxazine-4,6(7H)-dione (Compound b-2) was obtained (yield: 80%) in a manner similar to Reference Example 1,B) step except for using Compound b-1 in place of Compound a-1.  NMR (CDCl3) δ(ppm):7.59(1H, d, J=5.2Hz), 6.30(1H, d, J=5.2Hz), 3.97(2H, t, J=7.0Hz), 1.15-1.93(4H, m), 0.96(3H, t, J=7.4Hz)  C) Compound b was obtained (yield: 92%) in a manner  similar to Reference Example 1,C) step except for using Compound b-2 in place of Compound a-2.  NMR (DMSO-d6) δ (ppm):7.34(1H, d, J=5.7Hz), 7.29(1H, d, J=5.7Hz), 4.32(2H, q, J=7.0Hz), 3.97(2H, t, J=7.3Hz), 1.60-1.71L2H, m), 1.30(3H, t, J=7.1Hz), 1.26-1.40(2H, m), 0.92(3H, t, J=7.3Hz) "
1035,hal-00720564.training.tei.xml,0,Ten windsurfers (nine males and one female) volunteered to participate in the study and provided written informed consent. The subjects were reminded that withdrawal from the study was possible at any time. For the physical characteristics of the subjects see Table 1.
1036,hal-00720564.training.tei.xml,1,"All subjects were members of the Tunisian and Turkish Windsurfing National teams. They had regularly participated insix to eight international events per year for at least the previous 2 years (World and European Championships, and European Olympic sailing weeks). Their averaged weekly training programs included two running, two muscle strengthening, and four to six windsurfing sessions. The study was approved by the University ethics committee."
1037,hal-00720564.training.tei.xml,2,"Light wind (LW) and medium wind (MW) were defined as wind speeds ranging from5 to 9 knots respectively. The races with too much wind variation switching from LW to MW were excluded from the analysis. Wind speed was measured with an anemometer (Plastimo, France) at the beginning of the race and every 10 min throughout the race. These values were then averaged to obtain the mean race wind speed. Data from 22 races (10 in LW and 12 in MW) were analysed. The averaged wind speeds for the LW and MW races were 6. 30 (Gue´velGue´vel et al. 1999; ISAF) occurred over 42 days during a national competition period (June and July). A typical Olympic course consists of a succession of legs, such as upwind (beating movement with the sail at ~45° to the wind direction), reaching (crosswind movement with the sail angle between ~45° and 90°) and running (downwind movement in the direction of the wind with the sail at ~to the wind). These last two legs are also called ''downwind''. Each race consisted of two laps. Standard Olympic boards were used (Mistral One Design, 2 sail) and the subjects were allowed to perform their own warm-up (Gue´velGue´vel et al. 1999). No changes in the usual racing procedure were required for the purposes of the experiment. As not all studied subjects participated in all the races, the analysis was of data from 61 individual races in LW and 82 in MW. At the end of each race the athletes were awarded classification points according to the current international rules (ISAF), i.e. one point for first place, two points for second, and so on. Each subject wore their HR monitor on land before their warm-up period. For each individual, the exact time of the start of the HR recording was noted. Each race was accurately timed with its strategic events, i.e. fleet start and individual finish. Then for each subject, the racing HR was isolated from the preand post-race HRs. The mean racing durations were "
1038,hal-00720564.training.tei.xml,3,"On-sea HR measurements HR was monitored with a HR monitor (Polar Vantage, Polar Electro, Kempele, Finland). The monitor consists of an electrode belt worn around the chest, a transmitter, and a wrist-mounted receiver. The sampling rate was set at0.2 Hz."
1039,hal-00720564.training.tei.xml,4,"Blood sampling and determination of blood lactate concentration Blood samples were collected3 min after the end of one LW and two MW races (for some subjects). A total of 4 LW and 10 MW samples were analysed. The 20-ll samples of capillary blood were withdrawn from an earlobe with Microzym micropipettes. They were stored in tubes containing 180 ll of a haemolytic solution to ensure good preservation of the samples at room temperature. Blood lactate concentration was subsequently measured using an enzymatic method (Microzym L, Setric Ge´nieGe´nie Industriel, Toulouse, France)."
1040,hal-00720564.training.tei.xml,5,"During the competitive period, each subject came to the laboratory to perform a maximal graded exercise test. The tests were performed in the afternoon from2 pm to 5 pm at a laboratory temperature of 20°C to 22°C. The subjects were asked to abstain from: (1) exercise the day prior to the test and (2) drinking caffeine beverages on the day of the test."
1041,hal-00720564.training.tei.xml,6,"A maximal graded exercise test on a treadmill allowed us to measure the subjects' maximum oxygen consumption (VO 2max ). Prior to the test subjects lay quietly on a bed for15 min for the measurement of their resting HR (HR rest ). Then the test was performed on a 0 graded treadmill (Woodway, Ergo XELG 90, Weil, Germany). It began with a 3-min warm-up period at 9 kmAEh-1 , immediately followed by 1 kmAEh-1 increment every minute till exhaustion. Oxygen consumption (VO 2 ) was considered maximal (VO 2max ) if the following criteria were achieved: (1) a levelling off of VO 2 despite a treadmill speed increase; (2) a respiratory gas exchange ratio higher than 1.1, and (3) attainment of age theoretical HR max ±5 bpm. All the subjects achieved physiological criteria for VO 2max. The peak HR attained at exhaustion during maximal graded exercise was considered to be HR max ( Chamari et al. 1995). Exhaustion occurred within 10-15 min of exercise for all subjects. The measurement of the cardiorespiratory variables was performed with a breath-by-breath cardiorespiratory device (ZAN 680, Oberthulba, Germany). This system allowed the continuous measurement of HR, pulmonary gas exchange, and ventilation during the test. Prior to each test the gas analysers were calibrated with gases of known concentrations. The ventilatory membrane was calibrated with a 1-l syringe. HR was determined from a six-lead ECG with 12 derivations. On the test reports, HR and the respiratory data were provided once every 30 s with the values averaged on the ten last respiratory cycles on a sliding technique basis ( Whipp et al. 1982)."
1042,hal-00720564.training.tei.xml,7,"Thetwo ventilatory thresholds, Th1 vent and Th2 vent , i.e. anaerobic threshold and the respiratory compensation threshold, respectively, were established according to Beaver et al. (1986)."
1043,hal-00720564.training.tei.xml,8,The windsurfing HR values are expressed as a percentage of:
1044,hal-00720564.training.tei.xml,9,"(1) HR max (%HR max ) and (2) HR reserve , i.e. HR max )HR rest (%HR reserve ). For each subject, the race that resulted in the highest HR (%HR reserve ) was called RacingPeak%HR reserve ."
1045,hal-00720564.training.tei.xml,10,"For the maximal graded exercise, the following values were selected: v Peak (treadmill peak velocity attained at exhaustion in the VO 2max trial), the first and second ventilatory thresholds expressed as a percentage of VO 2max (Th1 vent % VO 2max and Th2 vent % VO 2max , respectively). As Paavolaı¨nen Paavolaı¨nen et al. (1999) have shown that peak treadmill velocity is more correlated to endurance performance than VO 2max , VO 2max,demand was calculated as proposed by the American College of Sports Medicine (1986): VO 2max,demand (mlAEkg )1AE min )1 )=12AEv (mAEs )1 )+3.5 (with0 graded treadmill), where v is peak treadmill velocity."
1046,hal-00720564.training.tei.xml,11,"Values are expressed as mean (standard deviation). A Pearson correlation coefficient and linear regression analysis were performed between the mean HRs measured during the races and: (1) performance (number of points obtained), the athlete's body mass, and race duration, and (2) the physiological variables measured at the laboratory. A linear regression analysis was realized between the racing HR and performance. With respect to the relationship between racing HR and the physiological variables, a forward stepwise multiple regression analysis was realized in order to establish which of the measured variables better explained the racing HR. The independent variables used in this analysis were the parameters of the VO 2max test, i.e.VO 2max , v Peak , Th1 vent % VO 2max , and Th2 vent % VO 2max. Statistical significance was fixed at the p<0.05 level."
1047,hal-00720564.training.tei.xml,12,For the results of the maximal graded treadmill exercise see Table 2.
1048,hal-00720564.training.tei.xml,13,The linear regression analysis showed that there was a significant correlation between performance and racing HR in both LW and MW.
1049,hal-00720564.training.tei.xml,14,"In LW, the relationship between racing classification and the HR values expressed as %HR reserve were related by the formula: Racing Classification Points (LW)=)0.12 (%HR reserve )+13.03; r=)0.71, r 2 =0.50, p<0.001 (Fig. 1). Racing HR expressed as %HR max was also significantly correlated to the racing classification points (r=)0.70, p<0.001)."
1050,hal-00720564.training.tei.xml,15,"In MW, the relationship between racing classification and the HR values expressed as %HR reserve were related by the formula: Racing Classification Points (MW)=)0.11 (%HR reserve )+10.99; r=)0.66, r 2 =0.43, p<0.001 (Fig. 2). Racing HR expressed as % HR max was also significantly correlated to the racing classification points (r=)0.66, p<0.001)."
1051,hal-00720564.training.tei.xml,16,"These correlations were not influenced by the race duration as there was no correlation between %HR reserve and race duration, r=0.03, p=0.70 (n=143)."
1052,hal-00720564.training.tei.xml,17,"For wind speeds over12.5 knots (6.43 mAEs )1 ), the correlation was significant between body mass and racing points classification (r=)0.451, r 2 =0.203, p<0.001, n=54). There was no correlation between body mass and performance in LW or MW between 10 and 12.5 knots."
1053,hal-00720564.training.tei.xml,18,"The Pearson correlation matrix showed that there were some significant relationships between sailing HR measurements and laboratory variables. When the highest individual racing HR (%HR reserve ) was plotted against the main aerobic power variables, the stepwise regression analysis showed that51% of the racing HR was explained by the second ventilatory threshold:"
1054,hal-00720564.training.tei.xml,19,"r=-0.71, r 2 =0.51, p<0.05."
1055,hal-00720564.training.tei.xml,20,The present study showed that there was a positive correlation between the Olympic windsurfing performance and racing HR in both LW and MW conditions. Furthermore the relationship between windsurfing racing HR and laboratory variables showed that the higher individual HR (expressed as a percentage of HR reserve ) was negatively correlated to the second ventilatory threshold expressed as a percentage of VO 2max .
1056,hal-00720564.training.tei.xml,21,"Despite the possible effect of some non-physiological variables on HR and the fact that HR measurement is an indirect estimation of the cardiorespiratory responses, it is widely admitted that this variable is a good indicator of continuous exercise intensity (ACSM 1978;Le´gerLe´ger 1994). It has been shown that the HR expressed as a percentage of HR max is more representative of exercise intensity than the absolute HR value ( Gilman and Wells 1993;Londeree et al. 1995). Furthermore, when expressed as a percentage of HR reserve , i.e. HR max-HR rest , the HR was closely related to VO 2 in continuous exercise (Gilman 1996). It has recently been shown that even if the exercise pattern is intermittent, in long aerobic exercise such as tennis, HR time course is similar to that of VO 2 ( Dansou et al. 2001). The present results show that the subjects sailed with HRs representing80.5 and 76.2%HR max in LW and MW, respectively. This is slightly lower than values reported by Gue´vel Gue´vel et al. (1999), namely 87.4 and 82.9%HRmax, respectively. This may not be due to race duration, as HR was not correlated to race duration in either study. De Vito et al. (1997) have reported even higher values representing about 92% HR max , but these values were measured over short periods of time in a race simulation. The slightly lower %HR max values observed in the present study may not be explained by the presence of a female windsurfer in the group, as De Vito et al. (1997) and Vogiatzis et al. (2002) have shown similar %HR max in male and female athletes when pumping. A possible cause of the lower relative HRs could be due to the pumping pattern of the studied subjects. Gue´vel Gue´vel et al. (2000) have shown that two periods of 6-minutes simulated pumping with two frequencies (1.5 vs. 1 Hz) with the same energy expenditure, resulted in a cardiac response representing 77.1 and 80.2%HRmax respectively. The time to exhaustion (in a separate test) was threefold longer for the short and frequent pumping with respect to the high amplitude one. It is then possible that the present study subjects used more the short pumping than the subjects of Gue´vel Gue´vel et al. (1999). It has been reported that if high magnitude pumping is more strenuous, it is more effective at increasing board speed (Gue´velGue´vel et al. 2000). Lastly, an experimental difference could have caused the observed discrepancies in racing intensity, as Gue´vel Gue´vel et al. (1999) only analysed the first race of each competition day. The present study analysed all the races sailed for a given day, which corresponded to two to three races a day with 15-20 min of recovery in between. Fig. 1 Linear regression between racing heart rate (%HR reserve ) and racing classification points in light wind (5-9 knots, n=61) Fig. 2 Linear regression between racing heart rate (%HR reserve ) and racing classification points in medium wind (10-16 knots, n=82)"
1057,hal-00720564.training.tei.xml,22,The cumulative effect of fatigue could have lowered sailing intensity for the second and third races of the day with respect to the first.
1058,hal-00720564.training.tei.xml,23,"Despite its complex metabolism, lactate is thought to be a relevant indicator of anaerobic glycolysis during exercise (Brooks 1991). The post-racing lactate measured in the present study was similar to values reported by Chardigny et al. (1990), Gue´velGue´vel et al. (1999 and De Vito et al. (1997) and confirms that Olympic windsurfing racing involves anaerobic metabolism. De Vito et al. (1997) have suggested that anaerobic conditions are related mostly to pumping. The present study's values may have been slightly underestimated, as the samples were taken from an earlobe whereas Gue´vel Gue´vel et al. (1999) and De Vito et al. (1997) took blood from the fingertip. Indeed, Dassonville et al. (1998) have shown that fingertip post-exercise lactate values are significantly higher than earlobe values if the arms are involved in the exercise. Lastly, Gue´vel Gue´vel et al. (1999) have clearly shown that the HR is significantly higher in the first half of the race than the second, probably for strategic purposes. Indeed, it seems that the athletes fight hard in the first half of the race to take a position as close as possible to the lead, and then the second part of the race serves to consolidate the windsurfer's position by a somewhat defensive attitude (Gue´velGue´vel et al. 1999). This could have led to a further underestimate of the involvement of anaerobic metabolism. Indeed, no study has stopped a race at the end of the first half to measure blood lactate."
1059,hal-00720564.training.tei.xml,24,"The exercise HR increases could be due to: (1) the exercise intensity per se and/or (2) to the type of muscle contractions. Indeed, De Vito et al. (1997) have noted that windsurfing is a mix of dynamic (e.g. arms, thighs) and static (e.g. trunk and forearms) contractions. Furthermore, the effect of the raised upper arms on the HR increase has to be considered (De Vito et al. 1997). As the races lasted for more than20 min it is possible that cardiac drift also influenced the averaged HR values (Gilman 1996)."
1060,hal-00720564.training.tei.xml,25,"With respect to the relationship between sailing HRs and laboratory variables, there was a negative correlation between individual maximal sailing HR expressed as a percentage of HR reserve and the second ventilatory threshold expressed as a percentage of VO 2max . Billat and Koralsztein (1996) have reported that the second ventilatory threshold is a good indicator of aerobic power. One explanation for the observed negative correlation could be that subjects with lower thresholds have to make a greater relative effort than the fitter ones to generate similar work, resulting in a higher HR. In that case, it is possible that the fittest athletes were often sailing under or just at the level of their respective anaerobic threshold leading to a relatively moderate HR (De Vito et al. 1997). It is possible that the athletes with good aerobic power need only to make a relatively low effort during races against athletes with poorer fitness levels. The present results confirm that Olympic windsurfing is an endurance activity in which physiological variables have an important bearing on performance (De Vito et al. 1997). Indeed it has been reported that, depending on the part of the race, Olympic windsurfing can be compared to a2000-m rowing event (i.e. 90-95% HR max for about 6.5 min) for the downwind leg and to what has been defined as the ''efficient active recovery stage'' of cycling and swimming (75-80% HR max ) for the upwind leg (Gue´velGue´vel et al. 2000)."
1061,hal-00720564.training.tei.xml,26,"To summarize, the present study showed that Olympic windsurfing performance depends upon the level of active involvement of each athlete during a race, indeed light and medium racing performances were correlated to the sailing HR, i.e. exercise intensity. These results reinforce the findings of former studies showing that from the moment ''pumping'' was allowed in international windsurfing events, racing intensity has increased. From5 to 16 knots, a windsurfer's performance is greatly dependent on their ability to maintain continuous pumping, resulting in high heart rates. Windsurfing athletes and coaches should consider these findings when planning their physical training programs."
1062,hal-00398330.training.tei.xml,0,"Context. The discovery of a short-period giant planet (a hot Jupiter) around the primary component of the triple star system HD 188753 has often been considered as an important observational evidence and as a serious challenge to planet-formation theories. Aims. Following this discovery, we monitored HD 188753 duringone year to better characterize the planetary orbit and the feasibility of planet searches in close binaries and multiple star systems. Methods. We obtained Doppler measurements of HD 188753 with the ELODIE spectrograph at the Observatoire de Haute-Provence. We then extracted radial velocities for the two brightest components of the system using our multi-order, two-dimensional correlation algorithm, TODCOR. Results. Our observations and analysis do not confirm the existence of the short-period giant planet previously reported around HD 188753 A. Monte Carlo simulations show that we had both the precision and the temporal sampling required to detect a planetary signal like the one quoted. Conclusions. From our failure to detect the presumed planet around HD 188753 A and from the available data on HD 188753, we conclude that there is currently no convincing evidence of a close-in giant planet around HD 188753 A."
1063,hal-00398330.training.tei.xml,1,"In a recent paper, Konacki (2005a) has reported the discovery of a3.35-day radial-velocity modulation of HD 188753 A with a semiamplitude of 149 m s −1 , and attributed this signal to the presence of a planet with a minimum mass of 1.14 M Jup in orbit around the star. Interestingly, HD 188753 A is the primary component of a triple star system, while the visual companion, HD 188753 B, is itself a spectroscopic binary with a period of 155 days (Griffin 1977;Konacki 2005a). HD 188753 is therefore a hierarchical system, with the primary component (A) hosting the giant planet and the secondary (Ba) and tertiary (Bb) components forming a close pair in orbit at a distance of 12.3 AU from the planet-host star."
1064,hal-00398330.training.tei.xml,2,"The discovery of a giant planet around HD 188753 A has attracted much attention, as the proximity of HD 188753 B poses a serious problem for planet formation theories. Indeed, the periastron distance of the visual binary is only6.2 AU, so that any protoplanetary disk around HD 188753 A would be truncated at ∼2 AU (Konacki 2005a; Jang-Condell 2007), i.e. probably inside the snow line. The favored core accretion model, which stipulates that the cores of giant planets form beyond the snow line, might thus have a problem accounting for the existence of a giant planet around HD 188753 A. Assuming that the system has maintained its current orbital configuration ever since the planet formed, explaining the existence of a hot Jupiter around the primary component of HD 188753 remains a challenge (Nelson 2000;Mayer et al. 2005;Boss 2006; Jang-Condell 2007) and might indicate that the core accretion mechanism is not able to account for the existence of all the planets discovered so far."
1065,hal-00398330.training.tei.xml,3,"An alternative way to explain the existence of a close-in Jovian planet around HD 188753 A and also to circumvent the theoretical problem is to assume that the current orbital configuration is the result of a dynamical process (Pfahl 2005;Portegies Zwart & McMillan 2005). According to this approach, HD 188753 A was a single star or had a more distant stellar companion at the planet formation phase, and this initial state was later transformed through a dynamical encounter into the triple system we presently observe. Pfahl & Muterspaugh (2006) estimate that dynamical interactions could deposit giant planets in about0.1% of the binaries with a semimajor axis a < 50 AU. The existence of a short-period Jovian planet around HD 188753 A might thus be the result of a dynamical process, rather than of an in situ formation (Pfahl 2005;Portegies Zwart & McMillan 2005;Jang-Condell 2007)."
1066,hal-00398330.training.tei.xml,4,"Among the ∼200 extrasolar planets discovered so far by Doppler spectroscopy, 33 are known to orbit the components of binaries or triple stars ( Eggenberger et al. 2004;Mugrauer et al. 2005;Raghavan et al. 2006). However, unlike the giant planet around HD 188753 A, most of these planets reside in systems with separations larger than 100 AU. The occurrence of planets in binaries with semimajor axes below 100 AU is therefore still largely unprobed. Despite this strong bias and prior to the discovery by Konacki, three planets were found in stellar systems with separations close to 20 AU: Gliese 86 ( Queloz et al. 2000;Els et al. 2001;Mugrauer & Neuhäuser 2005;Lagrange et al. 2006), γ Cephei ( Hatzes et al. 2003;Neuhäuser et al. 2007;Torres 2007), and HD 41004 ( Zucker et al. 2003Zucker et al. , 2004). Two features render the planet around HD 188753 A particularly interesting. First, both the planetary and the binary orbital parameters are known (only γ Cephei shares this property). Second, the binary periastron distance may be small enough to preclude giant planet formation according to the canonical models."
1067,hal-00398330.training.tei.xml,5,"Doppler searches for planets in close binaries have recently proven feasible using dedicated reduction techniques based ontwo-dimensional correlation ( Zucker et al. 2003;Konacki 2005b). In order to probe the occurrence of planets in close stellar systems, two surveys searching for planets in spectroscopic binaries are currently underway ( Eggenberger et al. 2003;Konacki 2005b), the planet around HD 188753 A being a product of Konacki's survey. Yet, deriving the velocity of HD 188753 A to the precision needed to reveal the presence of a planet is particularly challenging, as the radial velocity of the secondary, HD 188753 Ba, varies with a semiamplitude of ∼13 km s −1 over a timescale of 155 days."
1068,hal-00398330.training.tei.xml,6,"In order to study this intriguing system further, we monitored HD 188753 duringone year with the ELODIE spectrograph ( Baranne et al. 1996). We then used our multi-order TODCOR algorithm ( Zucker et al. 2003) to derive the radial velocities of the two brightest components, namely HD 188753 A and HD 188753 Ba. Unfortunately, our data and analysis do not confirm the existence of the planet reported by Konacki (2005a) around HD 188753 A. We present our observations and data reduction technique in Sect. 2. Our results and the lack of evidence of a hot Jupiter around HD 188753 A are described in Sect. 3 and discussed in Sect. 4."
1069,hal-00398330.training.tei.xml,7,"We observed HD 188753 with the ELODIE echelle spectrograph ( Baranne et al. 1996) at the Observatoire de Haute-Provence (France) betweenJuly 2005 and August 2006. Altogether, we gathered 48 spectra with a typical signal-to-noise ratio of 55 (per pixel at 550 nm). In the course of the observations, we realized that the precision seemed degraded when the two brightest stars (A and Ba) had similar velocities (velocity difference of less than a few km s −1 ), and we subsequently avoided observing the system at those particular phases of the 155-day modulation."
1070,hal-00398330.training.tei.xml,8,"Our spectra were reduced and cross-correlated online, the wavelength calibration being provided by the thorium-argon simultaneous reference technique ( Baranne et al. 1996). The double-lined nature of HD 188753 was evident at the telescope, wheretwo blended correlation features could be seen, one corresponding to component A, the other corresponding to component Ba. The contribution of the third component (Bb) to the total flux is quite modest, so that the system can basically be considered as a double-lined spectroscopic binary. Nonetheless, extracting precise radial velocities for the two brightest stars of the system is challenging, for two main reasons: (i) the difference in mean velocity between components A and Ba is currently only of 2 km s −1 ; (ii) the radial velocity of component Ba varies with a period of 155 days and a semiamplitude of ∼13 km s −1 , which is not very different from the sum of the intrinsic widths of the two correlation features. This leads to a situation where the two correlation features are always strongly blended, the blend changing continuously due to the 155-day modulation."
1071,hal-00398330.training.tei.xml,9,"Given the double-lined nature of HD 188753 and the strong line blending, we derived radial velocities using a multi-order version ( Zucker et al. 2003) of thetwo-dimensional correlation algorithm TODCOR (Zucker & Mazeh 1994). This algorithm uses two templates with a given flux ratio and unknown Doppler shifts to compute the two-dimensional correlation function, whose maximum simultaneously gives the radial velocity of both components. Our version of TODCOR uses as templates high signal-to-noise stellar spectra built up using spectra from our planet search programs with CORALIE and ELODIE (Queloz et al. 2000;Perrier et al. 2003), along with spectra from the surveys for low-mass companions to M dwarfs by Delfosse and coworkers ( Delfosse et al. 1998Delfosse et al. , 1999). Each template can furthermore be convolved with a rotational broadening profile, thus allowing for a better fit to the observed spectrum. In the multi-order version of TODCOR, the flux ratio is a function of wavelength and is calculated for each order according to the spectral types of the two templates using the library of spectral energy distributions by Pickles (1998). To insure fine-tuning with the observed system, the table of flux ratios can also be multiplied by a global normalization factor."
1072,hal-00398330.training.tei.xml,10,"To derive the radial velocities of HD 188753 A and Ba, we ran TODCOR for a variety of different pairs of templates from our library, searching for the pair that best matches our observed composite spectra. Thetwo templates finally retained were the spectrum of HD 224752 (a G6 dwarf) for HD 188753 A and the spectrum of HD 225208 (a K0 dwarf) for HD 188753 Ba. This template configuration was chosen because (i) it gave the lowest residuals for the spectroscopic orbit of component Ba, and (ii) the primary template maximized the correlation coefficient for component A. Additional fine-tuning consisted of broadening the primary and secondary templates by υ sin i of 1 km s −1 and 3 km s −1 , respectively, on top of their initial broadening of 3.6 km s −1 and 1.4 km s −1. With this TODCOR setup, a third correlation feature corresponding to component Bb was visible at some orbital phases. Although not feasible with TODCOR, extracting the exact radial velocity of component Bb from our spectra is possible and will be the subject of a forthcoming paper (Mazeh et al. in prep.)."
1073,hal-00398330.training.tei.xml,11,"Our radial velocities for HD 188753 A and HD 188753 Ba are displayed in Fig. 1 and are listed in Table 1 available in the electronic version. Internal uncertainties for each measurement were first estimated using aone-dimensional error analysis as explained in Zucker & Mazeh (1994). Internal uncertainties computed in this way seem correct from a relative point of view, but are too pessimistic in absolute terms when compared to external uncertainties measured by the root-mean-squares (rms) around the orbits. This is true not only for HD 188753 A and Ba, but also for the few other systems we have analyzed so far with TODCOR. We consequently used the one-dimensional uncertainties as relative weights, but then rescaled them globally, forcing the match with the rms derived from the external error analysis. The internal uncertainties finally used (and given in electronic form) are those rescaled values."
1074,hal-00398330.training.tei.xml,12,"The error analysis shows that the precision on our radial velocities is degraded (factor of about 2) when the velocity difference between components A and Ba is less than3 km s −1. Note that, although not reported, Konacki probably faced the same problem, since in his measurements, taken in 7 runs spread over 466 days, the difference in radial velocity between the two brightest components is always greater than 7 km s −1 , which is not likely to be a coincidence as this happens only about half of the time. In consequence, we derived orbital solutions using only our 41 measurements with a velocity difference larger than 3 km s −1 ."
1075,hal-00398330.training.tei.xml,13,"As shown by Konacki (2005a), the spectroscopic binary with a period of155 days detected by Griffin (1977) is the B component. Our single-lined spectroscopic orbit for HD 188753 B (including a linear drift to take the long-period AB motion into account) is displayed in Fig. 1. The corresponding orbital elements are listed in Table 2. All these elements are relatively well constrained."
1076,hal-00398330.training.tei.xml,14,"For the A component, the dominant motion seen in our data is a steady decrease in velocity. To check the consistency of this velocity decrease with the25.7-year orbital motion of the AB pair, we fitted our radial velocities with a Keplerian curve computed using the elements from the visual orbit (Söderhjelm 1999) as fixed parameters. The only two free parameters in the fit were thus the systemic velocity and the velocity semiamplitude, K A. The result is displayed in Fig. 1 and shows that the observed velocity decrease is indeed consistent with the orbital motion of the AB pair. The fit in Fig. 1 yields a velocity semiamplitude K A = 5.54 ± 0.25 km s −1. Combining this value with the sum K A + K B = 9.23 ± 0.86 km s −1 derived from the elements of the visual orbit 1 , we obtain a mass ratio q AB = M B /M A = 1.50 ± 0.37. This value is fully consistent with the masses reported by Konacki (2005a), which yield a mass ratio q AB = 1.54 ± 0.12."
1077,hal-00398330.training.tei.xml,15,"In order to further study the radial-velocity variation induced by the wide orbit, we show in Fig. 2 the velocity of HD 188753 A along with the velocity of HD 188753 Ba after having removed 1 Söderhjelm (1999) does not give uncertainties on the orbital elements. Our calculation assumes uncertainties of0.1 years on the period, of 0.02 on the angular semimajor axis, and of 0.03 on the eccentricity. the 155-day modulation. This is presented in two panels. The left-hand panel depicts the two sets of velocities as a function of time, where the opposite slopes of the two stars can be seen easily. The right-hand panel shows a Wilson-Mazeh plot (Wilson 1941;Mazeh et al. 2002) in which the velocities of HD 188753 Ba and HD 188753 A are plotted against each other (with the average velocity removed from each data set). As explained in Wilson (1941), plotting the pairs of velocities corresponding to each observation in this way should result in a straight line, whose negative slope is the inverse of the mass ratio. Figure 2 shows that our data points are indeed consistent with a straight line, except for two somewhat discordant points. Fitting the plot in Fig. 2 with a linear relation through the origin, taking individual uncertainties on both axes into account and ignoring the two discordant measurements, we obtain a mass ratio q AB = 1.54 ± 0.19. This result is in very good agreement with the two values reported previously, emphasizing the consistency between our radial velocities, the visual orbit determined by Söderhjelm (1999), and the masses derived by Konacki (2005a)."
1078,hal-00398330.training.tei.xml,16,"To search for evidence of the planet found by Konacki (2005a), we computed the Lomb-Scargle periodogram of the residuals around the long-period AB orbit for component A. This is shown in Fig. 3. Contrary to our expectations, we did not find any significant signal in this periodogram, in particular at a frequency of0.30 days −1 , corresponding to the 3.35-day planet reported by Konacki (2005a). That is, our velocities for HD 188753 A show no sign of a short-period signal in addition to the velocity decrease related to the AB orbital motion. The rms of 60 m s −1 is basically noise and can be interpreted as the precision we achieve on the radial velocity of this star."
1079,hal-00398330.training.tei.xml,17,"Could it be that we missed the planet discovered by Konacki (2005a), either because of inadequate temporal sampling or because of insufficient precision? To check the adequacy of our precision and temporal sampling to detect the potential hot Jupiter around HD 188753 A, we constructed1000 artificial velocity sets by adding a residual value (drawn at random from our residual data set for component A) to a planetary signal computed from the orbital parameters quoted by Konacki (2005a), and sampled at our own observing epochs. Note that in the planetary signal we included the linear and quadratic trends reported by Konacki (2005a) and corresponding to the 25.7-year orbital motion. We then analyzed each of these artificial velocity sets by subtracting a linear drift representing the long-period AB orbital motion and by computing the Lomb-Scargle periodogram of the residuals. All the mock residual sets displayed an rms ≥105 m s −1 , that is, about twice larger than the value actually observed. In the Lomb-Scargle periodogram, all the artificial sets displayed a marked peak at the expected frequency, with a false alarm probability ≤2%. As we found the inserted periodicity in all our 1000 simulations, there is less than a 0.1% probability than the planet discovered by Konacki (2005a) could hide in our data set. These results confirm that we indeed have an adequate temporal sampling and sufficient precision for detecting a planetary signal similar to the one reported by Konacki (2005a). From our failure to detect such a signal, we conclude that our data show no evidence of a short-period massive planet orbiting HD 188753 A."
1080,hal-00398330.training.tei.xml,18,"Our observations of HD 188753 confirm that this system is triple and that HD 188753 B is itself a spectroscopic binary with a period of155 days. Our orbital parameters for HD 188753 Ba are similar to those reported by Konacki (2005a), and both analyses indicate that the B component is more massive than component A. However, we failed to detect the presumed short-period planetary companion to HD 188753 A, despite our clear ability to do so. Instead, our data and analysis show only a steady drift in radial velocity, consistent with the 25.7-year orbital motion of the visual pair."
1081,hal-00398330.training.tei.xml,19,"Although we disagree with Konacki (2005a) regarding the existence of a hot Jupiter orbiting HD 188753 A, Konacki's velocities are consistent with our analysis and conclusions, provided we admit that the precision on these velocities is not15 m s −1 , but rather about 140 m s −1. Moreover, it should be noted that Konacki's velocities do not support the planetary hypothesis any more than do our own data. Indeed, when plotting a Lomb-Scargle periodogram of the residuals of his measurements around the quadratic drift he quotes, the peak at 3.35 days has a false alarm probability of 24%, and so is not significant. Even if the planetary solution reported by Konacki (2005a) satisfactorily fits his 11 velocity points (9 epochs), we consider that this model is very likely an artificial high-frequency fit to a small data set with large residuals. We thus conclude that there is currently no convincing evidence of a close-in giant planet around HD 188753 A."
1082,hal-00398330.training.tei.xml,20,"The example of HD 188753 shows that extracting precise radial velocities for the primary component of a spectroscopic binary can be challenging and that estimating the precision we really achieve on these velocities is even harder. Using our multi-order TODCOR algorithm that is specially designed to detect faint secondaries, we typically achieve a precision of10-30 m s −1 on the radial velocity of the primary component of a double or triple system with a magnitude difference 3 < ∆V < 5.5; see Zucker et al. (2004) or Eggenberger et al. (2006 for examples. In this respect, the precision of 60 m s −1 obtained here for HD 188753 A seems abnormally poor. One possible reason may be the lower magnitude difference (∆V = 0.75) coupled to the large-amplitude 155-day modulation induced by component Ba. Another possibility may be the presence of the spectrum of component Bb in our data, which we ignored in the present analysis. To take this additional contribution into account and to properly analyze triple-lined systems, we are developing a three-dimensional correlation algorithm ( Mazeh et al. in prep.). Apart from enabling us to extract all the information contained in our composite spectra, this new algorithm should prove useful in identifying the main factor that limits our current precision on the velocities of HD 188753 A."
1083,hal-00398330.training.tei.xml,21,"Whilethree planets have been found so far in binaries with a separation of ∼20 AU (Sect. 1), HD 188753 Ab was the only planet known to reside in a tighter system. If HD 188753 Ab is removed from the list of planetary candidates, it may be tempting to associate the value of ∼20 AU with a ""minimum separation"" for considering that a binary (or a pair) possibly harbors a circumprimary giant planet. Nevertheless, we must bear in mind that selection effects are still strongly against planet detection in the closest binaries. Hence, the ""limit"" at ∼20 AU may alternatively reflect our present detection capabilities, or rather their limits. In order to investigate the occurrence of planets in the closest binaries, we must therefore first characterize our detection capabilities in detail for various types of systems. This work is in progress, and definitive results from the two planet search programs targeting spectroscopic binaries ( Eggenberger et al. 2003;Konacki 2005b) should provide stronger constraints on the reality of the 20-AU ""limit""."
1084,jpa-00251546.training.tei.xml,0,"Macromolecules form different kinds of aggregates in aqueous solution. This holds especially for amphiphiles with a hydrophilic and a hydrophobic part. Depending on the molecular structure of the amphiphiles, their concentration and the temperature, these aggregates may be micelles, vesicles, cubic phases, gels or lamellar stacks. Scattering methods like X-ray or neutron small-angle scattering, dynamic and static light scattering are widely used to determine the structure of such aggregates. Rheological experiments show the formation of gels indicated by a strong increase of viscosity. Rheological experiments interact strongly with the sample and scattering experiments are rather time consuming and difficult to interpret, especially in the case of higher concentration."
1085,jpa-00251546.training.tei.xml,1,Two newand complementary methods will be described in the following. The speed of sound waves depends slightly on the number of dissolved particles at fixed mass. Precision experiments using the pulse method can be used to follow micellization in a wide concentration range. Densitometry using the mechanical oscillator method is well established since about 2 decades . A new modified version of this instrument allows the determination of the ratio between the damping force and the elastic force of the oscillator. This method can be used to monitor qualitatively the formation and dissociation of gels as will be shown.
1086,jpa-00251546.training.tei.xml,2,"Triblock Copolymers of the type PEO/PPO/PEO (polyethylene oxide / polypropylene oxide / polyethylene oxide) exist in complex states of aggregation in aqueous solution depending on the relative block sizes. Monomers, micelles and larger aggregates coexist in relative proportions that depend on temperature and concentration. The presently used material (designated Pluronics P-85) has a monomeric molecular weight of4600(PE02,/PP03,/PE0,,) purified by Serren Hvidt (Roskilde, Denmark) to remove PEO/PPO dimers. This system has allready been studied by dynamic and static light scattering and oscillatory shear measurements ."
1087,jpa-00251546.training.tei.xml,3,"These experiments showed a transition from a mostly monomeric state at low temperatures to a micellar solution at about2 0"" ~ decreasing with concentration. This transition is difficult to measure with increasing concentration by means of scattering techniques due to increasing interparticle interaction. At concentrations higher than 25%(W/W) and temperatures of about 35type'Cthere is a sharp transition from the micellar phase to a solid-like glass-clear gel. At higher temperatures the gel ""dissolves"" again. The transitions to and from the gel state are accompanied by extreme changes in viscosity. It will be shown in the following that the formation of micelles can be followed in a wide concentration range by measurements of the speed of sound waves (pulses). The formation of the gel phase can be detected by densitometry with the advantage of negligible energy input into the sample (compared with oscillatory shear experiments)."
1088,jpa-00251546.training.tei.xml,4,"The speed of sound wave (SSW) is a function of the adiabatic compressibility and the density of the fluid. The compressibility of solutions is a function of the tempera-ture and of the solute properties. The speed of sound can be measured with high precision utilizing the pulse mode. The time required for a short pulse () to run trough a well defined sample cell (total volume 2ml) is used to calculate the speed with an accuracy of +1mjs(DSA 48, Anton Paar K.G. Graz, Austria). The speed of sound at low concentrations is linearly depending on the amount of solute in the solution. In addition there is a further dependence on the number of particles in the solution at fixed concentration. So it is possible to follow the hydrolisis of sucrose into glucose and fructose. This pseudo firstorder reaction leads to an exponential decrease of the concentration of sucrose. In a logarithmic plot the data points should fit a straight line. As shown in Fig.1, this inversion can nicely be followed by SSW measurements. This increase of the speed of sound with increasing particle number is the starting point for the micellization studies using SSW experiments, as the number of particles is significantly reduced during micellization. The dependence of the speed on the particle number is very small. We therefore use the specific speed S, of these waves:"
1089,jpa-00251546.training.tei.xml,5,"This specific speed S, is the relative change of the speed in the solution SsOwm compared to the speed in water S , , ."
1090,jpa-00251546.training.tei.xml,6,The influence of the temperature on the speed of sound waves in water is eliminated automatically when the specific speed is introduced.
1091,jpa-00251546.training.tei.xml,7,"We want to follow the formation of micelles with increasing temperature. Firstof all the specific speed will be reduced by the temperature dependence of the solution. In addition we expect an additional reduction during micellization, as the number of particles in the solution is reduced by this process. Changes can be best visualized by taking the derivative of the interesting variable and, taking into account the decrease, one can use the negative first derivative of the specific speed S ' :"
1092,jpa-00251546.training.tei.xml,8,The dependence of S' on the temperature T for a sucrose solution (10%W/W) and a P-85 solution (5%W/W) is shown in F i g. 2 .
1093,jpa-00251546.training.tei.xml,9,F+g. 2: Change of the specific speed of sound waves S' as a functlon of the temperature T for10%sucrose (xxx) and 5%P-85 (+++).
1094,jpa-00251546.training.tei.xml,10,"The solution with sucrose shows a monotonically decreasing behaviour, i. e., there is only a slight change of the specific speed with temperature. The curve for the P-85 solution is quite different. It matches the sucrose curve only at low and high temperatures. Deviations from the temperature behavior of sucrose starting at about20aC, show a clear maximum (highest micelle formation rate) at 29'C and vanish at about 45'C. This indicates that the formation of micelles essentially starts at 20°C and is finished at 45°Cwith a maximum in the formation rate at 29°C. We can also see from this figure that the magnitude of the effects depend on the chosen sample. We get a much higher signal for a 5%P-85 solution than for a 10%sucrose solution."
1095,jpa-00251546.training.tei.xml,11,"The influence of a varying concentration of P-85 is given in the F i g s . 3 and 4. The signal has been divided by the concentration in order to scale the curves.We also see the nonlinearity of the effect for concentrations higher than 20% P-85. With decreasing concentration we have a constantly decreasing signal with an increase of the relative error. The effect of micellization can be measured nicely at concentrations as small as 0.5% (see Fig. 4). We have performed experiments at even lower concentrations. There is still a significant peak at the micellization temperature, but more sophisticated data manipulations (smoothing) and repeated experiments would be necessary at concentrations lower than 0.5%."
1096,jpa-00251546.training.tei.xml,12,"It can be concluded from the above results, that SSW experiments can be used to follow micellization. The resulting change in the specific speed of the sound waves indicates and covers the temperature regime where the micellization takes place. The maximum of this formation curve can be used as a parameter for a mean micellization temperature. The fact that it is really a micellization that takes place cannot be found from such experiments. For this purpose it is still necessary to perform some, much more time consuming, scattering experiments."
1097,jpa-00251546.training.tei.xml,13,"Precision densitometry uses the mechanical oscillator method. This method is based on the measurement of the eigenfrequency of a Ushaped glass tube filled with the sample. In the standard setup (DMA 58, A. PAAR K.G., Graz, Aus-tria) the external force is of the same amplitude as the damping force but phase shifted by 180degrees to maintain steady oscillation. Such a mechanically driven oscillation can be represented by the external force F , and the following three internal forces: the elastic spring force which is proportional to the dislocation x of the mass cx (c is the spring constant), the damping force (energy absorbing) which is proportional to the velocity x' of the mass bx' (b is the damping constant) and the mass force which is proportio-nal to the acceleration mx"" of the mass m of the system . The external force F , must compensate the damping force, i.e., it must be equal to-bx'. Such a system with synchronized external force will perform a steady harmonic oscillation with a dislocation This situation is visualized in Fig. 5. We call this operation mode, where the external force is balancing the damping force, in phase mode. Here we are not further interested in the well known details of densitometry, but we have to discuss the modifications that allow the qualitative determination of the damping force (viscosity) with such a densitometer. Actually it can be shown that it is possible to measure the ratio between the damping force and the spring force utilizing a second experiment with a phase shifted mode. In this mode the external force Fen is shifted by 45 degrees. The amplitude is increased so that-.""C;1 the component in direction of the damping force remains constant (see In this mode we observe a different frequency a , or period T,. Combining the twocorresponding differential equations we get"
1098,jpa-00251546.training.tei.xml,14,"We see that the ratio DR between the damping force and the spring force is simply related to the oscillating periods. This function runs through a maximum at r]' (see Fig. 7) and goes tozeroat high values of r ] , where the fluid is no longer streaming in the tube but acts like a stiff rod. The value v* is about 100mPa.s in the basic mode (or fundamental mode, frequency w,) of the oscillator. The maximum 7' can be shifted to about 600mPa.s if the first order mode oscillation (frequency ul) of the tube is excited, but this does not change the situation essentially. This means that this technique is not suited to ""measure"" viscosity qualitatively in a wide range. We have to keep in mind these facts when we discuss the results from gelation data, where the viscosity increases in a sharp transition over many orders of magnitude. The advantage of the densitometer method lies in the facts that only a small amount of sample ( * 1ml) is simply filled in a well thermostated and closed glass tube and that only a negligible energy input into the sample takes place (extremely low shear strain). More difficult is the relation between this ratio (mostly depending on the damping force with a quasi constant spring force) and the viscosity r ]. There is a linear dependence for small values of fi ."
1099,jpa-00251546.training.tei.xml,15,The P-85 solution is a fluid with low viscosity at low temperatures and at low concentrations. At concentrations lower than20%W/W there is a weak temperature dependence up to 60'C as we can see from Fig. 8 for experiments using basic mode oscillation. Then follows a constant increase up to the maximum measured temperature from 70 to 78°C. A gel phase with moderate viscosity exists in this high temperature regime and the transition is not sharp. This behaviour is also found by oscillatory shear experiments.
1100,jpa-00251546.training.tei.xml,16,"At concentrations of25% and more exists a gel phase with very high viscosity and sharp boundaries in a certain temperature interval1 around 4 0"" ~. The width of this regime increases with concentration. If we take for example a concentration of 25%, we see a slight increase of the damping ratio (i.e. viscosity) between 20'C and 30'C. At 31°C the gel is formed. It has such a high viscosity ( 7 >> 7 ' ) that the damping ratio is smaller than the one for water (negative values). The stiff gel is dissolved again between 50'C and 52°C. The regime of the stiff gel is much wider for the 30% solution, where it starts at 23'C and ends above 61'C. The anomaleous behavior of the 30%solution at temperatures higher than 65°C is not yet studied in detail."
1101,jpa-00251546.training.tei.xml,17,"1 .oo If we use the first order mode of oscillation, we have to expect a higher damping ratio due to its linear dependence on the frequency o (o, * 600) and a wider range of measurable ratios as 7 'increases. So it is possible to measure higher damping ratios at the sharp phase boundaries of the stiff gel (see Fig. 9)."
1102,jpa-00251546.training.tei.xml,18,Densitometry has proved to be a new and complementary method to follow qualitatively changes in the viscosity of a fluid with minimum energy input into the sample and small volume. Excellent temperature control is guaranteed. Formation and dissolution of gels in P-85 solutions can nicely be detected. Calibration techniques may allow quantitative experiments in the future. I thank the Labor fur Messtechnik Dr. Hans Stabinger and his coworkers for technical assistance and fruitful discussions.
1103,inserm-01196815.training.tei.xml,0,Background: Recent findings showed that elite Ironman triathletes competing in 'Ironman Hawaii' improved both split and overall race times. The present study investigated whether elite athletes also improved in transition time (i.e. time needed between disciplines for changing clothes and equipment).
1104,inserm-01196815.training.tei.xml,1,"Triathlon is a sport consisting of sequential swimming, cycling and running. Triathlon races are mainly held on the short or Olympic distance (i.e. 1.5 km swimming, 40 km cycling, and 10 km running)  and the long distance, the Ironman distance (i.e. 3.8 km swimming, 180 km cycling, and 42.2 km running) . Apart from these distances, also the sprint distance  and the half Ironman distance 'Ironman 70.3' are of high popularity."
1105,inserm-01196815.training.tei.xml,2,"As one of the toughest and oldest long-distance triathlons in the world, the 'Ironman Hawaii' now serves as the World Championship for Ironman distance triathletes'. It has been recently shown that the annual top ten finishers have improved split and overall race times between 1983 and 2012, whilst also becoming older . During the same period, the annual ten fastest women and men improved their swimming (only men), cycling, running, and overall race times . In addition, the sex difference in overall race time decreased significantly from 15.2% to 11.3%. For the split disciplines, the sex difference remained unchanged for swimming (12.5 ± 3.7%) and cycling (12.5 ± 2.7%) but decreased for running from 13.5 ± 8.1% to 7.3 ± 2.9% ."
1106,inserm-01196815.training.tei.xml,3,"During a triathlon, the triathletes have to change clothes and equipment between the swimming and the cycling part and then again between the cycling and the running part. These transition times between thethree disciplines are part of the overall race time. As such, a triathlete's ability to negotiate each transition quickly and effectively has been highlighted as an important factor for overall success in the event . However, to our knowledge, no study has previously focused on the changes in the transition times over the years in Ironman triathletes. Overall race time could also be substantially improved by a reduction in transition times. Therefore, the purpose of the present study was to investigate the change in transition time in 'Ironman Hawaii' across years. Since both female and male triathletes have improved overall race times and split times during the last three decades, we hypothesized that both women and men also improved transition times across years. To investigate a potential difference in transition times between different race distances, we also compared transition times in 'Ironman Hawaii' to transition times in the World Championships 'Ironman 70.3' covering the half distance (1.9 km swimming, 90 km cycling and 21.1 km running) of the Ironman distance triathlon."
1107,inserm-01196815.training.tei.xml,4,"All procedures used in the study met were approved by the Institutional Review Board of Kanton St. Gallen, Switzerland with a waiver of the requirement for informed consent of the participants given the fact that the study involved the analysis of publicly available data."
1108,inserm-01196815.training.tei.xml,5,"The data set for this study was obtained from the race website of the 'Ironman World Championship'. All competitors who finished within the top ten in the 'Ironman World Championship' Hawaii between 1998 and 2013 were analyzed regarding their change in performance, the sex difference in performance and the transition times. Before 1998, transition times were not separately recorded in the race results. Therefore, data before 1998 could not be included into data analysis. To determine the sex difference in performance, overall race times and split times of the annual top and of the annual top ten women and men were taken from the race results of the organizer. Unfortunately, data from [6] do not allow to differentiate the two transition times (i.e. swimming to cycling and cycling to running) during the whole period 19982013. Therefore, absolute transition times were determined in absolute values by calculating the difference between overall race time and split times using the equation overall race times-(swimming times + cycling Table 1 Multi-level regression analyses for change in transition times (i.e. in absolute times and relative in % of overall race times) for the annual fastest and annual ten fastest across years after correction for multiple finishes in 'Ironman Hawaii' times + running times). Relative transition time was expressed as a percentage of overall race times using the equation 100/overall race times × transition times. The sex difference was calculated using the equation (times in women-times in men)/times in men × 100, where the sex difference was calculated for every pair of equally placed athletes (e.g. between female and male winner, between men and women 2 nd place, etc.) before calculating mean value and standard deviation of all pairs. In order to facilitate reading all sex differences were transformed to absolute values before analysing."
1109,inserm-01196815.training.tei.xml,6,"In order to investigate a potential difference in transition times between different triathlon distances, we compared the transition times in 'Ironman Hawaii' to the transition times in the World Championships 'Ironman70.3' covering the half distance of the Ironman triathlon (i.e. 1.9 km swimming, 90 km cycling and 21.1 km running) ."
1110,inserm-01196815.training.tei.xml,7,"Each set of data was tested for normal distribution and for homogeneity of variances prior to statistical analyses. Normal distribution was tested using a D' Agostino and Pearson omnibus normality test and homogeneity of variances was tested using a Levene's test. Single and multilevel regression analyses were used to investigate changes in performance. A hierarchical regression model was used to avoid the impact of a cluster-effect on results in case one athlete finished more thanonce in the annual top or annual top ten. Since the change in sex difference in endurance performance is assumed to be non-linear , we additionally calculated the non-linear regression model that fits the data best. The result of the linear regression analysis was compared to the best-fit result of the non-linear analysis using Akaike's Information Criteria (AIC) and F-test in order to show which model would be the most appropriate to explain the trend of the data. Split times in swimming, cycling, and running as well as overall race times and transition times in top ten women and men and differences in transition times between 'Ironman Hawaii' and 'Ironman 70.3' in top ten men and women were compared using multiple t-tests with Holm-Sidak correction for multiple comparisons. Statistical analyses were performed using IBM SPSS Statistics (Version 22, IBM SPSS, Chicago, IL, USA) and GraphPad Prism (Version 6.01, GraphPad Software, La Jolla, CA, USA)."
1111,inserm-01196815.training.tei.xml,8,Significance was accepted at P <0.05 (two-sided for ttests). Data in the text and figures are given as mean ± standard deviation (SD).
1112,inserm-01196815.training.tei.xml,9,Changes in transition times in 'Ironman Hawaii'
1113,inserm-01196815.training.tei.xml,10,"The transition times (Figure 1) remained unchanged acrossyears for the annual fastest women but increased linearly for the annual fastest men in both absolute and relative terms (Table 1). For the annual ten fastest, the transition times increased linearly for both the annual ten fastest women and men in both absolute and relative terms. The sex difference in the transition times remained unchanged across years for the annual fastest triathletes, but decreased linearly for the annual ten fastest triathletes from 48.0 ± 47.6% (1998) to 18.9 ± 28.3% (2013). Changes in transition times in 'Ironman 70.3'"
1114,inserm-01196815.training.tei.xml,11,"In 'Ironman70.3' (Figure 2), the transition times remained unchanged for the annual fastest in both absolute and relative terms (Table 2). For the annual ten fastest, transition times decreased linearly for both women and men in both absolute and relative terms. The sex difference in transition times remained unchanged across years for both the annual fastest and the annual ten fastest triathletes."
1115,inserm-01196815.training.tei.xml,12,Hawaii' and 'Ironman70.3'
1116,inserm-01196815.training.tei.xml,13,When the changes in transition times acrossyears between 'Ironman Hawaii' and 'Ironman 70.3' for the The sex difference in transition times remained unchanged at 18.4 ± 11.5% during the whole period.
1117,inserm-01196815.training.tei.xml,14,"Changes in split times, overall race times and sex difference in 'Ironman Hawaii'"
1118,inserm-01196815.training.tei.xml,15,"For the annual fastest competitors (Figure 4), swimming split times remained unchanged, cycling split times decreased linearly in men, running split times decreased linearly in women and overall race times decreased linearly in both women and men (Table 3). For the annualten fastest ( Figure 5), swimming split times decreased linearly in women, cycling split times, running split times and overall race times decreased linearly in both women and men (Table 3). The sex difference in performance ( Figure 6) decreased linearly in swimming for the annual ten fastest, in swimming for the annual fastest and the annual ten fastest and for overall race in the annual ten fastest (Table 4). For cycling, the sex difference remained unchanged for both the annual fastest and the annual ten fastest."
1119,inserm-01196815.training.tei.xml,16,"This study investigated the changes in transition times in 'Ironman Hawaii' acrossyears between 1998 and 2013. It was hypothesized that both elite women and men would improve transition times across years since they also improved split and overall race times across years. To investigate a potential difference in transition times between different race distances, transition times in 'Ironman Hawaii' were compared to transition times in the World Championships 'Ironman 70.3' covering the half distance of the Ironman triathlon. The main findings were that (i) in 'Ironman Hawaii' , transition times increased in both women and men whereas the sex difference in transition times decreased, (ii) in 'Ironman 70.3', transition times decreased in both women and men whereas the sex difference remained unchanged, and (iii), transition times were slower in 'Ironman Hawaii' compared to 'Ironman 70.3'."
1120,inserm-01196815.training.tei.xml,17,"In 'Ironman Hawaii' , swimming split times decreased linearly in women, cycling split times, running split times and overall race times decreased linearly in both women and men for the annualten fastest. The transition times increased linearly for both the annual ten fastest women and men in both absolute and relative terms. Generally, elite Ironman triathletes competing in 'Ironman Hawaii' improved performance in both split and overall race times . A recent study investigating the change in performance across years in elite finishers in 'Ironman Hawaii' from 1983 to 2012 using only linear regression analyses showed similar findings where the annual ten fastest women and men improved their swimming (only men), cycling, running, and overall race times ."
1121,inserm-01196815.training.tei.xml,18,"The new finding in the present study was that transition times increased acrossyears, although it was hypothesized that transition times would decrease since both women and men improved split and overall race times. A potential explanation for this unexpected finding might be that the transition area has changed across years. The distances between the exit out of the water to the cycling area and the distance between the finish of the cycling to the start of the running might have increased and the athletes might have needed to run longer to change the discipline. A potential change in the transition area could be due to the increasing number of participants in 'Ironman Hawaii' in the last years. Another explanation could be that the athletes focused Table 3 Multi-level regression analyses for changes in split and overall race times for the annual fastest and annual ten fastest women and men across years after correction for multiple finishes more on the performance in swimming, cycling and running and were not aware that overall race time could also be substantially improved by a reduction in transition times. And a further explanation could be the different time periods investigated in the different studies."
1122,inserm-01196815.training.tei.xml,19,Differences in transition times between 'Ironman70.3' and 'Ironman Hawaii'
1123,inserm-01196815.training.tei.xml,20,"The comparison of the transition times between 'Ironman70.3' and 'Ironman 70.3' showed that transition times decreased in both absolute and relative terms in the shorter race distance 'Ironman 70.3' whereas in the longer race distance 'Ironman Hawaii' the transition times increased also in both absolute and relative terms. When the transition times were compared between 'Ironman 70.3' and 'Ironman Hawaii' , athletes changed faster in the shorter race 'Ironman 70.3' than in the longer race 'Ironman Hawaii'. A potential explanation for this finding could be that the shorter race is faster in both the split and the transition times than the longer race. A further explanation could be that the transition area is smaller in 'Ironman 70.3' compared to 'Ironman Hawaii' due to the different numbers of participants."
1124,inserm-01196815.training.tei.xml,21,"During theyears, athlete may also have improved their technique to change faster in the transition area ."
1125,inserm-01196815.training.tei.xml,22,"Maybe the athletes are more exhausted in the full Ironman than in thehalf Ironman after a split discipline when they enter the transition area and are therefore slower in the transition in the full Ironman distance compared to the half Ironman distance. Other explanations could be different transition areas in the different races and the different location of the races. Apart from changes in the transition area across years, the World Championship 'Ironman Hawaii' was held during the investigated period at the same place whereas the World Championship 'Ironman 70.3' was held in 20062010in Clearwater, Florida, USA, and in 2011 in Henderson, Nevada, USA. The two different race courts might also have different transition areas."
1126,inserm-01196815.training.tei.xml,23,"A limitation in this study is that the transition area may have changed during this period and therefore the absolute time an athlete spent betweentwo disciplines may have be different from year to year. However, the transition times were expressed here as absolute values and as relative values as a percentage of overall race time. When the transition times changed due to changes of the transition area, overall race time changed accordingly. A further limitation is that transition times between swimming and cycling and between cycling and running were not separated but unfortunately transition times between the disciplines were provided only in recent years. The age of the athletes was not included. It has recently been shown that sex differences in overall race times differed between age groups in sprint, Olympic distance, and half Ironman triathlon . Since the sex difference in long-distance triathlon increased with increasing race distance , future studies should investigate the sex difference in transition times for different triathlon distances. In addition, the present study focused on elite triathletes only, it would interesting to analyse (i) the sex difference in transition times for age groups triathletes and (ii) the agerelated change in transition times. It is also worth considering in future research what would constitute a practically meaningful change in 'Ironman' and 'Ironman 70.3' transition times in the context of transition times alone and in relation to overall performance. For example, when the typical within-athlete variability is not available, the average time difference between each of the top ten finishing positions would give an idea of a worthwhile enhancement in performance times, and, more importantly, in transition times."
1127,inserm-01196815.training.tei.xml,24,"In summary, transition times increased for both women and men in 'Ironman Hawaii' whereas the sex difference in transition times decreased. In 'Ironman70.3' , transition times decreased for both women and men whereas the sex difference in transition times remained unchanged. Generally, transition times were slower in 'Ironman Hawaii' compared to 'Ironman 70.3'. Top ten finishers in 'Ironman Hawaii' might further improve their overall race times by reducing their transition times. Table 4 Multi-level regression analyses for changes in sex difference in split and overall race times for the annual fastest and annual ten fastest women and men across years after correction for multiple finishes"
1128,inserm-01196815.training.tei.xml,25,The authors declare that they have no competing interests.
1129,inserm-01196815.training.tei.xml,26,"Authors' contributions CR performed the statistical analyses and drafted the manuscript, BK collected the data and helped in drafting the manuscript, TR and RL helped in drafting the manuscript. All authors read and approved the final manuscript."
1130,hal-00535466.training.tei.xml,0,Copyright Springer Verlag
1131,hal-00535466.training.tei.xml,1,"HAL is a multidisciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt etàetà la diffusion de documents scientifiques de niveau recherche, publiés ou non, ´ emanant desétablissementsdesétablissements d'enseignement et de recherche français oú etrangers, des laboratoires publics ou privés. Abstract In most Western European countries, including Belgium, judicial alternative sanctions are increasingly being used for drug users. Because no study into the effectiveness of Belgian judicial alternatives for drug users has yet been carried out, this became the objective of the current research. The design of this study comprises a pre and post measurement of the criminal activity, drug use and situation in different spheres of life of565 drug-dependent offenders. Two conclusions can be drawn. First, after an alternative sanction or measure is imposed, there is a reduction in the criminal activity of the offender. Second, this crime reduction goes hand in hand with a progress in several relevant life spheres."
1132,hal-00535466.training.tei.xml,2,"In the Belgian social, academic and policy context, the application of alternative sanctions and measures for drug users is increasingly stimulated. Belgian knowledge concerning the effectiveness of alternative measures and sanctions, however, is still largely based upon American effect studies. In the United States, effect studies gain in importance, whereas Europe lags behind. Effect studies are rarely conducted in Europe and even for meta-analyses European authors are thus forced to rely mainly on American research results. Every effect study that is conducted in Europe thus Crime Law Soc Change (2009) 52:513-525 DOI 10.1007 adds to the knowledge that is so badly needed in this field. The current research is aimed at measuring effects of alternative measures and sanctionsfor drug users. The objective of the research is to examine whether alternative measures and sanctions generate effects, and if so, what is the nature of these effects."
1133,hal-00535466.training.tei.xml,3,"The effects examined in the current study concern recidivism and progress in several life spheres. The methods and techniques used in this study are built along the lines set out in previous studies. Effect evaluations of judicial interventions generally focus on recidivism. Arrest and reconviction rates are the most commonly used indicators for the prevalence of recidivism, although European studies tend to restrict their operationalisation of recidivism to reconviction rates. In this study, however, we did attempt to find a European counterpart for arrest rates. The operationalisation ""arrest rate"" cannot simply be borrowed from American literature, but instead has to be adapted to the Western-European judicial registration. We opted for criminal charge as a counterpart for arrest, considering the fact that this is the basis of registered crime in Belgium. Effect evaluations of treatment interventions more often integrate progress in several life spheres in their design. However, measuring progress in life spheres is a difficult task. The main difficulty is to find criteria that measure progress, without them being too subjective or morally coloured by the researcher. As a consequence, we can see that a lot of differences exist in the way this progress is measured in several studies. We based our choice of the life spheres and the criteria for improvement mainly on the EuropASI-instrument, which is used as an assessment instrument for clients of drug treatment services. We thus studied drug use, financial situation, housing, social and family relationships and leisure time. For each of these life spheres we defined an ultimate goal, which is reached when there is no more need for help or treatment. This information was drawn from judicial files, which include social reports. We assessed the offenders' situation both before and after the judicial alternative, using the grid shown in Table 1.Design and data sources"
1134,hal-00535466.training.tei.xml,4,"The design of this study comprises a pre and post measurement of the criminal activity of565 research subjects, and of their situation in relevant life domains. Regarding recidivism, the pre measurement concerns the calculation of rates of criminal charges and convictions (number per year) before the judicial alternative was granted. For the post measurement the same rates are calculated, but in this case for the period after the judicial alternative was granted. The calculation of these rates is based on criminal record data, gathered from criminal courts. Our results are thus only based on registered crime, while not necessarily all offences committed by the research subjects are known to the police. This so-called dark number is a much described limitation of officially registered crime, which has to be considered in the interpretation of the results. The measurement of progress in relevant life spheres comprises a pre and post measurement as well. The situation in these life spheres was studied in judicial files (of the prosecutor's office and of judicial assistants). Based on these files the researcher assessed the situation in the life spheres before and after the judicial alternative was granted, on the basis of the grid shown in Table 1."
1135,hal-00535466.training.tei.xml,5,"For every type of alternative sanction or measure, an overview of judicial files was requested from the prosecutor's office or at the justice houses in three judicial districts. The reference years of the judicial files were 1999 and 2001. The reference years are thus quite recent while still allowing for a considerable follow-up period. From the lists that were provided by the justice authorities, probability samples were drawn for each modality. In order to determine whether a causal relationship exists between two variables (e.g. an alternative sanction and recidivism), the criteria for methodological quality are high . Mostly, the Maryland Scientific Methods (MSM) Scale, a five level scale of research designs, is used as reference point for the identification of effect studies of good methodological quality. Only based on a (quasi-)experimental design, level three to five of the MSM Scale, can the conclusion be drawn that a reduction in recidivism is caused by the alternative sanction. In a quasi-experimental design recidivism would be measured before and after the intervention (i.e. the judicial alternative) in experimental and comparable control conditions. Our study does however not reach this methodological standard. In fact, our study corresponds to the second level of the MSM Scale, which implies the measurement of recidivism before and after the alternative measure/sanction, but without a comparable control condition. The Belgian sentencing practice does not allow for the use of the classical experimental design: magistrates generally opt for alternative sanctions when adjudicating a drug offender. Only if the offences are too serious, or if the offender persists in crime, is a traditional sanction imposed . Given this sentencing practice, any control group would substantially differ from the experimental groupthose who are granted a judicial alternative are not comparable to those who are adjudicated the traditional way."
1136,hal-00535466.training.tei.xml,6,"The control condition that is present in a quasi-experimental design serves as a means to rule out other factors, such as ""trends"" , ""regression to the mean"" , spontaneous recovery etcetera that may influence the results. The fact that we did not make use of a control group thus implies that we cannot abstract from influencing factors and that we have to bear in mind that the results we find cannot fully be attributed to the intervention studied. We considered this design a Previously existing trends persist. In a pre-post design the tendency is found that the follow-up values are again situated closer to the mean, because random error in the pre-measurement possibly caused a proportion of the extreme values.Improvement occurs spontaneously and does not stem from the judicial intervention. No structured/meaningful leisure activities good alternative, however, because it is longitudinal and allows outcomes to be studied in existing services and under day-to-day circumstances."
1137,hal-00535466.training.tei.xml,7,The results of this study concern the effectiveness of alternative measures for drug users. The objective was to find out to what extent these persons reoffend and whether progress occurs in the various spheres of life examined.
1138,hal-00535466.training.tei.xml,8,The sample
1139,hal-00535466.training.tei.xml,9,"We performed analyses on a sample of565 individuals who were sentenced to a modality of alternative sanctioning. The research included nine modalities of alternative measures, situated at the various levels in the criminal justice system (prosecution, sentencing, execution of sentences). The average age of the research subjects was 28 years, and the large majority were men (88,3%). Various types of narcotics were used by the research subjects, with cannabis and heroin heading the list. Most research subjects consumed more than one product. The majority of the research subjects were granted the judicial alternative because of possessing or selling narcotics (i.e drug offences: 58,2%). Just over one third had committed property offences or violent crime (37,1%). Different types of judicial alternatives were integrated in this study (cf. Table 2) This was deemed necessary, because in Belgium, every level of the criminal justice system has its own sanctions/measures that are designed to divert (drug) offenders away from crime and drugs. All these judicial alternatives, applied at different levels of the criminal justice system, are used in such a way that they allow for an individualized approach to dealing with (drug) offenders."
1140,hal-00535466.training.tei.xml,10,"We therefore cannot pass over the fact that the composition of the research sample is rather heterogeneous. Differences exist in the profiles of the research subjects according to the judicial alternative that was granted to them. Alternatives at the prosecutor's level were granted to the youngest research subjects, who had committed less serious offences (e.g. possession of marihuana). Those who were granted probation orders often had not only possessed drugs, but had also sold it. The detected use was no longer limited to (mainly) marihuana, but also encompassed, inter alia, extasy, cocaine and heroin. The research subjects who were granted an alternative for a prison sentence are somewhat older drug users in comparison with the above measures. Mostly, they had committed property and violent offences, and used heroin or cocaine."
1141,hal-00535466.training.tei.xml,11,"In order to take into account these differences in profiles, we present the general results and the results per level of the criminal justice system."
1142,hal-00535466.training.tei.xml,12,"Prevalence of recidivism Research subjects were followed up during a5 year period, in order to determine whether they had reoffended or not. The prevalence of recidivism (= having a new criminal charge) amounts to 71,7%. However, not every criminal charge was followed by a conviction. Only 36,6% of the research subjects were reconvicted. This percentage of new convictions is therefore far lower than the prevalence of new criminal charges. The prevalence of recidivism differs according to the level of the criminal justice system at which the judicial alternative was granted (see Table 3)."
1143,hal-00535466.training.tei.xml,13,"More conditionally/provisionally released offenders had a new criminal charge (81.9%) than those who had received an alternative sanction at sentencing level (70.0%); and those who had received their sanction at prosecution level had the fewest new criminal charges (57.7%). In addition, the prevalence of reconviction increased in line with the level of settlement of the judicial alternative (see Table 4). These differences in recidivism rates have to be considered in relation to the differences in the profiles of the research subjects according to the judicial alternative that was granted to them (see above: The sample)."
1144,hal-00535466.training.tei.xml,14,"Evolutions in criminal activity can be measured by calculating rates of criminal charges and convictions. We compare the rate (number per year) of criminal charges and convictions prior to the judicial alternative with the rate afterwards. Table 5 shows that the average rate of criminal charges decreases to about half of its original value. Whereas the research subjects on average were subjected to more than two criminal charges per year before the alternative was granted, this was reduced to only one afterwards. The research subjects thus have less criminal charges after the alternative was granted. This is true for the alternatives at every level of the criminal justice system: the average rate of criminal charges is lower after the alternative was granted than prior to it. This reduction is significant (p<0,001). At prosecution level, the rate was reduced from 1.87 to 0.82 (t=6,742; df=118; p<0.001); at sentencing level from 2.53 to 1.17 (t=8,481; df=179; p<0.001) and at the level of execution of sentences from 2.68 to 1.48. (t=6,608; df=164; p<0.001)."
1145,hal-00535466.training.tei.xml,15,"The same rates can be calculated with regard to the number of convictions (see Table 6). A first rate reflects the number of convictions on a yearly basis before the alternative was granted, a second rate reflects this number after the alternative was granted. If we compare both rates, we see that there is a significant reduction in the average rate (p<0.001). The reduction in the average rate is found for alternatives at every level of the criminal justice system. At prosecution level the average conviction rate fell from 0.31 to 0.15 (t=3,887; df=155; p<0.001); at sentencing level from 0.85 to 0.25 (t=8,575; df=181; p<0.001); and at the level of execution of sentences from 1.05 to 0.33 (t=7,874; df=162; p<0.001)."
1146,hal-00535466.training.tei.xml,16,"In the current study we gave attention tothree specific categories of offences, namely drug offences (infringements of the drug laws), property offences and violent offences. Before the alternative was granted, 504 research subjects had been charged with offences against the drug laws, 336 with property offences and 306 with violent offences. After the alternative was granted, this picture changes: 238 had a charge for drug offences (Chi Square=9,324; df=1; p<0.01), 202 for property offences (Chi Square=74,489; df=1; p<0.001) and 219 for violent offences (Chi Square=53,992; df=1; p<0.001). This means that, although all research subjects were drug users, only 45% of these persons got charged with drug offences again in the period after receiving their alternative sanction (see Fig. 1)"
1147,hal-00535466.training.tei.xml,17,"In the judicial files (in particular the reports of the judicial assistants) information was obtained concerning the situation in different life spheres. The situation in the life spheres before and after the alternative was assessed. The research subjects experience a progress in the life spheres studied after the judicial alternative is granted. The percentage of research subjects who use in a non-problematic way, who have a regular job, fixed residence, predominantly supportive contacts with nonusers and a meaningful occupation of their leisure time is higher at the end of the alternative than at the beginning (see Fig. 2). The percentage regarding drug use rose from32,9% to 67,1%, regarding employment from 20,5% to 30.5%, regarding housing from 56,5% to 69,4%, regarding social relations from 11% to 64.9% and regarding leisure time from 35% to 75,3%. Different steps are thus taken that benefit their reintegration. Improvements in the field of drug use, social and familial relationships and leisure-time activities are the most pronounced. The progress in life spheres parallels the reduction in criminal charges. Those who do not use problematically at the end of the follow-up period are also those that have been charged less. The same holds true for those having a regular job and fixed residence: they reoffended less than those who had not. (see Table 7)"
1148,hal-00535466.training.tei.xml,18,"As shown in the results, the prevalence of recidivism after a judicial alternative was granted to drug users was considerable. However, studies examining prevalence of recidivism often find high percentages. For example, Zanis studied persons who were referred to treatment services via ""early parole"". He found that22% of the research subjects were reconvicted within 24 months, compared to 34% of the control group (parolees who had not been referred to treatment). In a study examining treatment in prison a relatively high prevalence of recidivism was found as well. After treatment in prison, it appears that 58% of the graduated was rearrested and 79% relapsed into drug use in a follow-up period of 5 years. In a Swedish study, Bishop examined the effects of treatment after imprisonment: the results showed that 64% of those who completed treatment committed a new offence within 3 years following completion of the treatment. A UK Home Office research found that 86% of the persons who were granted a Drug Treatment and Testing Order, were reconvicted within a follow-up period of 2 years. The high prevalence of recidivism thus is not out of line with results of other studies. The fact that the percentages are so high, makes people question the effectiveness of alternative measures for drug users. Although the percentage of research subjects again charged with an offence is quite high, it should be taken into account that this just reflects a lower limit: it shows how many people succeeded in achieving the ideal situation, i.e. no new charge. In the current study, almost 30% of the research subjects did not have a new"
1149,hal-00535466.training.tei.xml,19,Nature charge
1150,hal-00535466.training.tei.xml,20,"Before After Fig. 1 Criminal charges before and after the judicial alternative charge filed against them after their judicial alternative. More than60% did not incur a new conviction. This dichotomous measurement of prevalence, however, passes over the slighter improvements in criminal activity that may occur. Studying rates of criminal charges and convictions may put the high prevalence of recidivism into perspective. In the current study, rates of criminal charges and conviction rates were lower after the alternative was imposed than prior to it. This means that-even if the research subjects reoffend-the number of charges per year decreases, as does the number of convictions per year. Considerable progress could also be observed in the current study as regards the nature of the offences. Only 45% of the sample (or 61% of the recidivists) had a new charge for infringements of drug legislation-although all research subjects are drug users. Even if account is only taken of the recidivists, it appears that a lower percentage of the research subjects commit infringements of drug legislation, property offences and violent crime after sanctioning than prior to it. The reduction in these offences is important, as it concerns those crime categories that have been described in literature as offences that are possibly drug-related (cf. consensual, property offences and expressive criminality)."
1151,hal-00535466.training.tei.xml,21,"In summary, sufficient arguments are found to support the link between alternative measures for drug users and recidivism reduction: (i) More than60% of the research subjects do not incur a new conviction; (ii) If the average rate of charges and of convictions of the research subjects prior to the alternative measures is compared with both these rates after the alternative measures, we see that there is a significant decrease. (iii) There is a reduction of infringements of the drug legislation, property offences and violent crime. Even if there is no certainty that these successes can be ascribed to alternative sanctions (because of the absence of a control group), it is still a fact that the alternative sanctions allowed for these developments in a way that is far less interventionist and harmful than a prison sentence. The recidivism reducing effects of alternative measures were also shown in other studies. In a research conducted by MacKenzie, for example, a self report was held among probationers. They were asked to compare their activities prior to the probation order with those after their arrest and the start of the probation period. The interviews were held shortly after the start of the probation period, and again after a period of 6 months. In the interviews it became clear that the average level of criminal activity and drug use was lower after arrest and during the probation period than previous to the arrest. We could even argue that the reduction in criminal activity is shown more convincingly in our study, because of the fact that we made use of registered crime data and not of self report measurement, which possibly is influenced by a social desirability bias. Moreover, it is shown that alternative measures have better results than traditional ones. In American studies regarding effectiveness of drug courts the results are generally similar: clients of the drug court programs have lower recidivism rates than those whose case was adjudicated the traditional way. Spohn observed that 42,1% of the drug court participants reoffended, compared to 60,8% of the research subjects with a traditional sanction. Comparable results (drug court participants have lower levels of recidivism) are found in eight out of nine studies regarding effectiveness of drug courts. Anyway, it is clear that drug using delinquents who receive some kind of treatment perform better than those who do not participate in treatment ."
1152,hal-00535466.training.tei.xml,22,"Finally, there is also an improvement in virtually every life sphere of the research subjects. The percentage of individuals using drugs in a non-problematic way and having a steady job and permanent housing increases. More people appear to succeed in surrounding themselves with people who support them (in a drug-free life) and in finding activities for their leisure time. In the context of alternative sanctions, various steps are thus taken that benefit the re-integration of the person involved. Kyvsgaard examined progress in life spheres based on a subjective evaluation by the persons concerned. In this Danish study, research subjects were asked whether they had received the help they needed during the follow-up period.54% felt helped concerning drug problems; 47% concerning their work situation; 46% concerning leisure time activities; 44% concerning social and family relationships; and 31% concerning housing. These research findings support the conclusion that alternative measures contribute to the social capital."
1153,hal-00535466.training.tei.xml,23,"In addition, we observed a strong association between the effects regarding progress in life spheres and recidivism. Research subjects who do well as regards drugs, work and housing at the end of their alternative and reach the 'desired' In the United States drug (treatment) courts were established in order to deal with the problems that had arisen following the increasing number of drug cases. Individuals who have committed drug related crime can be brought before the drug court. Certain offenders are however excluded, e.g. violent offenders. The drug court program implies the start of interventions aimed at the reduction of drug use and crime and based on regular contacts with the drug court, drug testing and treatment. See:. situation reoffend less often. The finding of the current study that the reduction in recidivism parallels the improvement in the life spheres has an important consequence. If the recidivism reduction would not be associated with the progress in life spheres, this could mean that the reduction in recidivism merely stems from the short-term deterring effect of the alternative measure: the offender commits less crime, because he is deterred by the higher costs that exist during the period of supervision. Because we find that the reduction in recidivism goes together with the establishment of social bonds, longer lasting changes can be expected. A body of research has already shown that societal bonds have an impact on criminal behaviour. The societal bonds of offenders often are weak. When offenders establish these bonds, however, chances increase that their lives will change drastically. If societal bonds (work, housing, family and social relationships) are intensified through the follow-up of a judicial alternative, informal social controls that are necessary for desistance of crime are stimulated. Especially the circumstances of having a partner (or broader: good family relationships) and being in employment induce a reduction in criminal behaviour. The establishment of these bonds can thus serve as a turning point that initiates positive changes."
1154,hal-00535466.training.tei.xml,24,"The present study comprises a measurement of effect based on565 judicial files. The objective was to measure effects of judicial alternatives on recidivism and life spheres. In general, the results can be considered moderately positive. The question asked in the title of this article cannot be answered in a straightforward manner. Judicial alternatives for drug users cannot be considered fruitless efforts nor miracle solutions. As is often the case, we have to adopt a middle course: it is clear that miracles cannot be expected from the application of alternative measures to drug users, but there is no need for pessimism either."
1155,hal-00535466.training.tei.xml,25,"It was found that a large majority of the research subjects faced a new criminal charge after the alternative sanction had been imposed. Not every charge was followed by a conviction, because less-serious charges are dismissed by the public prosecutor. Just overone third of the research subjects were reconvicted. The prevalence of recidivism differed according to the level of alternative sanction: the prevalence of criminal charges and reconviction increased in line with the level of settlement of the judicial alternative. Even though the prevalence of recidivism is high, literature shows that this is not out of line with other studies. Besides, it is beyond doubt that there is a reduction in recidivism after a judicial alternative is granted. On average, the research subjects incur less criminal charges and convictions after the judicial alternative is granted than prior to it. Also there is a decrease in the consensual drug related crime, property crime and violent crime committed by research subjects (expressive and acquisitive crime). This goes together with a remarkable progress in life spheres: research subjects use less drugs-and use it in a less problematic way-have more supportive family and social relationships and their leisure time is occupied. Regarding their work situation there is a progress as well, but less pronounced. The association found between recidivism reduction and life spheres is important. Progress in the abovementioned life spheres supports the positive evolution regarding recidivism reduction. The perspectives are thus promising, as chances are higher that changes will last longer. From this perspective, a judicial alternative is possibly a life event that sparks important changes."
1156,hal-01205501.training.tei.xml,0,"Whole-body control in unknown environments is challenging: Unforeseen contacts with obstacles can lead to poor tracking performance and potential physical damages of the robot. Hence, a whole-body control approach for future humanoid robots in (partially) unknown environments needs to take contact sensing into account, e.g., by means of artificial skin. However, translating contacts from skin measurements into physically well-understood quantities can be problematic as the exact position and strength of the contact needs to be converted into torques. In this paper, we suggest an alternative approach that directly learns the mapping from both skin and the joint state to torques. We propose to learn such an inverse dynamics models with contacts using a mixture-of-contacts approach that exploits the linear superimposition of contact forces. The learned model can, making use of uncalibrated tactile sensors, accurately predict the torques needed to compensate for the contact. As a result, tracking of trajectories with obstacles and tactile contact can be executed more accurately. We demonstrate on the humanoid robot iCub that our approach improve the tracking error in presence of dynamic contacts."
1157,hal-01205501.training.tei.xml,1,"A key challenge for torque-controlled humanoid robots is to accurately control their dynamics and the contacts with the environment, for example during manipulation in clutter, locomotion  and whole-body movements . In such complex situations, the dynamics of the system can be exploited to compute the required joint torques necessary to realize the desired trajectories of the joints. This is the idea at the base of computed torque control , also used in inverse dynamics control  and model predictive control applied to robot reaching and locomotion ."
1158,hal-01205501.training.tei.xml,2,"Inaccurate joint torques estimation can deteriorate the performances of computed torque control and tracking of desired trajectories, which can ultimately cause severe problems when controlling the robot motion in presence of uncertainties, disturbances and unexpected contacts with the environment. However, accurate estimation of joint torques and external forces in presence of contacts is difficult without dedicated sensors."
1159,hal-01205501.training.tei.xml,3,The research leading to these results has received funding from the European Council under grant agreement #600716 (CoDyCo-FP7/2007. MPD was supported by an Imperial College Junior Research Fellowship.
1160,hal-01205501.training.tei.xml,4,"1 Intelligent Autonomous Systems, TU Darmstadt, Germany calandra@ias.tu-darmstadt.de 2 Inria, Villers-l` es-Nancy, F-, France; CNRS, Loria, UMR n.7503 and Université de Lorraine, Vandoeuvre-l` es-Nancy, F-5460054500, France serena.ivaldi@inria.fr Its movement is obstructed by an unexpected obstacle, which decreases the tracking performance of a desired trajectory. We reduce the tracking error by learning the torque control to compensate the contact. This is also true for robots equipped with whole-body distributed tactile sensing, as small errors in the estimation of the contact location can substantially deteriorate the quality of the inverse dynamics estimation ."
1161,hal-01205501.training.tei.xml,5,"Two main lines of research are currently addressing this issue. On one side modelbased approaches, which rely on identification techniques to improve the dynamics parameters of the analytic models . On the other side, we have machine learning approaches, where nonparametric models of the robot dynamics can be learned from data collected on the robot . The main advantages of the latter approaches are that they do not usually require a prior calibration (kinematics/dynamics calibration  or spatial calibration of the tactile sensors ), are less prone to model errors, and can easily deal with noise and varying contexts ."
1162,hal-01205501.training.tei.xml,6,"In previous work, we proposed a data-driven mixtureof-experts learning approach based on Gaussian Processes, which predicts joint torques from joint encoders, tactile and force/torque sensors data. We applied such a model to learn the joint torques of the arm of iCub in presence of several varying contacts, showing that the learned model was more accurate than analytic dynamics models using the same sensors data . However, the proposed learning approach required the availability of joint torque sensors to supervise the learning process and provide the ground truth; Therefore limiting the application of this approach in many robots, in particular to almost all iCubs ."
1163,hal-01205501.training.tei.xml,7,"In this paper, we generalize the previous approach, re-laxing the need for joint torque sensors on the robot. Most importantly, we demonstrate in an active control task with the humanoid robot iCub (see Figure 1) the effectiveness of the proposed model, by showing that it can be efficiently used for computed torque control in presence of contacts with the environment. With the learned model we can decrease the tracking error in presence of known or novel contacts."
1164,hal-01205501.training.tei.xml,8,"To the best of our knowledge, this is the first demonstration of controlling a humanoid robot with a learned torque model using tactile and force/torque sensing in presence of dynamic contacts."
1165,hal-01205501.training.tei.xml,9,"Without contacts with the environment, the inverse dynamics (ID) of a robot with m degrees of freedom can be generally described by"
1166,hal-01205501.training.tei.xml,10,"where q ∈ R m , ˙ q ∈ R m and¨qand¨ and¨q ∈ R m are the joint positions, velocities and accelerations, respectively, M (q) ∈ R m×m is the inertia matrix. h (q, ˙ q) ∈ R m×m is the matrix combining the contributions from Coriolis and centripetal, friction (viscous and static) and gravity forces:"
1167,hal-01205501.training.tei.xml,11,"The term (q, ˙ q, ¨ q) captures the errors of the model, such as unmodeled dynamics (e.g., elasticities and Stribeck friction), inaccuracies in the dynamic parameters (e.g., masses, inertia), vibrations, couplings and sensor noise."
1168,hal-01205501.training.tei.xml,12,"In presence of a set of contacts C = {c 1. .. c n } between the robot and the environment, Equation (1) becomes"
1169,hal-01205501.training.tei.xml,13,"by including τ CM to accounts for the effect of the external wrenches (forces and moments) γ i applied at the contact location c i , and J ci (q) is the contact Jacobian."
1170,hal-01205501.training.tei.xml,14,"The joint torques τ RBD can be computed analytically through the rigid body dynamics model of the robot, a standard parametric description of the robot, with known or identified kinematics and dynamics parameters . A method to estimate joint torques and external forces integrating force/torque and tactile measurements was proposed in . The term (q, ˙ q, ¨ q) is often neglected, or modeled and estimated, or implicitly taken into account by considering a perturbation in the dynamics parameters of τ RBD .  The contact location c i is not necessarily fixed, as the contacts may occur on the whole robotic structure and not exclusively at the end-effectors. If the contact location is not known a priori, it must be estimated, typically, through distributed tactile sensors. To compute the contact Jacobian Jc i (q), we need the position of the contact point with respect to the reference frame of the link, which requires a kinematic/spatial calibration of the skin . As discussed in , model-based approaches for computing the inverse dynamics have some limitations: for example, the required prior calibration of the robot, the difficulty in modeling accurately complex nonlinear dynamics and in generating suitable exciting trajectories for the parameters identification. By contrast, machine learning methods offer an alternative, data-driven approach for learning a robot dynamics model, without the need to describe it with complex parametric models . Relying on collected robot data, learning the robot dynamics can be performed offline  or incrementally and online . Without the need for compensating inaccurate dynamics parameters and accumulated errors, a learned dynamics model can improve the tracking and control performance of a robot, as shown in for an industrial manipulator. In this paper, we make a step forward and show that learning the robot dynamics can also improve tracking and control performances of a humanoid arm in presence of contacts."
1171,hal-01205501.training.tei.xml,15,"In this section, we describe our approach for learning ID models with contacts. We first formalize the problem as learning a mixture-of-experts model. Then we detail how to implement Gaussian processes as the corresponding experts."
1172,hal-01205501.training.tei.xml,16,"When learning the inverse dynamics with contacts (Equation (3)), we assume that the (contact-free) inverse dynamics τ ID can be computed precisely, either from an analytic model or from a learned model. As a result, only the model of the additional term of the contact forces τ CM has to be learned. In this paper, we consider a robot that is provided with skin measurements s from the tactile sensors, force measurements F from the force torque sensors (FTS). A visual representation of these relevant components is shown in Figure 2. Predicting the contact forces τ CM can be formalized as the regression task"
1173,hal-01205501.training.tei.xml,17,where
1174,hal-01205501.training.tei.xml,18,"the inputs. Additionally, is an i.i.d. Gaussian measurement noise with mean0 and variance σ 2 n. Therefore, our regression problem becomes"
1175,hal-01205501.training.tei.xml,19,"It is necessary to consider the skin s as an input since contacts with different parts of the body lead to different effects in the dynamics. Intuitively, s is required to identify the position of the contact. The force/torque measurements F could be avoided if we were interested in learning contacts that do not change between training and test time, which would restrict us to static objects, such as a rigid floor, walls or stationary obstacles. However, as this assumption is limiting in our context, we include the force/torque measurements F in our model. The resulting regression of Equation (5) is challenging due to the high dimensionality of the input x ∈ X (the skin measurements s alone account forhundreds of dimensions) and nonlinearity. Observing that the contact forces term τ CM is a sum over multiple contacts, we can tackle this challenging regression by decomposing the τ CM into a sum of multiple regression problems, where each regression problem is a single contact. Since each contact is uniquely defined by its position, we can make contact torque contribution of each single contact model independent of the skin, and use the skin only to decide which contact model is currently active. As a result, we can rephrase the regression of Equation (5) as the learning of the mixture-of-experts model (""mixture of contacts"" in our case)"
1176,hal-01205501.training.tei.xml,20,"where J is the set of active experts f j identified by the gating network. The purpose of the gating network is to select the experts that are currently active, based on the skin input s, and to add their contributions. An illustration of this approach is shown in Figure 3. Note that in Equation 6 the skin input s is no longer explicitly part of the inputs of the experts, since it is now only used in the gating network. Hence, each single expert f j is now sufficiently low-dimensional to be modeled independently, but at the same time the possibility of summing the contribution of each contact allows to account for complex behaviors. For expert f j we use Gaussian"
1177,hal-01205501.training.tei.xml,21,"information regarding GP models and their training are given in the next subsection. The gating network can be considered as a classifier J = g(q, s, F ) that selects which contact is currently active. For simple tasks, this gating network can be designed using heuristics (e.g., using thresholds on the activation of the tactile sensors). Alternatively, for more complex systems an approach based on machine learning is more suitable (e.g., by learning a classifier using support vector machine). Another important issue is chosing the number of contact models to use. Generally, the more contact models the better, as each one would specialize in its own particular contact class (i.e., contacts that results is similar effects on"
1178,hal-01205501.training.tei.xml,22,"Gaussian Processes are a state-of-the-art regression method. They have been used in robotics to learn dynamics models  and for control . In the context of this paper, a GP is a distribution over inverse dynamics models"
1179,hal-01205501.training.tei.xml,23,"fully defined by a prior mean m f and a covariance function k f. When learn the contact models τ CM , we choose as prior mean m f ≡ τ ID and as covariance function k f the squared exponential with automatic relevance determination and Gaussian noise:"
1180,hal-01205501.training.tei.xml,24,where
1181,hal-01205501.training.tei.xml,25,") and δ pq is the Kronecker delta (which is one if p = q andzero otherwise). Here, l i are the characteristic length-scales, σ 2 f is the variance of the latent function f (·) and σ 2 w the noise variance. In our experiments, when learning contact models, the input is defined as X = [q, F ] and the output (observations) is y = τ are the torques. Hence, given n training inputs X = [x 1 , ..., x n ] and corresponding training targets y = [y 1 , ..., y n ], we define the training data set D = {X, y}. Training the GP corresponds to finding good hyperparameters θ = [l i , σ f , σ w ], which can be done by the standard procedure of maximizing the marginal likelihood ."
1182,hal-01205501.training.tei.xml,26,The GP yields the predictive distribution over torques for a new input
1183,hal-01205501.training.tei.xml,27,where the mean µ(x * ) and the variance σ 2 (x * ) are
1184,hal-01205501.training.tei.xml,28,"The entries of the matrix K are K ij = k(x i , x j ), and we define k * * = k(x * , x * ) and k * = k(X, x * )."
1185,hal-01205501.training.tei.xml,29,"In absence of contacts C = {0}, and with a perfect model, we can define the task-space nonlinear feedforward control:"
1186,hal-01205501.training.tei.xml,30,"where the u are the torques given as motor command and τ ID are the torques computed from the inverse dynamics (or a learned model of it). To compensate for any noise or inaccuracies in the feed-forward dynamics, an additional feedback controller (impedance controller or PD controller) is added, such that"
1187,hal-01205501.training.tei.xml,31,"Intuitively, the magnitude of the control signals contributed from the PD controller τ PD can be used to measure the goodness of the inverse dynamics model. Accurate inverse dynamics models will only need small corrections by the feedback controller during tracking of a desired trajectory, while inaccurate models will rely more heavily on it. In case of inaccurate models increasing the PD gains can still lead to acceptable tracking performance, but often at the expense of safety and compliance."
1188,hal-01205501.training.tei.xml,32,"In case of contacts, we can further extend the controller of Equation (12) to become"
1189,hal-01205501.training.tei.xml,33,"where τ CM is the contribution from the contacts (see Equation (3)), which is estimated using the learned model from Equation (6). In the experimental evaluations presented in the next section we will compare the performance of all thesethree control schema. τ PD drastically decrease when using the learned ID, therefore suggesting that the learned ID model is accurate."
1190,hal-01205501.training.tei.xml,34,"In this section we present the experimental results obtained from a tracking task in the presence of contacts. First we demonstrate that an accurate (contact-free) ID model can be learned. This ID model will be used as base for comparing the tracking performance in presence of obstacles. Second we demonstrate that our proposed model can be used to compensate for an unknown obstacle and achieve better tracking performance. To conclude, we demonstrate the generalization capabilities of our learned model by testing it on a novel obstacle."
1191,hal-01205501.training.tei.xml,35,"The experimental evaluations are performed on the iCubDarmstadt01, the most recent version (v. 2.5) of iCub. The iCub has 53 degrees of freedom and is 104 cm tall for 24 kg of weight. The robot is not equipped with joint torque sensors, but it has four 6-axis force/torque sensors placed proximally in the middle of legs and arms, and an artificial skin consisting of more than 3000 tactile sensors, mounted on the robot covers . Each of these sensors provide a signal in the interval ∈ [0,255] correlated to the force applied orthogonally to the sensor. In our experiments, we control 4 DoF of the iCub arm: shoulder pitch, roll and jaw, and elbow. The skin input s from the forearm consists of 384 sensors. In the iCub the joint torques are not estimated by the motors currents, but via an analytic model with identified dynamics parameters , which updates the estimation thanks to the inertial, tactile and force/torque measurements . We compute, on a PC outside the robot, the desired joint torques at the frequency of 50 Hz and then send the commands to the robot interface Yarp. A low-level controller implemented on the motor boards takes care of tracking the desired joint torques, sent through the robot interface. Both the estimation of the joint torques and the low-lever controller are transparent to our learned model as we use the desired torques (instead of the measured torques) as targets y during the training of the models."
1192,hal-01205501.training.tei.xml,36,"Before learning the contact models, a pre-requisite of our approach is the existence of a (contact-free) rigid-body inverse dynamics model. In our case, we decided to learn such ID model from scratch using Gaussian Processes. The reason for this choice are the unmodeled dynamics (q, ˙ q, ¨ q), which introduce substantial errors even in absence of contacts. In this subsection we present an experimental validation of the learned (contact-free) inverse dynamics. To validate the learned ID model we firstly compared the tracking error, in absence of obstacles. The task used for the experimental validation of the proposed approach consists of a tracking task with the left arm of the iCub where each of thefour joints of the arm should follow a pre-designed trajectory. In Table I we collected the tracking errors obtained averaging over 20 repetitions. The results shows that the learned ID model reduces the tracking error, compared to the simple PD controller. The visualization of Figure 4 shows the average error tracking for the shoulder jaw. As second comparison we analyzed the contribution to the desired torque from the PD controller τ PD. In Figure 5 it is visible that when using the learned ID model, τ PD is drastically reduced. Both experimental results suggests that the learned ID model is accurate and useful for the desired tracking task."
1193,hal-01205501.training.tei.xml,37,"For the validation of the inverse dynamics with contacts model we consider the same trajectory tracking task used in the previous evaluation. However, in this experiments we include the presence of a dynamic obstacle, specifically a filled1-liter bottle of water. For classical controllers, when an obstruction occur, the rigid body inverse dynamics does not account for this variation. As a result, the tracking error increases and with it also the contribution of the PD feedback controller increases to compensate for this tracking error. In this scenario, we demonstrate that using a learned contact model improves the tracking accuracy when unforeseen and dynamic obstacles are encountered along the path."
1194,hal-01205501.training.tei.xml,38,"We first performed the tracking experiment (with the obstacle) once using as controller the PD + ID, in order to collect training data. Using these data collected, we trained the contact model using as target the difference between the desired torque and the inverse dynamics model contribution τ ID (we here assume a single contact model). Following, we repeated the experiment comparing the full controller PD + ID + CM, the simple PD, and PD + ID. In Table II are collected the tracking errors for thethree control schema and for all the four joint, averaged over 20 repetitions. The presence of the contact model proved beneficial in terms of tracking error for three out of four joints. For the elbow, the PD +ID performed slightly better. However, it is important to notice that the joints that are most involved in the trajectory are the shoulder pitch and shoulder jaw. In Figure 6 is shows the average tracking error (for the shoulder jaw) for the considered control modes. It is clearly visible that after about 1 second from the contact with the obstacle, the contact model reduce the tracking error. From these results we can conclude that using the additional learned contact model is beneficial. The profile of the average torque contributed by τ CM is shown in Figure 7."
1195,hal-01205501.training.tei.xml,39,"One limitation of the current results is that the gating network does not always recognize the presence of the contact. This is due to hardware limitations of the iCub skin. In fact, the current skin presentstwo main issues: 1) high levels of noise. Even without contacts, some sensors will occasionally fire as if in presence of strong forces applied. 2) Mechanical compliance of the skin. When applying a force, The sum of these two issues resulted in the introduction of delays for the identification of contacts and therefore the use of the additional contact model. This delay, which we estimate around 500ms is visible in both Figure 6 and Figure 7. Often even for a human expert it is hard to distinguish between measurement noise and a real contact when observing short (i.e., less than 100 ms) windows of data. A second issue we encountered during the training of both inverse dynamics model and contact models, is that the torques applied from the basic PD controller implemented on the iCub suffer from a high noise. As a result of this high degree of noise, we had to filter the desired torques. However, filtering the desired torques inevitably introduce small errors in the models learned. We expect that a smoother desired torques would result in a more accurate learned model and even lower tracking error."
1196,hal-01205501.training.tei.xml,40,"To analyze the generalization capabilities of a single contact model we experimented the use of obstacles not included in the training data. In a similar manner to the previous experiment we collected data using the PD + ID ontwo different obstacles: the 1-liter bottle of water and a bin filled with 2 liters of water. Using the data collected from both these obstacles we trained the contact model. We tested the resulting learned model on a new obstacle: a bin filled with 1.3 liters of water. Table III report the average tracking error over 25 experiments. It can be noticed that for the two joints with the highest usage during the task (i.e., shoulder roll and jaw), the contact model considerably improve the tracking accuracy (respectively of 2.5 • and 1.8 • ). A visualization of the tracking error for the shoulder roll is shown in Figure 8. However, for the other two joints we observed a small (< 0.25 • ) decrease of the tracking performance. Overall, these results suggest that the contact model is capable of some degree of generalization to novel obstacles with different properties (e.g., weight)."
1197,hal-01205501.training.tei.xml,41,"In this paper, we experimentally evaluated on the humanoid robot iCub a data-driven approach for learning inverse dynamics models in presence of contacts. Our proposed solution enables a fast and accurate prediction of the joint torques in situations when the robot is in contact with obstacles, trough the use of the tactile skin."
1198,hal-01205501.training.tei.xml,42,"We demonstrate on a pushing task that our learned model substantially improve the tracking performance in presence of unknown obstacles. The estimation from the learned model does not rely on dynamic parameters, but it is completely data-driven: uncalibrated tactile sensors provide raw information about the contact, while force/torque sensors provide information about the wrenches perceived by the robotic structure. As a result, our approach does not require a spatially calibrated model of the skin. We believe that this feature is particularly relevant with the increasing availability of miniaturized skin sensors. For future robot skin, composed ofthousands or millions of sensors, it would be extremely challenging and time consuming to perform a full spatial calibration. Our approach is a first step toward the use of learning methods to alleviate this problem."
